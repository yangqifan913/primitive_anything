# -*- coding: utf-8 -*-
"""
åˆ†æ®µå¼3Dæ£€æµ‹è®­ç»ƒå™¨
æ”¯æŒteacher forcing -> scheduled sampling -> pure generationçš„æ¸è¿›è®­ç»ƒ
å¤šGPUã€SwanLabæ—¥å¿—ã€å®Œæ•´éªŒè¯å’Œcheckpointç®¡ç†
"""

import os
import json
import time
import math
import random
import warnings
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, asdict

import numpy as np
import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from torch.cuda.amp import autocast, GradScaler

try:
    import swanlab
    SWANLAB_AVAILABLE = True
except ImportError:
    SWANLAB_AVAILABLE = False
    warnings.warn("SwanLab not available. Install with: pip install swanlab")

from config_loader import ConfigLoader
from primitive_anything_3d import PrimitiveTransformer3D
from loss_3d import AdaptivePrimitiveTransformer3DLoss
from dataloader_3d import create_dataloader, generate_equivalent_box_representations, normalize_angle

def select_best_equivalent_representation(pred_box, equivalent_boxes):
    """
    é€‰æ‹©ä¸é¢„æµ‹boxæ—‹è½¬è§’åº¦è¯¯å·®æœ€å°çš„ç­‰ä»·è¡¨ç¤ºï¼ˆè¿‡æ»¤æ‰è§’åº¦å¤§äº60åº¦çš„ç­‰æ•ˆboxï¼‰
    
    Args:
        pred_box: é¢„æµ‹box (x, y, z, l, w, h, roll, pitch, yaw)
        equivalent_boxes: ç­‰ä»·è¡¨ç¤ºåˆ—è¡¨ [(x, y, z, l, w, h, roll, pitch, yaw), ...]
    
    Returns:
        best_box: æœ€ä¼˜çš„ç­‰ä»·è¡¨ç¤º
        min_loss: æœ€å°æ—‹è½¬loss
    """
    import math
    
    # ğŸ”§ è¿‡æ»¤æ‰ä»»ä½•è§’åº¦å¤§äº180åº¦çš„ç­‰æ•ˆbox
    valid_equivalent_boxes = []
    for equiv_box in equivalent_boxes:
        roll, pitch, yaw = equiv_box[6], equiv_box[7], equiv_box[8]

        if abs(roll) <= math.pi/3 and abs(pitch) <= math.pi/3 and abs(yaw) <= math.pi/3:
            valid_equivalent_boxes.append(equiv_box)
    
    # å¦‚æœè¿‡æ»¤åæ²¡æœ‰æœ‰æ•ˆçš„ç­‰æ•ˆboxï¼ŒæŠ¥é”™
    if len(valid_equivalent_boxes) == 0:
        raise ValueError(f"âŒ é”™è¯¯ï¼šæ‰€æœ‰ç­‰æ•ˆboxéƒ½åŒ…å«å¤§äº60åº¦çš„è§’åº¦ï¼åŸå§‹ç­‰æ•ˆboxæ•°é‡: {len(equivalent_boxes)}")
    
    # ğŸ” æ‰“å°ç­›é€‰ç»Ÿè®¡ä¿¡æ¯
    # print(f"     ğŸ“Š ç­‰æ•ˆboxç­›é€‰ç»Ÿè®¡: åŸå§‹={len(equivalent_boxes)}, ç­›é€‰å={len(valid_equivalent_boxes)}")
    
    min_loss = float('inf')
    best_box = valid_equivalent_boxes[0]  # é»˜è®¤é€‰æ‹©ç¬¬ä¸€ä¸ªæœ‰æ•ˆbox
    
    pred_roll, pred_pitch, pred_yaw = pred_box[6], pred_box[7], pred_box[8]
    
    # ç¡®ä¿è§’åº¦å€¼æ˜¯æ ‡é‡ï¼Œä¸æ˜¯åˆ—è¡¨
    if isinstance(pred_roll, list):
        pred_roll = pred_roll[0]
    if isinstance(pred_pitch, list):
        pred_pitch = pred_pitch[0]
    if isinstance(pred_yaw, list):
        pred_yaw = pred_yaw[0]
    
    def angular_error(pred_angle, gt_angle):
        """è®¡ç®—è§’åº¦è¯¯å·®ï¼Œç®€å•L1 loss"""
        return abs(pred_angle - gt_angle)
    
    # åªåœ¨æœ‰æ•ˆçš„ç­‰æ•ˆboxä¸­é€‰æ‹©æœ€ä¼˜çš„
    for equiv_box in valid_equivalent_boxes:
        gt_roll, gt_pitch, gt_yaw = equiv_box[6], equiv_box[7], equiv_box[8]
        
        # è®¡ç®—ç®€å•L1 lossçš„æ—‹è½¬è§’åº¦è¯¯å·®
        roll_loss = angular_error(pred_roll, gt_roll)
        pitch_loss = angular_error(pred_pitch, gt_pitch)
        yaw_loss = angular_error(pred_yaw, gt_yaw)
        
        total_loss = roll_loss + pitch_loss + yaw_loss
        
        if total_loss < min_loss:
            min_loss = total_loss
            best_box = equiv_box
    
    return best_box, min_loss


@dataclass
class TrainingPhase:
    """è®­ç»ƒé˜¶æ®µé…ç½®"""
    name: str
    epochs: int
    teacher_forcing_ratio: float    # 1.0=å®Œå…¨teacher forcing, 0.0=å®Œå…¨ç”Ÿæˆ
    scheduled_sampling: bool        # æ˜¯å¦ä½¿ç”¨scheduled sampling
    sampling_strategy: str          # linear, exponential, inverse_sigmoid
    description: str


@dataclass
class TrainingStats:
    """è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
    epoch: int
    phase: str
    teacher_forcing_ratio: float
    train_loss: float
    train_classification_loss: float
    train_iou_loss: float
    train_delta_loss: float
    train_eos_loss: float
    train_mean_iou: float
    val_loss: float
    val_generation_loss: float
    val_mean_iou: float
    val_generation_iou: float
    learning_rate: float
    adaptive_cls_weight: float
    adaptive_delta_weight: float
    epoch_time: float
    
    # ğŸ” æ·»åŠ æ—‹è½¬è§’åº¦æŸå¤±ç»Ÿè®¡
    train_roll_cls_loss: float = 0.0
    train_pitch_cls_loss: float = 0.0
    train_yaw_cls_loss: float = 0.0
    train_roll_delta_loss: float = 0.0
    train_pitch_delta_loss: float = 0.0
    train_yaw_delta_loss: float = 0.0


class AdvancedTrainer:
    """é«˜çº§3Dæ£€æµ‹è®­ç»ƒå™¨"""
    
    def __init__(
        self,
        config_loader: ConfigLoader,
        output_dir: str = "experiments",
        experiment_name: str = "primitive_3d_exp",
        resume_from: Optional[str] = None,
        local_rank: int = 0,
        world_size: int = 1,
        use_swanlab: bool = True,
        validation_samples: List[int] = None  # æŒ‡å®šç”¨äºéªŒè¯å¯è§†åŒ–çš„æ ·æœ¬ç´¢å¼•
    ):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        Args:
            config_loader: é…ç½®åŠ è½½å™¨
            output_dir: è¾“å‡ºç›®å½•
            experiment_name: å®éªŒåç§°
            resume_from: æ¢å¤è®­ç»ƒçš„checkpointè·¯å¾„
            local_rank: æœ¬åœ°GPUæ’å
            world_size: æ€»GPUæ•°é‡
            use_swanlab: æ˜¯å¦ä½¿ç”¨SwanLab
            validation_samples: éªŒè¯å¯è§†åŒ–æ ·æœ¬ç´¢å¼•åˆ—è¡¨
        """
        self.config_loader = config_loader
        self.local_rank = local_rank
        self.world_size = world_size
        
        self.is_main_process = (local_rank == 0)
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        self.output_dir = Path(output_dir) / experiment_name
        self.checkpoint_dir = self.output_dir / "checkpoints"
        self.validation_dir = self.output_dir / "validation_results"
        
        if self.is_main_process:
            self.output_dir.mkdir(parents=True, exist_ok=True)
            self.checkpoint_dir.mkdir(exist_ok=True)
            self.validation_dir.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–è®¾å¤‡
        self.device = torch.device(f'cuda:{local_rank}' if torch.cuda.is_available() else 'cpu')
        torch.cuda.set_device(local_rank)
        
        # åŠ è½½é…ç½®
        self.model_config = config_loader.get_model_config()
        self.training_config = config_loader.get_training_config()
        
        # åˆå§‹åŒ–SwanLab - åªåœ¨ä¸»è¿›ç¨‹åˆå§‹åŒ–
        self.use_swanlab = use_swanlab and SWANLAB_AVAILABLE and self.is_main_process
        if self.use_swanlab:
            if self.is_main_process:
                print(f"ğŸ“Š åˆå§‹åŒ–SwanLabæ—¥å¿—...")
            swanlab.init(
                project="primitive-3d-detection",
                experiment_name=experiment_name,
                description="3Dç‰©ä½“æ£€æµ‹withåˆ†æ®µå¼è®­ç»ƒ",
                config=dict(
                    model_config=self.model_config,
                    training_config=self.training_config
                )
            )
            if self.is_main_process:
                print(f"âœ… SwanLabåˆå§‹åŒ–å®Œæˆ")
        
        # ä»é…ç½®æ–‡ä»¶åŠ è½½è®­ç»ƒé˜¶æ®µ
        phases_config = config_loader.get_training_phases()
        self.training_phases = []
        
        for phase_name, phase_config in phases_config.items():
            self.training_phases.append(TrainingPhase(
                name=phase_name,
                epochs=phase_config.get('epochs', 15),
                teacher_forcing_ratio=phase_config.get('teacher_forcing_ratio', 1.0),
                scheduled_sampling=phase_config.get('scheduled_sampling', False),
                sampling_strategy=phase_config.get('sampling_strategy', "none"),
                description=phase_config.get('description', f"{phase_name}é˜¶æ®µ")
            ))
        
        # éªŒè¯æ ·æœ¬ç´¢å¼•
        global_config = self.config_loader.get_global_config()
        self.validation_samples = validation_samples or global_config['logging']['validation_samples']
        
        # è®¾ç½®pad_idå±æ€§
        self.pad_id = global_config['pad_id']
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.current_phase_idx = 0
        self.best_val_loss = float('inf')
        self.best_generation_loss = float('inf')
        self.training_stats = []
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        self.use_amp = self.training_config.get('mixed_precision', True)
        if self.use_amp:
            try:
                # ä½¿ç”¨æ–°çš„APIï¼ˆPyTorch 2.0+ï¼‰
                self.scaler = torch.amp.GradScaler('cuda')
            except TypeError:
                # å›é€€åˆ°æ—§API
                self.scaler = GradScaler()
        else:
            self.scaler = None
        
        # è®­ç»ƒä¼˜åŒ–é…ç½®
        opt_config = self.training_config.get('optimizations', {})
        
        # æå‰åœæ­¢é…ç½®
        early_stop_config = opt_config.get('early_stopping', {})
        self.enable_early_stopping = early_stop_config.get('enabled', True)
        self.eos_threshold = early_stop_config.get('eos_threshold', 0.5)
        self.adaptive_sequence_length = early_stop_config.get('adaptive_sequence_length', True)
        
        # å¢é‡æ¨ç†é…ç½®
        inference_config = opt_config.get('incremental_inference', {})
        self.use_incremental_inference = inference_config.get('enabled', True)
        self.incremental_temperature = inference_config.get('temperature', 1.0)
        
        # æ¢¯åº¦è£å‰ªé…ç½®
        grad_clip_config = opt_config.get('gradient_clipping', {})
        self.use_grad_clipping = grad_clip_config.get('enabled', True)
        self.max_grad_norm = grad_clip_config.get('max_norm', 1.0)
        
        # PyTorchç¼–è¯‘ä¼˜åŒ–é…ç½®
        self.torch_compile_config = opt_config.get('torch_compile', {})
        
        # CuDNNä¼˜åŒ–é…ç½®
        cudnn_config = opt_config.get('cudnn_optimizations', {})
        if cudnn_config.get('benchmark', True):
            torch.backends.cudnn.benchmark = True
        if not cudnn_config.get('deterministic', True):
            torch.backends.cudnn.deterministic = False
        
        # åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œæ•°æ®åŠ è½½å™¨
        self._setup_model_and_data()
        
        # æ¢å¤è®­ç»ƒçŠ¶æ€
        if resume_from:
            self._load_checkpoint(resume_from)
        
        if self.is_main_process:
            print(f"âœ… è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ (Rank {local_rank}/{world_size})")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
            print(f"ğŸ¯ è®­ç»ƒé˜¶æ®µ: {len(self.training_phases)}ä¸ªé˜¶æ®µ")
            print(f"ğŸ“Š SwanLabæ—¥å¿—: {'å¯ç”¨' if self.use_swanlab else 'ç¦ç”¨'}")
            print(f"ğŸ–¥ï¸  GPUæ•°é‡: {torch.cuda.device_count()}")
            print(f"ğŸš€ åˆ†å¸ƒå¼è®­ç»ƒ: {'å¯ç”¨' if (self.world_size > 1 and torch.cuda.device_count() > 1) else 'ç¦ç”¨'}")
        else:
            print(f"ğŸ”„ å·¥ä½œè¿›ç¨‹å¯åŠ¨ (Rank {local_rank}/{world_size})")
            print(f"   GPUè®¾å¤‡: {self.device}")
            print(f"   ç­‰å¾…ä¸»è¿›ç¨‹åŒæ­¥...")
        
        # å¤šGPUåŒæ­¥ç‚¹
        if self.world_size > 1 and torch.cuda.device_count() > 1:
            if torch.distributed.is_initialized():
                torch.distributed.barrier()
                if self.is_main_process:
                    print(f"âœ… æ‰€æœ‰è¿›ç¨‹åŒæ­¥å®Œæˆï¼Œå¼€å§‹è®­ç»ƒ...")
    
    def _setup_model_and_data(self):
        """è®¾ç½®æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œæ•°æ®åŠ è½½å™¨"""
        # åˆ›å»ºæ¨¡å‹
        model_config = self.config_loader.get_model_config()
        global_config = self.config_loader.get_global_config()
        
        # è·å–æ•°æ®é…ç½®ç”¨äºè¿ç»­èŒƒå›´
        data_config = self.config_loader.get_data_config()
        continuous_ranges = data_config.get('continuous_ranges', {})
        
        # ä»æ¨¡å‹é…ç½®ä¸­è·å–å„ä¸ªéƒ¨åˆ†
        discretization = model_config.get('discretization', {})
        embeddings = model_config.get('embeddings', {})
        transformer = model_config.get('transformer', {})
        image_encoder = model_config.get('image_encoder', {})
        conditioning = model_config.get('conditioning', {})
        advanced = model_config.get('advanced', {})
        
        # éªŒè¯å¿…è¦çš„é…ç½®éƒ¨åˆ†æ˜¯å¦å­˜åœ¨
        if not discretization:
            raise ValueError("æ¨¡å‹é…ç½®ä¸­ç¼ºå°‘ 'discretization' éƒ¨åˆ†")
        if not embeddings:
            raise ValueError("æ¨¡å‹é…ç½®ä¸­ç¼ºå°‘ 'embeddings' éƒ¨åˆ†")
        if not transformer:
            raise ValueError("æ¨¡å‹é…ç½®ä¸­ç¼ºå°‘ 'transformer' éƒ¨åˆ†")
        if not image_encoder:
            raise ValueError("æ¨¡å‹é…ç½®ä¸­ç¼ºå°‘ 'image_encoder' éƒ¨åˆ†")
        if not conditioning:
            raise ValueError("æ¨¡å‹é…ç½®ä¸­ç¼ºå°‘ 'conditioning' éƒ¨åˆ†")
        if not advanced:
            raise ValueError("æ¨¡å‹é…ç½®ä¸­ç¼ºå°‘ 'advanced' éƒ¨åˆ†")
        
        self.model = PrimitiveTransformer3D(
            # ç¦»æ•£åŒ–å‚æ•° - 3ä¸ªå±æ€§
            num_discrete_position=discretization['num_discrete_position'],
            num_discrete_rotation=discretization['num_discrete_rotation'],
            num_discrete_size=discretization['num_discrete_size'],
            
            # è¿ç»­èŒƒå›´ - 3ä¸ªå±æ€§
            continuous_range_position=continuous_ranges['position'],
            # ğŸ”§ ä¿®å¤ï¼šå°†è§’åº¦åˆ¶è½¬æ¢ä¸ºå¼§åº¦åˆ¶
            continuous_range_rotation=[[math.radians(continuous_ranges['rotation'][0][0]), math.radians(continuous_ranges['rotation'][0][1])],
                                      [math.radians(continuous_ranges['rotation'][1][0]), math.radians(continuous_ranges['rotation'][1][1])],
                                      [math.radians(continuous_ranges['rotation'][2][0]), math.radians(continuous_ranges['rotation'][2][1])]],
            continuous_range_size=continuous_ranges['size'],
            
            # åµŒå…¥ç»´åº¦ - 3ä¸ªå±æ€§
            dim_position_embed=embeddings['dim_position_embed'],
            dim_rotation_embed=embeddings['dim_rotation_embed'],
            dim_size_embed=embeddings['dim_size_embed'],
            
            # æ¨¡å‹å‚æ•°
            max_primitive_len=global_config['max_seq_len'],
            dim=transformer['dim'],
            attn_depth=transformer['depth'],
            attn_heads=transformer['heads'],
            attn_dim_head=transformer['dim_head'],
            attn_dropout=transformer['attn_dropout'],  # æ³¨æ„åŠ›dropout
            ff_dropout=transformer['ff_dropout'],      # å‰é¦ˆdropout
            
            # å›¾åƒç¼–ç å™¨å‚æ•°
            image_encoder_dim=image_encoder['output_dim'],
            use_fpn=image_encoder['use_fpn'],
            backbone=image_encoder['backbone'],
            pretrained=image_encoder['pretrained'],
            
            # æ¡ä»¶åŒ–é…ç½®
            condition_on_image=conditioning['condition_on_image'],
            gateloop_use_heinsen=advanced['gateloop_use_heinsen'],
            
            # å…¶ä»–å‚æ•°
            pad_id=global_config['pad_id']
        )
        self.model = self.model.to(self.device)
        
        # PyTorch 2.0 ç¼–è¯‘ä¼˜åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if hasattr(torch, 'compile') and self.torch_compile_config.get('enabled', False):
            compile_mode = self.torch_compile_config.get('mode', 'default')
            if self.is_main_process:
                print(f"ğŸ”¥ å¯ç”¨PyTorch 2.0ç¼–è¯‘ä¼˜åŒ– (mode: {compile_mode})")
            self.model = torch.compile(self.model, mode=compile_mode)
        
        # å¤šGPUè®¾ç½® - åªæœ‰åœ¨çœŸæ­£çš„å¤šGPUç¯å¢ƒä¸‹æ‰å¯ç”¨DDP
        if self.world_size > 1 and torch.cuda.device_count() > 1:
            if self.is_main_process:
                print(f"ğŸš€ å¯ç”¨åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ (DDP)")
            # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            self.model = self.model.to(self.device)
            # ä½¿ç”¨DDPåŒ…è£…æ¨¡å‹
            self.model = DDP(self.model, device_ids=[self.local_rank], output_device=self.local_rank)
            if self.is_main_process:
                print(f"âœ… DDPæ¨¡å‹åŒ…è£…å®Œæˆ")
        elif self.world_size > 1 and self.is_main_process:
            print(f"âš ï¸  æ£€æµ‹åˆ°å•GPUç¯å¢ƒï¼Œç¦ç”¨DDP")
        else:
            # å•GPUæˆ–world_size=1çš„æƒ…å†µ
            if self.is_main_process:
                print(f"ğŸ–¥ï¸  å•GPUè®­ç»ƒæ¨¡å¼")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨ - ç§»é™¤ç¡¬ç¼–ç 
        data_config = self.config_loader.get_data_config()
        global_config = self.config_loader.get_global_config()
        dataset_config = data_config['dataset']
        dataloader_config = data_config['dataloader']
        
        # è°ƒè¯•ä¿¡æ¯
        if self.is_main_process:
            print(f"ğŸ” æ•°æ®åŠ è½½å™¨é…ç½®:")
            print(f"   åŸå§‹batch_size: {dataloader_config.get('batch_size', 'N/A')}")
            print(f"   world_size: {self.world_size}")
            print(f"   GPUæ•°é‡: {torch.cuda.device_count()}")
            print(f"   æ•°æ®é›†è·¯å¾„: {dataset_config.get('data_root', 'N/A')}")
        
        # ä½¿ç”¨ä¿®å¤çš„create_dataloaderå‡½æ•°
        from dataloader_3d import create_dataloader
        
        # å‡†å¤‡æ•°æ®é›†å‚æ•°
        dataset_kwargs = {
            'data_root': dataset_config['data_root'],
            'stage': "train",
            'max_boxes': global_config['max_seq_len'],
            'image_size': global_config['image_size'],
            'continuous_ranges': data_config['continuous_ranges'],
            'augmentation_config': data_config['augmentation']
        }
        
        # å‡†å¤‡DataLoaderå‚æ•°
        dataloader_kwargs = {
            'batch_size': dataloader_config['batch_size'],
            'shuffle': True,
            'num_workers': dataloader_config['num_workers'],
            'pin_memory': dataloader_config['pin_memory'],
            'drop_last': True,
            'prefetch_factor': dataloader_config['prefetch_factor'],
            'persistent_workers': dataloader_config['persistent_workers']
        }
        
        # å…ˆåˆ›å»ºæ•°æ®é›†ï¼ˆç”¨äºåˆ†å¸ƒå¼é‡‡æ ·å™¨ï¼‰
        from dataloader_3d import Box3DDataset
        
        train_dataset = Box3DDataset(**dataset_kwargs)
        
        val_dataset_kwargs = dataset_kwargs.copy()
        val_dataset_kwargs['stage'] = "val"
        val_dataset_kwargs['augmentation_config'] = {}  # éªŒè¯æ—¶ä¸ä½¿ç”¨æ•°æ®å¢å¼º
        val_dataset = Box3DDataset(**val_dataset_kwargs)
        
        # åˆ†å¸ƒå¼é‡‡æ ·å™¨ - åªæœ‰åœ¨çœŸæ­£çš„å¤šGPUç¯å¢ƒä¸‹æ‰åˆ›å»º
        if self.world_size > 1 and torch.cuda.device_count() > 1:
            if self.is_main_process:
                print(f"ğŸš€ å¯ç”¨åˆ†å¸ƒå¼é‡‡æ ·å™¨")
            self.train_sampler = DistributedSampler(train_dataset)
            self.val_sampler = DistributedSampler(val_dataset, shuffle=False)
            if self.is_main_process:
                print(f"âœ… åˆ†å¸ƒå¼é‡‡æ ·å™¨åˆ›å»ºå®Œæˆ")
        else:
            if self.world_size > 1 and self.is_main_process:
                print(f"âš ï¸  æ£€æµ‹åˆ°å•GPUç¯å¢ƒï¼Œç¦ç”¨åˆ†å¸ƒå¼é‡‡æ ·å™¨")
            self.train_sampler = None
            self.val_sampler = None
            if self.is_main_process:
                print(f"ğŸ–¥ï¸  ä½¿ç”¨æ™®é€šæ•°æ®åŠ è½½å™¨")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        from torch.utils.data import DataLoader
        
        # å¤šGPUæ—¶è°ƒæ•´batch_size
        original_batch_size = dataloader_config['batch_size']
        effective_batch_size = original_batch_size
        
        if self.world_size > 1 and torch.cuda.device_count() > 1:
            # è®¡ç®—æ¯GPUçš„batch_size
            effective_batch_size = max(1, original_batch_size // self.world_size)
            
            # å¦‚æœæ¯GPUçš„batch_sizeå¤ªå°ï¼Œè‡ªåŠ¨è°ƒæ•´æ€»batch_size
            if effective_batch_size < 1:
                effective_batch_size = 1
                adjusted_total_batch_size = self.world_size
                if self.is_main_process:
                    print(f"âš ï¸  åŸå§‹batch_size={original_batch_size}è¿‡å°ï¼Œè‡ªåŠ¨è°ƒæ•´ä¸ºæ€»batch_size={adjusted_total_batch_size}")
            else:
                adjusted_total_batch_size = original_batch_size
            
            if self.is_main_process:
                print(f"ğŸ“Š å¤šGPUè®­ç»ƒ: æ€»batch_size={adjusted_total_batch_size}, æ¯GPU={effective_batch_size}")
        else:
            if self.is_main_process:
                print(f"ğŸ“Š å•GPUè®­ç»ƒ: batch_size={effective_batch_size}")
        
        # æœ€ç»ˆéªŒè¯
        if effective_batch_size < 1:
            print(f"âŒ é”™è¯¯: batch_size={effective_batch_size}ï¼Œå¼ºåˆ¶è®¾ç½®ä¸º1")
            effective_batch_size = 1
        
        # æ›´æ–°batch_size
        dataloader_kwargs['batch_size'] = effective_batch_size
        
        # åˆ›å»ºè®­ç»ƒDataLoaderï¼ˆä½¿ç”¨ä¿®å¤çš„create_dataloaderï¼‰
        self.train_loader = create_dataloader(**dataset_kwargs, **dataloader_kwargs)
        
        # åˆ›å»ºéªŒè¯DataLoader
        val_dataloader_kwargs = dataloader_kwargs.copy()
        val_dataloader_kwargs['shuffle'] = False
        val_dataloader_kwargs['drop_last'] = False
        
        self.val_loader = create_dataloader(**val_dataset_kwargs, **val_dataloader_kwargs)
        
        # ä¼˜åŒ–å™¨ - ç§»é™¤ç¡¬ç¼–ç 
        optimizer_config = self.training_config['optimizer']
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=optimizer_config['lr'],
            weight_decay=optimizer_config['weight_decay'],
            betas=optimizer_config['betas']
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨ - æ”¯æŒå¤šç§è°ƒåº¦å™¨ç±»å‹
        scheduler_config = self.training_config['scheduler']
        if scheduler_config['type'] == 'CosineAnnealingLR':
            total_epochs = sum(phase.epochs for phase in self.training_phases)
            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=scheduler_config['T_max'],
                eta_min=scheduler_config['eta_min']
            )
        elif scheduler_config['type'] == 'CosineAnnealingWarmRestarts':
            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
                self.optimizer,
                T_0=scheduler_config['T_0'],
                T_mult=scheduler_config.get('T_mult', 2),
                eta_min=scheduler_config['eta_min']
            )
        else:
            self.scheduler = None
    
    def _compute_teacher_forcing_ratio(self, phase: TrainingPhase, epoch_in_phase: int) -> float:
        """è®¡ç®—å½“å‰çš„teacher forcingæ¯”ä¾‹"""
        if not phase.scheduled_sampling:
            return phase.teacher_forcing_ratio
        
        # Scheduled samplingç­–ç•¥
        progress = epoch_in_phase / phase.epochs
        
        if phase.sampling_strategy == "linear":
            # çº¿æ€§è¡°å‡
            return phase.teacher_forcing_ratio * (1 - progress)
        elif phase.sampling_strategy == "exponential":
            # æŒ‡æ•°è¡°å‡
            return phase.teacher_forcing_ratio * (0.5 ** (progress * 5))
        elif phase.sampling_strategy == "inverse_sigmoid":
            # åsigmoidè¡°å‡ï¼šä»1.0å¹³æ»‘è¡°å‡åˆ°æ¥è¿‘0
            k = 5  # æ§åˆ¶è¡°å‡é€Ÿåº¦
            # è°ƒæ•´å…¬å¼ç¡®ä¿ä»1.0å¼€å§‹è¡°å‡
            sigmoid_factor = 1 / (1 + math.exp(-k * (progress - 0.5)))
            return phase.teacher_forcing_ratio * (1 - sigmoid_factor)
        else:
            return phase.teacher_forcing_ratio
    
    def _create_loss_function(self, phase_name: str):
        """ä¸ºä¸åŒè®­ç»ƒé˜¶æ®µåˆ›å»ºæŸå¤±å‡½æ•°"""
        # æ ¹æ®è®­ç»ƒé˜¶æ®µæ˜ å°„åˆ°é…ç½®ä¸­çš„stage
        stage_mapping = {
            "teacher_forcing": "warmup",
            "scheduled_sampling": "main", 
            "pure_generation": "finetune"
        }
        
        # åˆ›å»ºæŸå¤±å‡½æ•°
        loss_config = self.config_loader.get_loss_config()
        global_config = self.config_loader.get_global_config()
        data_config = self.config_loader.get_data_config()
        model_config = self.config_loader.get_model_config()
        
        # ç§»é™¤ç¡¬ç¼–ç ï¼Œå¼ºåˆ¶ä»é…ç½®è¯»å–
        base_weights = loss_config['base_weights']
        adaptive_weights = loss_config['adaptive_weights']
        continuous_ranges = data_config['continuous_ranges']
        discretization = model_config['discretization']
        algorithm_config = loss_config['algorithm']
        data_processing = loss_config['data_processing']
        
        return AdaptivePrimitiveTransformer3DLoss(
            # ç¦»æ•£åŒ–å‚æ•° - 9ä¸ªå•ç‹¬å±æ€§
            num_discrete_x=discretization['num_discrete_position'],
            num_discrete_y=discretization['num_discrete_position'],
            num_discrete_z=discretization['num_discrete_position'],
            num_discrete_w=discretization['num_discrete_size'],
            num_discrete_h=discretization['num_discrete_size'],
            num_discrete_l=discretization['num_discrete_size'],
            num_discrete_roll=discretization['num_discrete_rotation'],
            num_discrete_pitch=discretization['num_discrete_rotation'],
            num_discrete_yaw=discretization['num_discrete_rotation'],
            
            # è¿ç»­èŒƒå›´å‚æ•° - 9ä¸ªå•ç‹¬å±æ€§
            continuous_range_x=continuous_ranges['position'][0],
            continuous_range_y=continuous_ranges['position'][1],
            continuous_range_z=continuous_ranges['position'][2],
            continuous_range_w=continuous_ranges['size'][0],
            continuous_range_h=continuous_ranges['size'][1],
            continuous_range_l=continuous_ranges['size'][2],
            # ğŸ”§ ä¿®å¤ï¼šå°†è§’åº¦åˆ¶è½¬æ¢ä¸ºå¼§åº¦åˆ¶ [-180Â°, 180Â°] -> [-Ï€, Ï€]
            continuous_range_roll=[math.radians(continuous_ranges['rotation'][0][0]), math.radians(continuous_ranges['rotation'][0][1])],
            continuous_range_pitch=[math.radians(continuous_ranges['rotation'][1][0]), math.radians(continuous_ranges['rotation'][1][1])],
            continuous_range_yaw=[math.radians(continuous_ranges['rotation'][2][0]), math.radians(continuous_ranges['rotation'][2][1])],
            
            # åŸºç¡€æŸå¤±æƒé‡
            base_classification_weight=base_weights['classification'],
            iou_weight=base_weights['iou'],
            delta_weight=base_weights['delta'],
            eos_weight=base_weights['eos'],
            
            # è‡ªé€‚åº”æƒé‡å‚æ•°
            adaptive_classification=adaptive_weights['adaptive_classification'],
            adaptive_delta=adaptive_weights['adaptive_delta'],
            min_classification_weight=adaptive_weights['classification_range']['min'],
            max_classification_weight=adaptive_weights['classification_range']['max'],
            min_delta_weight=adaptive_weights['delta_range']['min'],
            max_delta_weight=adaptive_weights['delta_range']['max'],
            iou_threshold_high=adaptive_weights['thresholds']['high'],
            iou_threshold_low=adaptive_weights['thresholds']['low'],
            
            # æ•°æ®å¤„ç†å‚æ•°
            pad_id=data_processing['pad_id'],
            label_smoothing=data_processing['label_smoothing'],
            
            # ç®—æ³•å‚æ•°
            distance_aware_cls=algorithm_config['distance_aware']['enabled'],
            distance_alpha=algorithm_config['distance_aware']['alpha'],
            focal_gamma=algorithm_config['focal']['gamma']
        )
    
    def _prepare_targets_with_equivalent_boxes(self, batch, outputs):
        """
        ä½¿ç”¨ç­‰ä»·è¡¨ç¤ºä¼˜åŒ–ç›®æ ‡æ•°æ®ï¼Œé€‰æ‹©æ—‹è½¬L1 lossæœ€å°çš„è¡¨ç¤º
        
        Args:
            batch: è¾“å…¥batchæ•°æ®
            outputs: æ¨¡å‹è¾“å‡º
            
        Returns:
            targets: ä¼˜åŒ–åçš„ç›®æ ‡æ•°æ®
        """
        batch_size = batch['x'].shape[0]
        device = batch['x'].device
        
        # åˆå§‹åŒ–ç›®æ ‡æ•°æ®ï¼ˆbatchå·²ç»åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šï¼‰
        targets = {
            'x': batch['x'].clone(),
            'y': batch['y'].clone(),
            'z': batch['z'].clone(),
            'w': batch['w'].clone(),
            'h': batch['h'].clone(),
            'l': batch['l'].clone(),
            'roll': batch['roll'].clone(),
            'pitch': batch['pitch'].clone(),
            'yaw': batch['yaw'].clone(),
        }
        
        # å¦‚æœæœ‰ç­‰ä»·è¡¨ç¤ºï¼Œè¿›è¡Œä¼˜åŒ–
        if 'equivalent_boxes' in batch:
            equivalent_boxes = batch['equivalent_boxes']
            
            # å¯¹æ¯ä¸ªæ ·æœ¬çš„æ¯ä¸ªboxè¿›è¡Œä¼˜åŒ–
            for b in range(batch_size):
                for s in range(batch['x'].shape[1]):
                    # è·³è¿‡padding
                    if batch['x'][b, s] == self.pad_id:
                        continue
                    
                    # æ„å»ºé¢„æµ‹boxï¼ˆä¿æŒå¼ é‡æ ¼å¼ï¼Œé¿å…è®¾å¤‡ä¸åŒ¹é…ï¼‰
                    pred_box_tensor = torch.stack([
                        outputs['continuous_dict']['x_continuous'][b, s],
                        outputs['continuous_dict']['y_continuous'][b, s],
                        outputs['continuous_dict']['z_continuous'][b, s],
                        outputs['continuous_dict']['l_continuous'][b, s],
                        outputs['continuous_dict']['w_continuous'][b, s],
                        outputs['continuous_dict']['h_continuous'][b, s],
                        outputs['continuous_dict']['roll_continuous'][b, s],
                        outputs['continuous_dict']['pitch_continuous'][b, s],
                        outputs['continuous_dict']['yaw_continuous'][b, s],
                    ], dim=0)  # [9]
                    
                    # è½¬æ¢ä¸ºPythonåˆ—è¡¨ç”¨äºç­‰ä»·boxé€‰æ‹©
                    pred_box = pred_box_tensor.detach().cpu().numpy().tolist()
                    
                    # ç¡®ä¿pred_boxæ˜¯æ‰å¹³çš„åˆ—è¡¨ï¼Œä¸æ˜¯åµŒå¥—åˆ—è¡¨
                    if isinstance(pred_box[6], list):
                        pred_box = [item[0] if isinstance(item, list) else item for item in pred_box]
                    
                    # è·å–è¯¥boxçš„ç­‰ä»·è¡¨ç¤º
                    if s < len(equivalent_boxes[b]):
                        equiv_boxes = equivalent_boxes[b][s]
                        
                        # é€‰æ‹©æœ€ä¼˜çš„ç­‰ä»·è¡¨ç¤º
                        best_box, min_loss = select_best_equivalent_representation(pred_box, equiv_boxes)
                        
                        # æ›´æ–°ç›®æ ‡æ•°æ®ï¼ˆç¡®ä¿åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šï¼‰
                        device = targets['x'].device
                        targets['x'][b, s] = torch.tensor(best_box[0], device=device, dtype=targets['x'].dtype)
                        targets['y'][b, s] = torch.tensor(best_box[1], device=device, dtype=targets['y'].dtype)
                        targets['z'][b, s] = torch.tensor(best_box[2], device=device, dtype=targets['z'].dtype)
                        targets['l'][b, s] = torch.tensor(best_box[3], device=device, dtype=targets['l'].dtype)
                        targets['w'][b, s] = torch.tensor(best_box[4], device=device, dtype=targets['w'].dtype)
                        targets['h'][b, s] = torch.tensor(best_box[5], device=device, dtype=targets['h'].dtype)
                        targets['roll'][b, s] = torch.tensor(best_box[6], device=device, dtype=targets['roll'].dtype)
                        targets['pitch'][b, s] = torch.tensor(best_box[7], device=device, dtype=targets['pitch'].dtype)
                        targets['yaw'][b, s] = torch.tensor(best_box[8], device=device, dtype=targets['yaw'].dtype)
        
        return targets
    
    def _forward_with_sampling_strategy(self, batch: Dict, teacher_forcing_ratio: float) -> Dict:
        """æ ¹æ®é‡‡æ ·ç­–ç•¥è¿›è¡Œå‰å‘ä¼ æ’­
        
        Args:
            batch: è¾“å…¥batchæ•°æ®
            teacher_forcing_ratio: é‡‡æ ·æ¯”ä¾‹
                - 1.0: çº¯Teacher Forcing (100% GT)
                - 0.0~1.0: Scheduled Sampling (éƒ¨åˆ†GT + éƒ¨åˆ†é¢„æµ‹)
                - 0.0: çº¯Generation (100% é¢„æµ‹)
        """
        rgbxyz = batch['image'].to(self.device)  # [B, 6, H, W]
        targets = {
            'x': batch['x'].to(self.device),
            'y': batch['y'].to(self.device),
            'z': batch['z'].to(self.device),
            'w': batch['w'].to(self.device),
            'h': batch['h'].to(self.device),
            'l': batch['l'].to(self.device),
            'roll': batch['roll'].to(self.device),
            'pitch': batch['pitch'].to(self.device),
            'yaw': batch['yaw'].to(self.device),
        }
        
        # è·å–æ¨¡å‹
        if hasattr(self.model, 'module'):
            model = self.model.module
        else:
            model = self.model
        
        # æ ¹æ®teacher_forcing_ratioé€‰æ‹©ä¸åŒçš„ç­–ç•¥
        if teacher_forcing_ratio >= 1.0:
            # å®Œå…¨teacher forcingï¼šç›´æ¥ç”¨GT
            inputs = targets
            # ç»Ÿä¸€ä½¿ç”¨forward_with_predictions - 3å±æ€§æ ¼å¼
            # æ„å»º3å±æ€§å¼ é‡
            position = torch.stack([inputs['x'], inputs['y'], inputs['z']], dim=-1)  # [B, seq_len, 3]
            rotation = torch.stack([inputs['roll'], inputs['pitch'], inputs['yaw']], dim=-1)  # [B, seq_len, 3]
            size = torch.stack([inputs['l'], inputs['w'], inputs['h']], dim=-1)  # [B, seq_len, 3]
            
            outputs = model.forward_with_predictions(
                position=position,
                rotation=rotation,
                size=size,
                image=rgbxyz
            )
            return outputs
        elif teacher_forcing_ratio == 0.0:
            return self._forward_with_pure_generation(rgbxyz, targets, model)
        else:
            return self._forward_with_scheduled_sampling(rgbxyz, targets, teacher_forcing_ratio, model)
        
        
    
    def _forward_with_scheduled_sampling(self, rgbxyz: torch.Tensor, targets: Dict, teacher_forcing_ratio: float, model) -> Dict:
        """Scheduled Samplingå®ç° - æ”¯æŒæ¢¯åº¦ä¼ æ’­"""
        batch_size = rgbxyz.size(0)
        seq_len = targets['x'].size(1)
        device = rgbxyz.device
        
        # ===== ç¬¬ä¸€æ¬¡æ¨ç†ï¼šTeacher Forcingæ¨¡å¼ï¼ˆä¸éœ€è¦æ¢¯åº¦ï¼‰ =====
        # ä½¿ç”¨Ground Truthä½œä¸ºè¾“å…¥ï¼Œå¾—åˆ°æ¨¡å‹çš„é¢„æµ‹ç»“æœ
        with torch.no_grad():
            # æ„å»º3å±æ€§å¼ é‡
            position = torch.stack([targets['x'], targets['y'], targets['z']], dim=-1)  # [B, seq_len, 3]
            rotation = torch.stack([targets['roll'], targets['pitch'], targets['yaw']], dim=-1)  # [B, seq_len, 3]
            size = torch.stack([targets['l'], targets['w'], targets['h']], dim=-1)  # [B, seq_len, 3]
            
            predicted_output = model.forward_with_predictions(
                position=position,
                rotation=rotation,
                size=size,
                image=rgbxyz
            )
        
        # ä»é¢„æµ‹è¾“å‡ºä¸­æå–è¿ç»­å€¼
        continuous_predictions = predicted_output['continuous_dict']
        
        # è®¡ç®—åºåˆ—é•¿åº¦å’Œåˆ›å»ºmask
        sequence_lengths = self._compute_sequence_lengths(targets)
        
        # æ„å»ºæ··åˆè¾“å…¥åºåˆ—ï¼ˆä¿æŒæ¢¯åº¦ï¼‰
        mixed_inputs = {}
        for attr in ['x', 'y', 'z', 'w', 'h', 'l', 'roll', 'pitch', 'yaw']:
            continuous_pred = continuous_predictions[f'{attr}_continuous']  # [B, seq_len]
            # ç¡®ä¿ç»´åº¦åŒ¹é…GT
            target_seq_len = targets[attr].shape[1]
            if continuous_pred.shape[1] > target_seq_len:
                continuous_pred = continuous_pred[:, :target_seq_len]
            elif continuous_pred.shape[1] < target_seq_len:
                # å¡«å……åˆ°ç›®æ ‡é•¿åº¦
                padding = torch.zeros(
                    continuous_pred.shape[0], 
                    target_seq_len - continuous_pred.shape[1], 
                    device=device,
                    requires_grad=True
                )
                continuous_pred = torch.cat([continuous_pred, padding], dim=1)
            
            # åˆå§‹åŒ–æ··åˆè¾“å…¥
            mixed_inputs[attr] = targets[attr].clone().float()  # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
            
            # åªåœ¨æœ‰æ•ˆä½ç½®è¿›è¡Œteacher forcing vs é¢„æµ‹çš„é€‰æ‹©
            for b in range(batch_size):
                seq_len_b = sequence_lengths[b].item()
                if seq_len_b > 0:
                    # åœ¨æœ‰æ•ˆä½ç½®éšæœºé€‰æ‹©ä½¿ç”¨GTè¿˜æ˜¯é¢„æµ‹
                    use_gt = torch.rand(seq_len_b, device=device) < teacher_forcing_ratio
                    
                    # åˆ›å»ºæ–°çš„åºåˆ—è€Œä¸æ˜¯åŸåœ°ä¿®æ”¹
                    new_sequence = mixed_inputs[attr][b].clone()
                    for pos in range(seq_len_b):
                        if use_gt[pos]:
                            new_sequence[pos] = targets[attr][b, pos]
                        else:
                            new_sequence[pos] = continuous_pred[b, pos]
                    # ä½¿ç”¨catæ“ä½œæ¥é¿å…åŸåœ°ä¿®æ”¹
                    mixed_inputs[attr] = torch.cat([
                        mixed_inputs[attr][:b],
                        new_sequence.unsqueeze(0),
                        mixed_inputs[attr][b+1:]
                    ], dim=0)
        
        # ===== ä½¿ç”¨æ··åˆåºåˆ—è¿›è¡Œå‰å‘ä¼ æ’­ï¼ˆä¿æŒæ¢¯åº¦ï¼‰ =====
        # æ„å»º3å±æ€§å¼ é‡
        position = torch.stack([mixed_inputs['x'], mixed_inputs['y'], mixed_inputs['z']], dim=-1)  # [B, seq_len, 3]
        rotation = torch.stack([targets['roll'], targets['pitch'], targets['yaw']], dim=-1)  # [B, seq_len, 3]
        size = torch.stack([mixed_inputs['l'], mixed_inputs['w'], mixed_inputs['h']], dim=-1)  # [B, seq_len, 3]
        
        return model.forward_with_predictions(
            position=position,
            rotation=rotation,
            size=size,
            image=rgbxyz
        )
    

    def _forward_with_pure_generation(self, rgbxyz: torch.Tensor, targets: Dict, model) -> Dict:
        """Pure Generationè®­ç»ƒ - æ”¯æŒæ¢¯åº¦ä¼ æ’­çš„å¢é‡ç”Ÿæˆ"""
        batch_size = rgbxyz.size(0)
        seq_len = targets['x'].size(1)
        device = rgbxyz.device
        
        # ===== ä½¿ç”¨æ”¯æŒæ¢¯åº¦çš„å¢é‡ç”Ÿæˆ =====
        return self._forward_with_gradient_preserving_generation(
            model, rgbxyz, targets, seq_len, device
        )
    

    def _forward_with_gradient_preserving_generation(self, model, rgbxyz: torch.Tensor, targets: Dict, seq_len: int, device: torch.device) -> Dict:
        """
        æ”¯æŒæ¢¯åº¦çš„å¢é‡ç”Ÿæˆ - ä½¿ç”¨çœŸæ­£çš„å¢é‡è§£ç 
        
        è¿™ä¸ªç‰ˆæœ¬ä½¿ç”¨ç±»ä¼¼ generate_next_box_incremental çš„é€»è¾‘ï¼Œä½†ä¿æŒæ¢¯åº¦æµåŠ¨
        """
        batch_size = rgbxyz.size(0)
        
        # 1. ç¼–ç å›¾åƒï¼ˆåªè®¡ç®—ä¸€æ¬¡ï¼‰
        image_embed = model.image_encoder(rgbxyz)
        
        # ğŸ”§ ä¿®å¤Bugï¼šæ·»åŠ 2Dä½ç½®ç¼–ç ï¼ˆä¸æ¨ç†ä»£ç ä¿æŒä¸€è‡´ï¼‰
        H = W = int(np.sqrt(image_embed.shape[1]))
        if H * W == image_embed.shape[1]:
            from primitive_anything_3d import build_2d_sine_positional_encoding
            pos_embed_2d = build_2d_sine_positional_encoding(H, W, image_embed.shape[-1])
            pos_embed_2d = pos_embed_2d.flatten(0, 1).unsqueeze(0).to(image_embed.device)
            image_embed = image_embed + pos_embed_2d
        
        image_cond = None
        if model.condition_on_image and model.image_film_cond is not None:
            pooled_image_embed = image_embed.mean(dim=1)
            image_cond = model.image_cond_proj_film(pooled_image_embed)
        
        # 2. åˆå§‹åŒ–åºåˆ—çŠ¶æ€
        from einops import repeat
        current_sequence = repeat(model.sos_token, 'n d -> b n d', b=batch_size)
        
        # 3. åˆå§‹åŒ–ç¼“å­˜ï¼ˆç”¨äºçœŸæ­£çš„å¢é‡è§£ç ï¼‰
        decoder_cache = None
        gateloop_cache = []
        
        # å­˜å‚¨æ¯ä¸€æ­¥çš„è¾“å‡º
        all_logits = {f'{attr}_logits': [] for attr in ['x', 'y', 'z', 'w', 'h', 'l', 'roll', 'pitch', 'yaw']}
        all_deltas = {f'{attr}_delta': [] for attr in ['x', 'y', 'z', 'w', 'h', 'l', 'roll', 'pitch', 'yaw']}
        all_continuous = {f'{attr}_continuous': [] for attr in ['x', 'y', 'z', 'w', 'h', 'l', 'roll', 'pitch', 'yaw']}
        all_eos_logits = []
        
        # 4. é€æ­¥ç”Ÿæˆï¼Œä½¿ç”¨çœŸæ­£çš„å¢é‡è§£ç 
        for step in range(seq_len):
            current_len = current_sequence.shape[1]
            if step == 0:
                # ç¬¬ä¸€æ­¥ï¼šå®Œæ•´å‰å‘ä¼ æ’­ï¼Œåˆå§‹åŒ–ç¼“å­˜
                primitive_codes = current_sequence
                
                # æ·»åŠ ä½ç½®ç¼–ç 
                pos_embed = model.pos_embed[:, :current_len, :]
                primitive_codes = primitive_codes + pos_embed
                
                # å›¾åƒæ¡ä»¶åŒ–
                if image_cond is not None:
                    primitive_codes = model.image_film_cond(primitive_codes, image_cond)
                
                # é—¨æ§å¾ªç¯å—ï¼ˆåˆå§‹åŒ–ç¼“å­˜ï¼‰
                if model.gateloop_block is not None:
                    primitive_codes, gateloop_cache = model.gateloop_block(primitive_codes, cache=None)
                
                # Transformerè§£ç ï¼ˆåˆå§‹åŒ–decoderç¼“å­˜ï¼‰
                attended_codes, decoder_cache = model.decoder(
                    primitive_codes,
                    context=image_embed,
                    cache=None,
                    return_hiddens=True
                )
            else:
                # åç»­æ­¥éª¤ï¼šåªå¤„ç†æ–°tokenï¼ˆçœŸæ­£çš„å¢é‡ï¼ï¼‰
                new_token = current_sequence[:, -1:, :]  # åªæœ‰æœ€æ–°çš„token
                
                # æ·»åŠ ä½ç½®ç¼–ç ï¼ˆåªå¯¹æ–°tokenï¼‰
                pos_embed = model.pos_embed[:, current_len-1:current_len, :]
                primitive_codes = new_token + pos_embed
                
                # å›¾åƒæ¡ä»¶åŒ–ï¼ˆåªå¯¹æ–°tokenï¼‰
                if image_cond is not None:
                    primitive_codes = model.image_film_cond(primitive_codes, image_cond)
                
                # é—¨æ§å¾ªç¯å—å¢é‡è®¡ç®—
                if model.gateloop_block is not None:
                    primitive_codes, gateloop_cache = model.gateloop_block(
                        primitive_codes, 
                        cache=gateloop_cache
                    )
                
                # çœŸæ­£çš„å¢é‡Transformerè§£ç ï¼ï¼ˆä¿æŒæ¢¯åº¦ï¼‰
                attended_codes, decoder_cache = model.decoder(
                    primitive_codes,
                    context=image_embed,
                    cache=decoder_cache,
                    return_hiddens=True
                )
            
            # é¢„æµ‹ä¸‹ä¸€ä¸ªtokenï¼ˆåªéœ€è¦æœ€åä¸€ä¸ªä½ç½®ï¼‰
            step_embed = attended_codes[:, -1, :]
            
            # é¢„æµ‹3ä¸ªå±æ€§ï¼ˆä¿æŒæ¢¯åº¦ï¼Œä½¿ç”¨Gumbel Softmaxï¼‰
            gumbel_temp = self.incremental_temperature
            
            # é¢„æµ‹ä½ç½®å±æ€§ (x, y, z)
            pos_result = model.predict_3d_vector_with_continuous_embed(
                step_embed, 'position', prev_embeds=None, use_gumbel=True, temperature=gumbel_temp
            )
            pos_logits = pos_result['logits']
            pos_deltas = pos_result['deltas'] 
            pos_continuous = pos_result['continuous']
            pos_embeds = [pos_result['embed']]
            
            # é¢„æµ‹æ—‹è½¬å±æ€§ (roll, pitch, yaw)
            rot_result = model.predict_3d_vector_with_continuous_embed(
                step_embed, 'rotation', prev_embeds=pos_embeds, use_gumbel=True, temperature=gumbel_temp
            )
            rot_logits = rot_result['logits']
            rot_deltas = rot_result['deltas']
            rot_continuous = rot_result['continuous']
            rot_embeds = [rot_result['embed']]
            
            # é¢„æµ‹å°ºå¯¸å±æ€§ (w, h, l)
            size_result = model.predict_3d_vector_with_continuous_embed(
                step_embed, 'size', prev_embeds=pos_embeds + rot_embeds, use_gumbel=True, temperature=gumbel_temp
            )
            size_logits = size_result['logits']
            size_deltas = size_result['deltas']
            size_continuous = size_result['continuous'] 
            size_embeds = [size_result['embed']]
            
            # å°†3Då‘é‡åˆ†è§£ä¸ºå•ç‹¬å±æ€§
            x_continuous, y_continuous, z_continuous = pos_continuous[:, 0], pos_continuous[:, 1], pos_continuous[:, 2]
            w_continuous, h_continuous, l_continuous = size_continuous[:, 0], size_continuous[:, 1], size_continuous[:, 2]
            roll_continuous, pitch_continuous, yaw_continuous = rot_continuous[:, 0], rot_continuous[:, 1], rot_continuous[:, 2]
            
            # EOSé¢„æµ‹ - éœ€è¦æ‰€æœ‰å±æ€§çš„embedding
            eos_input = torch.cat([step_embed] + pos_embeds + rot_embeds + size_embeds, dim=-1)
            eos_logits = model.to_eos_logits(eos_input).squeeze(-1)
            
            # ä¿å­˜è¿™ä¸€æ­¥çš„è¾“å‡º
            # ä½ç½®å±æ€§ (x, y, z) - pos_logitsæ˜¯[B, sum(num_bins)]å½¢çŠ¶ï¼Œéœ€è¦æŒ‰ç»´åº¦åˆ†å‰²
            num_bins = model.num_discrete_position
            x_logits = pos_logits[:, 0:num_bins]  # [B, num_bins]
            y_logits = pos_logits[:, num_bins:2*num_bins]  # [B, num_bins]
            z_logits = pos_logits[:, 2*num_bins:3*num_bins]  # [B, num_bins]
            
            all_logits['x_logits'].append(x_logits)
            all_logits['y_logits'].append(y_logits)
            all_logits['z_logits'].append(z_logits)
            
            all_deltas['x_delta'].append(pos_deltas[:, 0])
            all_deltas['y_delta'].append(pos_deltas[:, 1])
            all_deltas['z_delta'].append(pos_deltas[:, 2])
            
            all_continuous['x_continuous'].append(x_continuous)
            all_continuous['y_continuous'].append(y_continuous)
            all_continuous['z_continuous'].append(z_continuous)
            
            # å°ºå¯¸å±æ€§ (w, h, l) - size_logitsæ˜¯[B, sum(num_bins)]å½¢çŠ¶ï¼Œéœ€è¦æŒ‰ç»´åº¦åˆ†å‰²
            size_num_bins = model.num_discrete_size
            w_logits = size_logits[:, 0:size_num_bins]  # [B, num_bins]
            h_logits = size_logits[:, size_num_bins:2*size_num_bins]  # [B, num_bins]
            l_logits = size_logits[:, 2*size_num_bins:3*size_num_bins]  # [B, num_bins]
            
            all_logits['w_logits'].append(w_logits)
            all_logits['h_logits'].append(h_logits)
            all_logits['l_logits'].append(l_logits)
            
            all_deltas['w_delta'].append(size_deltas[:, 0])
            all_deltas['h_delta'].append(size_deltas[:, 1])
            all_deltas['l_delta'].append(size_deltas[:, 2])
            
            all_continuous['w_continuous'].append(w_continuous)
            all_continuous['h_continuous'].append(h_continuous)
            all_continuous['l_continuous'].append(l_continuous)
            
            # æ—‹è½¬å±æ€§ (roll, pitch, yaw) - rot_logitsæ˜¯[B, sum(num_bins)]å½¢çŠ¶ï¼Œéœ€è¦æŒ‰ç»´åº¦åˆ†å‰²
            rot_num_bins = model.num_discrete_rotation
            roll_logits = rot_logits[:, 0:rot_num_bins]  # [B, num_bins]
            pitch_logits = rot_logits[:, rot_num_bins:2*rot_num_bins]  # [B, num_bins]
            yaw_logits = rot_logits[:, 2*rot_num_bins:3*rot_num_bins]  # [B, num_bins]
            
            all_logits['roll_logits'].append(roll_logits)
            all_logits['pitch_logits'].append(pitch_logits)
            all_logits['yaw_logits'].append(yaw_logits)
            
            all_deltas['roll_delta'].append(rot_deltas[:, 0])
            all_deltas['pitch_delta'].append(rot_deltas[:, 1])
            all_deltas['yaw_delta'].append(rot_deltas[:, 2])
            
            all_continuous['roll_continuous'].append(roll_continuous)
            all_continuous['pitch_continuous'].append(pitch_continuous)
            all_continuous['yaw_continuous'].append(yaw_continuous)
            
            all_eos_logits.append(eos_logits)
            
            # æ„å»ºä¸‹ä¸€æ­¥çš„è¾“å…¥ï¼šå½“å‰token + é¢„æµ‹çš„è¿ç»­å€¼
            # ç°åœ¨æˆ‘ä»¬æœ‰3Då‘é‡çš„embedsï¼Œç›´æ¥ç»„åˆï¼ˆé¡ºåºï¼šä½ç½®+æ—‹è½¬+å°ºå¯¸ï¼‰
            combined_embeds = pos_embeds + rot_embeds + size_embeds
            combined_embed = torch.cat(combined_embeds, dim=-1)  # [B, total_embed_dim]
            projected_embed = model.project_in(combined_embed).unsqueeze(1)  # [B, 1, model_dim]
            
            # æ›´æ–°å½“å‰åºåˆ—ï¼ˆä¿æŒæ¢¯åº¦ï¼‰
            current_sequence = torch.cat([current_sequence, projected_embed], dim=1)
        
        # 5. ç»„åˆæ‰€æœ‰è¾“å‡º
        result = {
            'logits_dict': {},
            'delta_dict': {},
            'continuous_dict': {},
            'eos_logits': torch.stack(all_eos_logits, dim=1)  # [B, seq_len]
        }
        
        for attr in ['x', 'y', 'z', 'w', 'h', 'l', 'roll', 'pitch', 'yaw']:
            result['logits_dict'][f'{attr}_logits'] = torch.stack(all_logits[f'{attr}_logits'], dim=1)
            result['delta_dict'][f'{attr}_delta'] = torch.stack(all_deltas[f'{attr}_delta'], dim=1)  
            result['continuous_dict'][f'{attr}_continuous'] = torch.stack(all_continuous[f'{attr}_continuous'], dim=1)
        
        return result
    
 
    
    def _compute_sequence_lengths(self, targets_dict: Dict[str, torch.Tensor]) -> torch.Tensor:
        """è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„çœŸå®åºåˆ—é•¿åº¦"""
        # ä½¿ç”¨xåæ ‡æ¥è®¡ç®—åºåˆ—é•¿åº¦ï¼ˆépaddingçš„ä½ç½®æ•°é‡ï¼‰
        x_targets = targets_dict['x']  # [B, max_boxes]
        batch_size = x_targets.shape[0]
        
        sequence_lengths = []
        for b in range(batch_size):
            # è®¡ç®—épaddingå€¼çš„æ•°é‡ï¼ˆå‡è®¾paddingå€¼ä¸º-1ï¼‰
            valid_mask = x_targets[b] != -1
            seq_len = valid_mask.sum().item()
            sequence_lengths.append(seq_len)
        
        return torch.tensor(sequence_lengths, device=x_targets.device)
    
    def _train_epoch(self, phase: TrainingPhase, epoch_in_phase: int, loss_fn) -> TrainingStats:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        if self.train_sampler:
            self.train_sampler.set_epoch(self.current_epoch)
        
        # è®¡ç®—å½“å‰teacher forcingæ¯”ä¾‹
        teacher_forcing_ratio = self._compute_teacher_forcing_ratio(phase, epoch_in_phase)
        
        total_loss = 0.0
        total_cls_loss = 0.0
        total_iou_loss = 0.0
        total_delta_loss = 0.0
        total_eos_loss = 0.0
        total_mean_iou = 0.0
        total_adaptive_cls_weight = 0.0
        total_adaptive_delta_weight = 0.0
        
        # ğŸ” åˆå§‹åŒ–æ—‹è½¬è§’åº¦æŸå¤±ç»Ÿè®¡
        total_roll_cls_loss = 0.0
        total_pitch_cls_loss = 0.0
        total_yaw_cls_loss = 0.0
        total_roll_delta_loss = 0.0
        total_pitch_delta_loss = 0.0
        total_yaw_delta_loss = 0.0
        
        num_batches = 0
        
        start_time = time.time()
        
        for batch_idx, batch in enumerate(self.train_loader):
            self.optimizer.zero_grad()
            
            # å°†batchç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
            batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}
            
            with autocast(enabled=self.use_amp):
                # å‰å‘ä¼ æ’­
                outputs = self._forward_with_sampling_strategy(batch, teacher_forcing_ratio)
                
                # å‡†å¤‡ç›®æ ‡æ•°æ® - ä½¿ç”¨ç­‰ä»·è¡¨ç¤ºä¼˜åŒ–
                targets = self._prepare_targets_with_equivalent_boxes(batch, outputs)
                
                # è®¡ç®—æŸå¤±
                sequence_lengths = self._compute_sequence_lengths(targets)
                loss_dict = loss_fn(
                    logits_dict=outputs['logits_dict'],
                    delta_dict=outputs['delta_dict'],
                    eos_logits=outputs['eos_logits'],
                    targets_dict=targets,
                    sequence_lengths=sequence_lengths
                )
                
                loss = loss_dict['total_loss']
            
            # åå‘ä¼ æ’­
            if self.use_amp:
                self.scaler.scale(loss).backward()
                # æ¢¯åº¦è£å‰ªï¼ˆæ··åˆç²¾åº¦ï¼‰
                if self.use_grad_clipping:
                    self.scaler.unscale_(self.optimizer)
                    grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                loss.backward()
                # æ¢¯åº¦è£å‰ªï¼ˆæ™®é€šç²¾åº¦ï¼‰
                if self.use_grad_clipping:
                    grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)
                self.optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            total_cls_loss += loss_dict['total_classification'].item()
            total_iou_loss += loss_dict['iou_loss'].item()
            total_delta_loss += loss_dict['total_delta'].item()
            total_eos_loss += loss_dict.get('eos_loss', torch.tensor(0.0)).item()
            total_mean_iou += loss_dict['mean_iou'].item()
            total_adaptive_cls_weight += loss_dict.get('adaptive_classification_weight', torch.tensor(0.0)).item()
            total_adaptive_delta_weight += loss_dict.get('adaptive_delta_weight', torch.tensor(0.0)).item()
            
            # ğŸ” æ·»åŠ æ—‹è½¬è§’åº¦æŸå¤±çš„å•ç‹¬ç»Ÿè®¡
            total_roll_cls_loss += loss_dict.get('roll_cls', torch.tensor(0.0)).item()
            total_pitch_cls_loss += loss_dict.get('pitch_cls', torch.tensor(0.0)).item()
            total_yaw_cls_loss += loss_dict.get('yaw_cls', torch.tensor(0.0)).item()
            total_roll_delta_loss += loss_dict.get('roll_delta', torch.tensor(0.0)).item()
            total_pitch_delta_loss += loss_dict.get('pitch_delta', torch.tensor(0.0)).item()
            total_yaw_delta_loss += loss_dict.get('yaw_delta', torch.tensor(0.0)).item()
            
            num_batches += 1
            
                    # æ—¥å¿—è®°å½• - åªåœ¨ä¸»è¿›ç¨‹æ‰“å°è¯¦ç»†æ—¥å¿—
        log_interval = self.config_loader.get_global_config()['logging']['log_interval']
        if log_interval > 0 and batch_idx % log_interval == 0 and self.is_main_process:
            eos_loss_val = loss_dict.get('eos_loss', torch.tensor(0.0)).item()
            print(f"Epoch {self.current_epoch} [{batch_idx}/{len(self.train_loader)}] "
                  f"Total: {loss.item():.4f} | "
                  f"Cls: {loss_dict['total_classification'].item():.4f} | "
                  f"IoU: {loss_dict['iou_loss'].item():.4f} | "
                  f"Delta: {loss_dict['total_delta'].item():.4f} | "
                  f"EOS: {eos_loss_val:.4f} | "
                  f"TF: {teacher_forcing_ratio:.3f}")
        
        epoch_time = time.time() - start_time
        
        # å­¦ä¹ ç‡è°ƒåº¦
        if self.scheduler:
            self.scheduler.step()
        
        # è¿”å›å¹³å‡ç»Ÿè®¡
        return TrainingStats(
            epoch=self.current_epoch,
            phase=phase.name,
            teacher_forcing_ratio=teacher_forcing_ratio,
            train_loss=total_loss / num_batches,
            train_classification_loss=total_cls_loss / num_batches,
            train_iou_loss=total_iou_loss / num_batches,
            train_delta_loss=total_delta_loss / num_batches,
            train_eos_loss=total_eos_loss / num_batches,
            train_mean_iou=total_mean_iou / num_batches,
            val_loss=0.0,  # ç¨åå¡«å……
            # ğŸ” æ·»åŠ æ—‹è½¬è§’åº¦æŸå¤±ç»Ÿè®¡
            train_roll_cls_loss=total_roll_cls_loss / num_batches,
            train_pitch_cls_loss=total_pitch_cls_loss / num_batches,
            train_yaw_cls_loss=total_yaw_cls_loss / num_batches,
            train_roll_delta_loss=total_roll_delta_loss / num_batches,
            train_pitch_delta_loss=total_pitch_delta_loss / num_batches,
            train_yaw_delta_loss=total_yaw_delta_loss / num_batches,
            val_generation_loss=0.0,
            val_mean_iou=0.0,
            val_generation_iou=0.0,
            learning_rate=self.optimizer.param_groups[0]['lr'],
            adaptive_cls_weight=total_adaptive_cls_weight / num_batches,
            adaptive_delta_weight=total_adaptive_delta_weight / num_batches,
            epoch_time=epoch_time
        )
    
    def _validate_epoch(self, loss_fn, phase: TrainingPhase) -> Dict[str, float]:
        """éªŒè¯ä¸€ä¸ªepochï¼Œè¿”å›è¯¦ç»†çš„lossç»„ä»¶"""
        self.model.eval()
        
        # Teacher forcingéªŒè¯ç»Ÿè®¡
        total_tf_loss = 0.0
        total_tf_cls_loss = 0.0
        total_tf_iou_loss = 0.0
        total_tf_delta_loss = 0.0
        total_tf_eos_loss = 0.0
        total_tf_iou = 0.0
        
        # ç”ŸæˆéªŒè¯ç»Ÿè®¡
        total_gen_loss = 0.0
        total_gen_iou = 0.0
        total_gen_cls_loss = 0.0
        total_gen_iou_loss = 0.0
        total_gen_delta_loss = 0.0
        total_gen_eos_loss = 0.0
        total_generated_boxes = 0.0
        total_gt_boxes = 0.0
        num_batches = 0

        total_x_error = 0.0
        total_y_error = 0.0
        total_z_error = 0.0
        total_w_error = 0.0
        total_h_error = 0.0
        total_l_error = 0.0
        total_roll_error = 0.0
        total_pitch_error = 0.0
        total_yaw_error = 0.0
        total_overall_error = 0.0
        
        # ä¿å­˜éªŒè¯æ ·æœ¬çš„æ¨ç†ç»“æœ
        validation_results = []
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(self.val_loader):
                # 1. Teacher forcingéªŒè¯ (ä¸è®­ç»ƒä¸€è‡´)
                tf_outputs = self._forward_with_sampling_strategy(batch, 1.0)  # å®Œå…¨teacher forcing
                
                targets = {
                    'x': batch['x'].to(self.device),
                    'y': batch['y'].to(self.device),
                    'z': batch['z'].to(self.device),
                    'w': batch['w'].to(self.device),
                    'h': batch['h'].to(self.device),
                    'l': batch['l'].to(self.device),
                    'roll': batch['roll'].to(self.device),
                    'pitch': batch['pitch'].to(self.device),
                    'yaw': batch['yaw'].to(self.device),
                }
                
                sequence_lengths = self._compute_sequence_lengths(targets)
                tf_loss_dict = loss_fn(
                    logits_dict=tf_outputs['logits_dict'],
                    delta_dict=tf_outputs['delta_dict'],
                    eos_logits=tf_outputs['eos_logits'],
                    targets_dict=targets,
                    sequence_lengths=sequence_lengths
                )
                
                # ç´¯ç§¯teacher forcing lossç»„ä»¶
                total_tf_loss += tf_loss_dict['total_loss'].item()
                total_tf_cls_loss += tf_loss_dict['total_classification'].item()
                total_tf_iou_loss += tf_loss_dict['iou_loss'].item()
                total_tf_delta_loss += tf_loss_dict['total_delta'].item()
                total_tf_eos_loss += tf_loss_dict.get('eos_loss', torch.tensor(0.0)).item()
                
                # è®¡ç®—çœŸæ­£çš„è¯„ä¼°IoU (ä½¿ç”¨ä¸ç”ŸæˆéªŒè¯ç›¸åŒçš„æ–¹æ³•)
                eval_iou = self._compute_tf_evaluation_iou(tf_outputs, targets)
                
                # 2. çº¯ç”ŸæˆéªŒè¯
                rgbxyz = batch['image'].to(self.device)
                
                if hasattr(self.model, 'module'):
                    model = self.model.module
                else:
                    model = self.model
                
                # æ ¹æ®é…ç½®é€‰æ‹©æ¨ç†æ–¹æ³•
                if self.use_incremental_inference:
                    # ä½¿ç”¨å¢é‡æ¨ç†è¿›è¡ŒéªŒè¯ç”Ÿæˆï¼ˆæ›´é«˜æ•ˆï¼‰
                    # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨é…ç½®ä¸­çš„æ­£ç¡®max_seq_lenï¼Œè€Œä¸æ˜¯ä¸å­˜åœ¨çš„max_primitive_len
                    max_len = self.config_loader.get_global_config()['max_seq_len']
                    gen_results = model.generate_incremental(
                        image=rgbxyz,
                        max_seq_len=max_len,
                        temperature=self.incremental_temperature,
                        eos_threshold=self.eos_threshold
                    )
                else:
                    # ä½¿ç”¨ä¼ ç»Ÿæ¨ç†æ–¹æ³•
                    # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨é…ç½®ä¸­çš„æ­£ç¡®max_seq_len
                    max_len = self.config_loader.get_global_config()['max_seq_len']
                    gen_results = model.generate(
                        image=rgbxyz,
                        max_seq_len=max_len,
                        temperature=1.0
                    )
                # print(f"gen_results: {gen_results}")
                
                # è®¡ç®—ç”Ÿæˆç»“æœçš„è¯¦ç»†æŸå¤±å’Œç»Ÿè®¡ä¿¡æ¯
                gen_metrics = self._compute_generation_metrics(gen_results, targets, loss_fn, verbose=False, equivalent_boxes=batch.get('equivalent_boxes'))
                
                # åˆ é™¤è¿™è¡Œé‡å¤çš„ç´¯ç§¯
                # total_tf_loss += tf_loss_dict['total_loss'].item()
                
                total_tf_iou += eval_iou  # ä½¿ç”¨è¯„ä¼°IoUè€Œä¸æ˜¯æŸå¤±IoU
                total_gen_iou += gen_metrics['iou']
                total_generated_boxes += gen_metrics['num_generated_boxes']
                total_gt_boxes += gen_metrics['num_gt_boxes']
                
                # ç´¯ç§¯ç»´åº¦è¯¯å·®
                total_x_error += gen_metrics.get('x_error', 0.0)
                total_y_error += gen_metrics.get('y_error', 0.0)
                total_z_error += gen_metrics.get('z_error', 0.0)
                total_w_error += gen_metrics.get('w_error', 0.0)
                total_h_error += gen_metrics.get('h_error', 0.0)
                total_l_error += gen_metrics.get('l_error', 0.0)
                total_roll_error += gen_metrics.get('roll_error', 0.0)
                total_pitch_error += gen_metrics.get('pitch_error', 0.0)
                total_yaw_error += gen_metrics.get('yaw_error', 0.0)
                total_overall_error += gen_metrics.get('overall_mean_error', 0.0)
                
                num_batches += 1
                
                # ä¿å­˜æŒ‡å®šæ ·æœ¬çš„éªŒè¯ç»“æœ
                if batch_idx in self.validation_samples and self.is_main_process:
                    validation_results.append({
                        'batch_idx': batch_idx,
                        'epoch': self.current_epoch,
                        'phase': phase.name,
                        'generation_results': gen_results,
                        'ground_truth': targets,
                        'tf_loss': tf_loss_dict['total_loss'].item(),
                        'gen_iou': gen_metrics['iou'],
                        'image_shape': rgbxyz.shape
                    })
        
        # ä¿å­˜éªŒè¯ç»“æœ - åªåœ¨ä¸»è¿›ç¨‹ä¿å­˜
        if validation_results and self.is_main_process:
            results_file = self.validation_dir / f"validation_epoch_{self.current_epoch:04d}.json"
            with open(results_file, 'w') as f:
                # å°†tensorè½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
                serializable_results = []
                for result in validation_results:
                    serializable_result = {}
                    for key, value in result.items():
                        if isinstance(value, torch.Tensor):
                            serializable_result[key] = value.detach().cpu().tolist()
                        elif isinstance(value, dict):
                            serializable_result[key] = {k: v.detach().cpu().tolist() if isinstance(v, torch.Tensor) else v 
                                                      for k, v in value.items()}
                        else:
                            serializable_result[key] = value
                    serializable_results.append(serializable_result)
                
                json.dump(serializable_results, f, indent=2)
        
        # è®¡ç®—å¹³å‡å€¼å¹¶è¿”å›è¯¦ç»†lossç»„ä»¶
        avg_tf_loss = total_tf_loss / num_batches if num_batches > 0 else 0.0
        avg_tf_cls_loss = total_tf_cls_loss / num_batches if num_batches > 0 else 0.0
        avg_tf_iou_loss = total_tf_iou_loss / num_batches if num_batches > 0 else 0.0
        avg_tf_delta_loss = total_tf_delta_loss / num_batches if num_batches > 0 else 0.0
        avg_tf_eos_loss = total_tf_eos_loss / num_batches if num_batches > 0 else 0.0
        avg_tf_iou = total_tf_iou / num_batches if num_batches > 0 else 0.0
        
        # ç§»é™¤è™šå‡çš„ç”ŸæˆæŸå¤±è®¡ç®—
        # avg_gen_loss = total_gen_loss / num_batches if num_batches > 0 else 0.0
        # avg_gen_cls_loss = total_gen_cls_loss / num_batches if num_batches > 0 else 0.0
        # avg_gen_iou_loss = total_gen_iou_loss / num_batches if num_batches > 0 else 0.0
        # avg_gen_delta_loss = total_gen_delta_loss / num_batches if num_batches > 0 else 0.0
        # avg_gen_eos_loss = total_gen_eos_loss / num_batches if num_batches > 0 else 0.0
        
        avg_gen_iou = total_gen_iou / num_batches if num_batches > 0 else 0.0
        avg_generated_boxes = total_generated_boxes / num_batches if num_batches > 0 else 0.0
        avg_gt_boxes = total_gt_boxes / num_batches if num_batches > 0 else 0.0
        
        # è®¡ç®—å¹³å‡è¯¯å·®
        avg_x_error = total_x_error / num_batches if num_batches > 0 else 0.0
        avg_y_error = total_y_error / num_batches if num_batches > 0 else 0.0
        avg_z_error = total_z_error / num_batches if num_batches > 0 else 0.0
        avg_w_error = total_w_error / num_batches if num_batches > 0 else 0.0
        avg_h_error = total_h_error / num_batches if num_batches > 0 else 0.0
        avg_l_error = total_l_error / num_batches if num_batches > 0 else 0.0
        avg_roll_error = total_roll_error / num_batches if num_batches > 0 else 0.0
        avg_pitch_error = total_pitch_error / num_batches if num_batches > 0 else 0.0
        avg_yaw_error = total_yaw_error / num_batches if num_batches > 0 else 0.0
        avg_overall_error = total_overall_error / num_batches if num_batches > 0 else 0.0
        
        # è¿”å›ç»“æœ - ç§»é™¤è™šå‡çš„æŸå¤±å€¼
        return {
            'tf_total_loss': avg_tf_loss,
            'tf_classification_loss': avg_tf_cls_loss,
            'tf_iou_loss': avg_tf_iou_loss,
            'tf_delta_loss': avg_tf_delta_loss,
            'tf_eos_loss': avg_tf_eos_loss,
            'tf_mean_iou': avg_tf_iou,
            'generation_iou': avg_gen_iou,
            'avg_generated_boxes': avg_generated_boxes,
            'avg_gt_boxes': avg_gt_boxes,
            'avg_x_error': avg_x_error,
            'avg_y_error': avg_y_error,
            'avg_z_error': avg_z_error,
            'avg_w_error': avg_w_error,
            'avg_h_error': avg_h_error,
            'avg_l_error': avg_l_error,
            'avg_roll_error': avg_roll_error,
            'avg_pitch_error': avg_pitch_error,
            'avg_yaw_error': avg_yaw_error,
            'avg_overall_error': avg_overall_error
        }
    
    def _compute_tf_evaluation_iou(self, outputs: Dict, targets: Dict, verbose: bool = False) -> float:
        """è®¡ç®—TFè¯„ä¼°IoUï¼Œä½¿ç”¨ä¸€å¯¹ä¸€åŒ¹é…ç­–ç•¥"""
        try:
            batch_size = targets['x'].size(0)
            total_iou = 0.0
            valid_samples = 0
            
            for b in range(batch_size):
                # æ„å»ºé¢„æµ‹çš„3D boxes
                pred_boxes = []
                pred_rotations = []
                gt_boxes = []
                gt_rotations = []
                
                # è·å–æ¯ä¸ªä½ç½®çš„é¢„æµ‹å€¼
                seq_len = targets['x'].size(1)
                for s in range(seq_len):
                    # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆä½ç½®ï¼ˆépaddingï¼‰
                    if targets['x'][b, s].item() != -1.0:
                        # å°†logitså’Œdeltaè½¬æ¢ä¸ºè¿ç»­é¢„æµ‹å€¼
                        pred_box = []
                        pred_rot = []
                        
                        # ä½ç½®å’Œå°ºå¯¸é¢„æµ‹
                        for attr in ['x', 'y', 'z', 'w', 'h', 'l']:
                            if attr + '_logits' in outputs['logits_dict'] and attr + '_delta' in outputs['delta_dict']:
                                logits = outputs['logits_dict'][attr + '_logits'][b, s]  # [num_bins]
                                delta = outputs['delta_dict'][attr + '_delta'][b, s]     # scalar
                                continuous_val = self._get_continuous_prediction(logits, delta, attr)
                                pred_box.append(continuous_val)
                                
                        
                        # æ—‹è½¬é¢„æµ‹
                        for attr in ['roll', 'pitch', 'yaw']:
                            if attr + '_logits' in outputs['logits_dict'] and attr + '_delta' in outputs['delta_dict']:
                                logits = outputs['logits_dict'][attr + '_logits'][b, s]  # [num_bins]
                                delta = outputs['delta_dict'][attr + '_delta'][b, s]     # scalar
                                continuous_val = self._get_continuous_prediction(logits, delta, attr)
                                pred_rot.append(continuous_val)
                        
                        if len(pred_box) == 6 and len(pred_rot) == 3:
                            pred_boxes.append(pred_box)
                            pred_rotations.append(pred_rot)
                            
                            # å¯¹åº”çš„GT box
                            gt_box = [
                                targets['x'][b, s].cpu().item(),
                                targets['y'][b, s].cpu().item(),
                                targets['z'][b, s].cpu().item(),
                                targets['l'][b, s].cpu().item(),
                                targets['w'][b, s].cpu().item(),
                                targets['h'][b, s].cpu().item(),
                            ]
                            gt_boxes.append(gt_box)
                            
                            # GTæ—‹è½¬ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                            if 'roll' in targets and 'pitch' in targets and 'yaw' in targets:
                                gt_rot = [
                                    targets['roll'][b, s].cpu().item(),
                                    targets['pitch'][b, s].cpu().item(),
                                    targets['yaw'][b, s].cpu().item(),
                                ]  # [3] euler angles
                                gt_rotations.append(gt_rot)
                            else:
                                # ä½¿ç”¨é›¶æ—‹è½¬ï¼ˆæ— æ—‹è½¬ï¼‰
                                gt_rotations.append([0.0, 0.0, 0.0])
                
                # è®¡ç®—è¯¥æ ·æœ¬çš„IoU
                if pred_boxes and gt_boxes:
                    sample_ious = []
                    
                    # è®¡ç®—æ¯ä¸ªé¢„æµ‹boxä¸å¯¹åº”GT boxçš„IoU
                    for i, (pred_box, pred_rot, gt_box, gt_rot) in enumerate(zip(pred_boxes, pred_rotations, gt_boxes, gt_rotations)):
                        try:
                            # æ£€æŸ¥boxå°ºå¯¸
                            pred_size = np.array(pred_box[3:])  # [l, w, h]
                            gt_size = np.array(gt_box[3:])     # [l, w, h]
                            
                            if np.any(pred_size <= 0) or np.any(gt_size <= 0):
                                continue
                            
                            # ä½¿ç”¨OBB IoUè®¡ç®—
                            iou = self._compute_box_iou(pred_box, gt_box, pred_rot, gt_rot)
                            sample_ious.append(iou)
                            
                        except Exception as e:
                            sample_ious.append(0.0)
                    
                    if sample_ious:
                        sample_mean_iou = sum(sample_ious) / len(sample_ious)
                        total_iou += sample_mean_iou
                        valid_samples += 1
            
            if valid_samples == 0:
                return 0.0
            
            mean_iou = total_iou / valid_samples
            return float(mean_iou)
            
        except Exception as e:
            if verbose:
                print(f"âš ï¸  è®¡ç®—TFè¯„ä¼°IoUæ—¶å‡ºé”™: {e}")
            return 0.0
    
    def _get_continuous_prediction(self, logits: torch.Tensor, delta: torch.Tensor, attr: str) -> float:
        """å°†åˆ†ç±»logitså’Œdeltaç»„åˆæˆè¿ç»­é¢„æµ‹å€¼"""
        # è·å–å±æ€§çš„é…ç½®ï¼ˆä»ConfigLoaderè¿”å›çš„å¹³é“ºç»“æ„ä¸­è·å–ï¼‰
        attr_configs = {
            # ä½ç½®å±æ€§
            'x': (self.model_config.get('num_discrete_position', 64), 
                  self.model_config.get('continuous_range_position', [[0.5, 2.5], [-2.0, 2.0], [-1.5, 1.5]])[0]),
            'y': (self.model_config.get('num_discrete_position', 64), 
                  self.model_config.get('continuous_range_position', [[0.5, 2.5], [-2.0, 2.0], [-1.5, 1.5]])[1]),
            'z': (self.model_config.get('num_discrete_position', 64), 
                  self.model_config.get('continuous_range_position', [[0.5, 2.5], [-2.0, 2.0], [-1.5, 1.5]])[2]),
            # æ—‹è½¬å±æ€§
            'roll': (self.model_config.get('num_discrete_rotation', 64), 
                     self.model_config.get('continuous_range_rotation', [[-1.5708, 1.5708], [-1.5708, 1.5708], [-1.5708, 1.5708]])[0]),
            'pitch': (self.model_config.get('num_discrete_rotation', 64), 
                      self.model_config.get('continuous_range_rotation', [[-1.5708, 1.5708], [-1.5708, 1.5708], [-1.5708, 1.5708]])[1]),
            'yaw': (self.model_config.get('num_discrete_rotation', 64), 
                    self.model_config.get('continuous_range_rotation', [[-1.5708, 1.5708], [-1.5708, 1.5708], [-1.5708, 1.5708]])[2]),
            # å°ºå¯¸å±æ€§
            'w': (self.model_config.get('num_discrete_size', 64), 
                  self.model_config.get('continuous_range_size', [[0.1, 1.0], [0.1, 1.0], [0.1, 1.0]])[0]),
            'h': (self.model_config.get('num_discrete_size', 64), 
                  self.model_config.get('continuous_range_size', [[0.1, 1.0], [0.1, 1.0], [0.1, 1.0]])[1]),
            'l': (self.model_config.get('num_discrete_size', 64), 
                  self.model_config.get('continuous_range_size', [[0.1, 1.0], [0.1, 1.0], [0.1, 1.0]])[2])
        }
        
        num_bins, value_range = attr_configs[attr]
        min_val, max_val = value_range
        
        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„binå†…offseté€»è¾‘
        # è·å–åˆ†ç±»é¢„æµ‹ï¼ˆç¡®å®šæ€§é‡‡æ ·ï¼‰
        discrete_pred = torch.argmax(logits).item()
        
        # è®¡ç®—bin_widthå’Œè¿ç»­å€¼
        bin_width = (max_val - min_val) / (num_bins - 1)
        continuous_base = min_val + discrete_pred * bin_width
        
        # ç¡®ä¿deltaåœ¨CPUä¸Šï¼Œå¹¶æ­£ç¡®ç¼©æ”¾ä¸ºbinå†…offset
        if delta.is_cuda:
            delta_val = float(delta.cpu().detach()) * bin_width
        else:
            delta_val = float(delta.detach()) * bin_width
        
        return continuous_base + delta_val

    def _compute_generation_metrics(self, gen_results: Dict, targets: Dict, loss_fn, verbose: bool = False, equivalent_boxes: List = None) -> Dict[str, float]:
        """
        è®¡ç®—ç”Ÿæˆç»“æœçš„è¯¦ç»†æŒ‡æ ‡
        Args:
            gen_results: ç”Ÿæˆç»“æœå­—å…¸ï¼ˆå·²ç»æ˜¯è¿ç»­å€¼ï¼‰
            targets: ç›®æ ‡å€¼å­—å…¸
            loss_fn: æŸå¤±å‡½æ•°ï¼ˆä¸ä½¿ç”¨ï¼Œå› ä¸ºgen_resultsä¸æ˜¯æ¨¡å‹è¾“å‡ºæ ¼å¼ï¼‰
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        Returns:
            metrics: åŒ…å«å„é¡¹ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        try:
            # ç”Ÿæˆç»“æœå·²ç»æ˜¯è¿ç»­å€¼ï¼Œç›´æ¥ä½¿ç”¨
            processed_gen_results = {}
            target_seq_len = targets['x'].shape[1]  # GTçš„åºåˆ—é•¿åº¦
            
            for attr in ['x', 'y', 'z', 'w', 'h', 'l', 'roll', 'pitch', 'yaw']:
                if attr in gen_results:
                    # è·å–ç”Ÿæˆç»“æœï¼ˆå·²ç»æ˜¯è¿ç»­å€¼ï¼‰
                    gen_values = gen_results[attr]  # [B, seq_len]
                    
                    # å¯¹é½åºåˆ—é•¿åº¦
                    if gen_values.shape[1] > target_seq_len:
                        gen_values = gen_values[:, :target_seq_len]
                    elif gen_values.shape[1] < target_seq_len:
                        # ä¸å¡«å……ï¼Œä¿æŒåŸå§‹é•¿åº¦ï¼Œè®©IoUè®¡ç®—è‡ªå·±å¤„ç†é•¿åº¦ä¸åŒ¹é…
                        pass
                    
                    processed_gen_results[attr] = gen_values
                else:
                    # å¦‚æœæŸä¸ªå±æ€§ç¼ºå¤±ï¼Œè·³è¿‡è¯¥å±æ€§ï¼Œä¸è¿›è¡ŒIoUè®¡ç®—
                    # print(f"âš ï¸  ç”Ÿæˆç»“æœä¸­ç¼ºå°‘å±æ€§ {attr}ï¼Œè·³è¿‡è¯¥å±æ€§çš„IoUè®¡ç®—")
                    continue
            
            # è®¡ç®—IoU
            gen_iou = self._compute_generation_iou(processed_gen_results, targets, verbose, equivalent_boxes)
            
            # è®¡ç®—9ä¸ªç»´åº¦çš„å¹³å‡è¯¯å·®ï¼ˆåŒ…æ‹¬æ—‹è½¬è§’åº¦ï¼‰
            dimension_errors = {}
            total_valid_predictions = 0
            
            # ğŸ”§ æ–°å¢ï¼šå¦‚æœæœ‰ç­‰æ•ˆboxä¿¡æ¯ï¼Œå…ˆé€‰æ‹©æœ€ä¼˜ç­‰æ•ˆbox
            if equivalent_boxes is not None:
                # ä¸ºæ¯ä¸ªbatchå’Œsequenceä½ç½®é€‰æ‹©æœ€ä¼˜ç­‰æ•ˆbox
                batch_size = targets['x'].shape[0]
                seq_len = targets['x'].shape[1]
                
                # åˆ›å»ºæœ€ä¼˜ç­‰æ•ˆboxçš„ç›®æ ‡å€¼
                optimal_targets = {}
                for attr in ['x', 'y', 'z', 'w', 'h', 'l', 'roll', 'pitch', 'yaw']:
                    optimal_targets[attr] = targets[attr].clone()
                
                for b in range(batch_size):
                    for s in range(seq_len):
                        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆä½ç½®
                        if targets['x'][b, s].item() != -1.0:
                            # ğŸ”§ æ£€æŸ¥ç”Ÿæˆç»“æœåºåˆ—é•¿åº¦æ˜¯å¦è¶³å¤Ÿï¼ˆå¢é‡ç”Ÿæˆå¯èƒ½æå‰åœæ­¢ï¼‰
                            gen_seq_len = processed_gen_results['x'].shape[1]
                            if s >= gen_seq_len:
                                # å¢é‡ç”Ÿæˆæå‰åœæ­¢ï¼Œè·³è¿‡è¯¥ä½ç½®ï¼ˆè¿™æ˜¯æ­£å¸¸è¡Œä¸ºï¼‰
                                continue
                            
                            # ğŸ”§ æ£€æŸ¥ç­‰æ•ˆboxæ˜¯å¦å­˜åœ¨ä¸”ç´¢å¼•ä¸è¶Šç•Œ
                            if b < len(equivalent_boxes) and s < len(equivalent_boxes[b]) and len(equivalent_boxes[b][s]) > 0:
                                # æ„å»ºé¢„æµ‹box
                                pred_box = [
                                    processed_gen_results['x'][b, s].item(),
                                    processed_gen_results['y'][b, s].item(),
                                    processed_gen_results['z'][b, s].item(),
                                    processed_gen_results['l'][b, s].item(),
                                    processed_gen_results['w'][b, s].item(),
                                    processed_gen_results['h'][b, s].item(),
                                    processed_gen_results['roll'][b, s].item(),
                                    processed_gen_results['pitch'][b, s].item(),
                                    processed_gen_results['yaw'][b, s].item(),
                                ]
                                
                                # é€‰æ‹©æœ€ä¼˜ç­‰æ•ˆbox
                                equiv_boxes = equivalent_boxes[b][s]
                                best_box, min_loss = select_best_equivalent_representation(pred_box, equiv_boxes)
                            else:
                                # å¦‚æœç­‰æ•ˆboxä¸å­˜åœ¨æˆ–ç´¢å¼•è¶Šç•Œï¼Œè·³è¿‡è¯¥ä½ç½®
                                if b >= len(equivalent_boxes):
                                    # print(f"âš ï¸  ç­‰æ•ˆbox batchç´¢å¼•è¶Šç•Œ: batch={b}, equiv_boxes_len={len(equivalent_boxes)}")
                                    pass
                                elif s >= len(equivalent_boxes[b]):
                                    # print(f"âš ï¸  ç­‰æ•ˆboxåºåˆ—ç´¢å¼•è¶Šç•Œ: batch={b}, seq={s}, equiv_boxes_len={len(equivalent_boxes[b])}")
                                    pass
                                continue
                            
                            # ğŸ” æ·»åŠ è¯¦ç»†logï¼šæ‰“å°æ—‹è½¬è¯¯å·®è®¡ç®—è¿‡ç¨‹ï¼ˆå·²æ³¨é‡Šï¼‰
                            # print(f"ğŸ” æ—‹è½¬è¯¯å·®è®¡ç®— - Batch {b}, Box {s}:")
                            # print(f"   é¢„æµ‹box: pos=({pred_box[0]:.3f}, {pred_box[1]:.3f}, {pred_box[2]:.3f}), "
                            #       f"size=({pred_box[3]:.3f}, {pred_box[4]:.3f}, {pred_box[5]:.3f}), "
                            #       f"rot=({pred_box[6]:.3f}, {pred_box[7]:.3f}, {pred_box[8]:.3f})")
                            # print(f"   é¢„æµ‹è§’åº¦(åº¦): roll={math.degrees(pred_box[6]):.1f}Â°, "
                            #       f"pitch={math.degrees(pred_box[7]):.1f}Â°, "
                            #       f"yaw={math.degrees(pred_box[8]):.1f}Â°")
                            
                            # print(f"   GTç­‰æ•ˆboxæ•°é‡: {len(equiv_boxes)}")
                            # for i, equiv_box in enumerate(equiv_boxes):
                            #     print(f"     ç­‰æ•ˆbox {i+1}: pos=({equiv_box[0]:.3f}, {equiv_box[1]:.3f}, {equiv_box[2]:.3f}), "
                            #           f"size=({equiv_box[3]:.3f}, {equiv_box[4]:.3f}, {equiv_box[5]:.3f}), "
                            #           f"rot=({equiv_box[6]:.3f}, {equiv_box[7]:.3f}, {equiv_box[8]:.3f})")
                            #     print(f"     ç­‰æ•ˆbox {i+1}è§’åº¦(åº¦): roll={math.degrees(equiv_box[6]):.1f}Â°, "
                            #           f"pitch={math.degrees(equiv_box[7]):.1f}Â°, "
                            #           f"yaw={math.degrees(equiv_box[8]):.1f}Â°")
                            
                            # print(f"   é€‰æ‹©çš„æœ€ä¼˜ç­‰æ•ˆbox: pos=({best_box[0]:.3f}, {best_box[1]:.3f}, {best_box[2]:.3f}), "
                            #       f"size=({best_box[3]:.3f}, {best_box[4]:.3f}, {best_box[5]:.3f}), "
                            #       f"rot=({best_box[6]:.3f}, {best_box[7]:.3f}, {best_box[8]:.3f})")
                            # print(f"   æœ€ä¼˜ç­‰æ•ˆboxè§’åº¦(åº¦): roll={math.degrees(best_box[6]):.1f}Â°, "
                            #       f"pitch={math.degrees(best_box[7]):.1f}Â°, "
                            #       f"yaw={math.degrees(best_box[8]):.1f}Â°")
                            # print(f"   æœ€å°æ—‹è½¬loss: {min_loss:.6f}")
                            
                            # æ›´æ–°ç›®æ ‡å€¼
                            optimal_targets['x'][b, s] = best_box[0]
                            optimal_targets['y'][b, s] = best_box[1]
                            optimal_targets['z'][b, s] = best_box[2]
                            optimal_targets['l'][b, s] = best_box[3]
                            optimal_targets['w'][b, s] = best_box[4]
                            optimal_targets['h'][b, s] = best_box[5]
                            optimal_targets['roll'][b, s] = best_box[6]
                            optimal_targets['pitch'][b, s] = best_box[7]
                            optimal_targets['yaw'][b, s] = best_box[8]
                
                # ä½¿ç”¨æœ€ä¼˜ç­‰æ•ˆboxä½œä¸ºç›®æ ‡å€¼
                targets_to_use = optimal_targets
            else:
                # æ²¡æœ‰ç­‰æ•ˆboxä¿¡æ¯ï¼Œä½¿ç”¨åŸå§‹ç›®æ ‡å€¼
                targets_to_use = targets
            
            for attr in ['x', 'y', 'z', 'w', 'h', 'l', 'roll', 'pitch', 'yaw']:
                if attr in processed_gen_results and attr in targets_to_use:
                    # è·å–ç”Ÿæˆç»“æœå’Œç›®æ ‡å€¼
                    gen_values = processed_gen_results[attr]  # [B, seq_len]
                    gt_values = targets_to_use[attr]         # [B, seq_len]
                    
                    # åˆ›å»ºæœ‰æ•ˆmaskï¼ˆæ’é™¤paddingå€¼ï¼‰
                    valid_mask = (gt_values != -1.0) & (gt_values != 0.0)  # GTépaddingä¸”éé›¶
                    
                    if valid_mask.sum() > 0:
                        # è®¡ç®—æœ‰æ•ˆä½ç½®çš„ç»å¯¹è¯¯å·®
                        # ç¡®ä¿ä¸¤ä¸ªå¼ é‡å½¢çŠ¶åŒ¹é…
                        min_len = min(gen_values.shape[1], gt_values.shape[1])
                        gen_values_aligned = gen_values[:, :min_len]
                        gt_values_aligned = gt_values[:, :min_len]
                        valid_mask_aligned = valid_mask[:, :min_len]
                        
                        # ğŸ”§ ä¿®å¤ï¼šå¯¹äºæ—‹è½¬è§’åº¦ï¼Œä½¿ç”¨å‘¨æœŸæ€§è¯¯å·®è®¡ç®—
                        if attr in ['roll', 'pitch', 'yaw']:
                            # è®¡ç®—è§’åº¦å·®å€¼å¹¶å½’ä¸€åŒ–åˆ°[-Ï€, Ï€]
                            angle_diff = gen_values_aligned - gt_values_aligned
                            angle_diff = torch.atan2(torch.sin(angle_diff), torch.cos(angle_diff))
                            abs_errors = torch.abs(angle_diff)
                        else:
                            # å¯¹äºä½ç½®å’Œå°ºå¯¸ï¼Œä½¿ç”¨æ™®é€šç»å¯¹è¯¯å·®
                            abs_errors = torch.abs(gen_values_aligned - gt_values_aligned)
                        
                        valid_errors = abs_errors[valid_mask_aligned]
                        
                        # è®¡ç®—å¹³å‡è¯¯å·®
                        mean_error = valid_errors.mean().item() if len(valid_errors) > 0 else 0.0
                        
                        # ğŸ”§ ä¿®å¤ï¼šå¯¹äºæ—‹è½¬è§’åº¦ï¼Œå°†å¼§åº¦è½¬æ¢ä¸ºè§’åº¦åˆ¶è®°å½•åˆ°SwanLab
                        if attr in ['roll', 'pitch', 'yaw']:
                            dimension_errors[f'{attr}_error'] = mean_error * 180.0 / math.pi  # å¼§åº¦è½¬è§’åº¦
                        else:
                            dimension_errors[f'{attr}_error'] = mean_error
                        total_valid_predictions += valid_mask_aligned.sum().item()
                        
                        # ğŸ” æ·»åŠ è°ƒè¯•æ—¥å¿—ï¼šæ‰“å°æ—‹è½¬è§’åº¦çš„è¯¯å·®ä¿¡æ¯
                        # if attr in ['roll', 'pitch', 'yaw'] and verbose:
                        #     print(f"ğŸ” {attr}è§’åº¦è¯¯å·®è®¡ç®—:")
                        #     print(f"   æœ‰æ•ˆé¢„æµ‹æ•°é‡: {valid_mask_aligned.sum().item()}")
                        #     print(f"   å¹³å‡è¯¯å·®: {mean_error:.6f}")
                        #     print(f"   ç”Ÿæˆå€¼èŒƒå›´: [{gen_values_aligned.min().item():.6f}, {gen_values_aligned.max().item():.6f}]")
                        #     print(f"   GTå€¼èŒƒå›´: [{gt_values_aligned.min().item():.6f}, {gt_values_aligned.max().item():.6f}]")
                    else:
                        dimension_errors[f'{attr}_error'] = 0.0
                        # if attr in ['roll', 'pitch', 'yaw'] and verbose:
                        #     print(f"âš ï¸  {attr}è§’åº¦æ²¡æœ‰æœ‰æ•ˆé¢„æµ‹ï¼ˆå…¨ä¸ºpaddingï¼‰")
                else:
                    dimension_errors[f'{attr}_error'] = 0.0
                    # if attr in ['roll', 'pitch', 'yaw'] and verbose:
                    #     print(f"âš ï¸  {attr}è§’åº¦ç¼ºå¤±åœ¨ç”Ÿæˆç»“æœæˆ–ç›®æ ‡ä¸­")
            
            # è®¡ç®—æ€»ä½“å¹³å‡è¯¯å·®
            if total_valid_predictions > 0:
                overall_mean_error = sum(dimension_errors.values()) / 9.0  # 9ä¸ªç»´åº¦
            else:
                overall_mean_error = 0.0
            
            # ç»Ÿè®¡ç”Ÿæˆçš„ç®±å­æ•°é‡ vs GTç®±å­æ•°é‡
            num_generated_boxes = 0
            num_gt_boxes = 0
            
            batch_size = targets['x'].shape[0]
            for b in range(batch_size):
                # ç»Ÿè®¡GTç®±å­æ•°é‡ (épaddingçš„ä½ç½®)
                gt_valid = (targets['x'][b] != -1.0).sum().item()
                num_gt_boxes += gt_valid
                
                # ç»Ÿè®¡ç”Ÿæˆçš„ç®±å­æ•°é‡ (ç®€åŒ–ï¼šæ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†æœ‰æ•ˆåæ ‡)
                if 'x' in processed_gen_results:
                    gen_valid = (processed_gen_results['x'][b] != 0.0).sum().item()  # å‡è®¾0ä¸ºå¡«å……å€¼
                    num_generated_boxes += gen_valid
            
            # åˆå¹¶æ‰€æœ‰æŒ‡æ ‡ - ç§»é™¤è™šå‡çš„æŸå¤±å€¼
            metrics = {
                'iou': gen_iou,
                'num_generated_boxes': num_generated_boxes,
                'num_gt_boxes': num_gt_boxes,
                'overall_mean_error': overall_mean_error,
                'x_error': dimension_errors['x_error'],
                'y_error': dimension_errors['y_error'],
                'z_error': dimension_errors['z_error'],
                'w_error': dimension_errors['w_error'],
                'h_error': dimension_errors['h_error'],
                'l_error': dimension_errors['l_error'],
                'roll_error': dimension_errors['roll_error'],
                'pitch_error': dimension_errors['pitch_error'],
                'yaw_error': dimension_errors['yaw_error']
            }
            
            return metrics
            
        except Exception as e:
            print(f"âš ï¸  è®¡ç®—ç”ŸæˆæŒ‡æ ‡æ—¶å‡ºé”™: {e}")
            # è¿”å›é»˜è®¤å€¼
            return {
                'iou': 0.0,
                'num_generated_boxes': 0,
                'num_gt_boxes': 0,
                'overall_mean_error': 1.0,
                'x_error': 1.0,
                'y_error': 1.0,
                'z_error': 1.0,
                'w_error': 1.0,
                'h_error': 1.0,
                'l_error': 1.0,
                'roll_error': 1.0,
                'pitch_error': 1.0,
                'yaw_error': 1.0
            }
    
    def _compute_generation_iou(self, gen_results: Dict, targets: Dict, verbose: bool = False, equivalent_boxes: List = None) -> float:
        """è®¡ç®—ç”Ÿæˆç»“æœä¸ç›®æ ‡çš„IoUï¼Œä½¿ç”¨ä¸€å¯¹ä¸€åŒ¹é…ç­–ç•¥"""
        try:
            # æ£€æŸ¥ç”Ÿæˆç»“æœ
            if not gen_results or gen_results['x'].shape[1] == 0:
                return 0.0
            
            batch_size = targets['x'].size(0)
            total_iou = 0.0
            valid_samples = 0
            
            for b in range(batch_size):
                # æ„å»ºé¢„æµ‹boxeså’ŒGT boxesçš„ä¸€å¯¹ä¸€åŒ¹é…
                pred_boxes = []
                pred_rotations = []
                gt_boxes = []
                gt_rotations = []
                
                # è·å–åºåˆ—é•¿åº¦
                seq_len = targets['x'].size(1)
                gen_len = gen_results['x'].shape[1]
                
                # ä¸€å¯¹ä¸€åŒ¹é…ï¼šåªè®¡ç®—åŒæ—¶å­˜åœ¨çš„ä½ç½®
                for s in range(seq_len):
                    # æ£€æŸ¥GTæ˜¯å¦ä¸ºæœ‰æ•ˆä½ç½®ï¼ˆépaddingï¼‰ä¸”ç”Ÿæˆç»“æœä¸­ä¹Ÿå­˜åœ¨è¯¥ä½ç½®
                    if targets['x'][b, s].item() != -1.0 and s < gen_len:
                        # å¯¹åº”çš„é¢„æµ‹box - ä¿®å¤è®¿é—®æ ¼å¼
                        pred_box = [
                            float(gen_results['x'][b, s]),
                            float(gen_results['y'][b, s]),
                            float(gen_results['z'][b, s]),
                            float(gen_results['l'][b, s]),
                            float(gen_results['w'][b, s]),
                            float(gen_results['h'][b, s]),
                        ]
                        
                        # ğŸ”§ ä¿®å¤ï¼šæ£€æŸ¥é¢„æµ‹boxæ˜¯å¦ä¸ºpadding
                        pred_pos = np.array(pred_box[:3])  # [x, y, z]
                        pred_size = np.array(pred_box[3:])  # [l, w, h]
                        
                        # æ£€æŸ¥æ˜¯å¦ä¸ºpaddingå€¼ï¼ˆ-1.0ï¼‰æˆ–å…¨é›¶
                        is_padding_value = np.allclose(pred_pos, -1.0, atol=1e-6) or np.allclose(pred_pos, 0.0, atol=1e-6)
                        is_zero_size = np.allclose(pred_size, 0.0, atol=1e-6) or np.allclose(pred_size, -1.0, atol=1e-6)
                        
                        if is_padding_value or is_zero_size:
                            continue
                        
                        # GT box
                        gt_box = [
                            targets['x'][b, s].cpu().item(),
                            targets['y'][b, s].cpu().item(),
                            targets['z'][b, s].cpu().item(),
                            targets['l'][b, s].cpu().item(),
                            targets['w'][b, s].cpu().item(),
                            targets['h'][b, s].cpu().item(),
                        ]
                        gt_boxes.append(gt_box)
                        pred_boxes.append(pred_box)
                        
                        # GTæ—‹è½¬ä¿¡æ¯
                        if 'roll' in targets and 'pitch' in targets and 'yaw' in targets:
                            gt_rot = [
                                targets['roll'][b, s].cpu().item(),
                                targets['pitch'][b, s].cpu().item(),
                                targets['yaw'][b, s].cpu().item(),
                            ]  # [3] euler angles
                            gt_rotations.append(gt_rot)
                        else:
                            gt_rotations.append([0.0, 0.0, 0.0])  # é›¶æ—‹è½¬
                        
                        # é¢„æµ‹æ—‹è½¬ä¿¡æ¯
                        if 'roll' in gen_results and 'pitch' in gen_results and 'yaw' in gen_results:
                            pred_rot = [
                                float(gen_results['roll'][b, s]),
                                float(gen_results['pitch'][b, s]),
                                float(gen_results['yaw'][b, s]),
                            ]  # [3] euler angles
                            pred_rotations.append(pred_rot)
                        else:
                            pred_rotations.append([0.0, 0.0, 0.0])  # é›¶æ—‹è½¬
                
                # ğŸ” æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥ç”Ÿæˆç»“æœé•¿åº¦
                # print(f"ğŸ” ç”ŸæˆIoUè°ƒè¯• - Batch {b}:")
                # print(f"   ç›®æ ‡åºåˆ—é•¿åº¦: {seq_len}")
                # print(f"   ç”Ÿæˆåºåˆ—é•¿åº¦: {gen_len}")
                # print(f"   é¢„æµ‹boxæ•°é‡: {len(pred_boxes)}")
                # print(f"   GT boxæ•°é‡: {len(gt_boxes)}")
                
                # è®¡ç®—è¯¥æ ·æœ¬çš„IoUï¼ˆä¸€å¯¹ä¸€åŒ¹é…ï¼‰
                if pred_boxes and gt_boxes:
                    sample_ious = []
                    
                    # è®¡ç®—æ¯ä¸ªé¢„æµ‹boxä¸å¯¹åº”GT boxçš„IoU
                    for i, (pred_box, pred_rot, gt_box, gt_rot) in enumerate(zip(pred_boxes, pred_rotations, gt_boxes, gt_rotations)):
                        try:
                            # ğŸ” æ·»åŠ è¯¦ç»†æ—¥å¿—ï¼šæ£€æŸ¥boxå°ºå¯¸
                            pred_size = np.array(pred_box[3:])  # [l, w, h]
                            gt_size = np.array(gt_box[3:])     # [l, w, h]
                            
                            if np.any(pred_size <= 0) or np.any(gt_size <= 0):
                                # print(f"ğŸš¨ æ£€æµ‹åˆ°å°ºå¯¸ä¸º0çš„box - Batch {b}, Box {i}:")
                                # print(f"   é¢„æµ‹box: pos={pred_box[:3]}, size={pred_size} (l={pred_size[0]:.6f}, w={pred_size[1]:.6f}, h={pred_size[2]:.6f})")
                                # print(f"   GT box:   pos={gt_box[:3]}, size={gt_size} (l={gt_size[0]:.6f}, w={gt_size[1]:.6f}, h={gt_size[2]:.6f})")
                                # print(f"   é¢„æµ‹æ—‹è½¬: {pred_rot}")
                                # print(f"   GTæ—‹è½¬:   {gt_rot}")
                                pass
                            
                            # ä½¿ç”¨OBB IoUè®¡ç®—
                            iou = self._compute_box_iou(pred_box, gt_box, pred_rot, gt_rot)
                            sample_ious.append(iou)
                            
                            # ğŸ” è¯¦ç»†è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°æ¯ä¸ªboxçš„è¯¦ç»†ä¿¡æ¯
                            # print(f"   Box {i}:")
                            # print(f"     é¢„æµ‹box: pos=({pred_box[0]:.3f}, {pred_box[1]:.3f}, {pred_box[2]:.3f}), size=({pred_box[3]:.3f}, {pred_box[4]:.3f}, {pred_box[5]:.3f})")
                            # print(f"     GT box:   pos=({gt_box[0]:.3f}, {gt_box[1]:.3f}, {gt_box[2]:.3f}), size=({gt_box[3]:.3f}, {gt_box[4]:.3f}, {gt_box[5]:.3f})")
                            # print(f"     é¢„æµ‹æ—‹è½¬: roll={pred_rot[0]:.3f}, pitch={pred_rot[1]:.3f}, yaw={pred_rot[2]:.3f}")
                            # print(f"     GTæ—‹è½¬:   roll={gt_rot[0]:.3f}, pitch={gt_rot[1]:.3f}, yaw={gt_rot[2]:.3f}")
                            
                            # ğŸ” å¦‚æœæœ‰ç­‰æ•ˆboxä¿¡æ¯ï¼Œæ˜¾ç¤ºæœ€ä¼˜ç­‰æ•ˆbox
                            if equivalent_boxes is not None and b < len(equivalent_boxes) and i < len(equivalent_boxes[b]):
                                try:
                                    # æ„å»ºé¢„æµ‹boxç”¨äºç­‰æ•ˆboxé€‰æ‹©
                                    pred_box_for_equiv = [
                                        pred_box[0], pred_box[1], pred_box[2],  # pos
                                        pred_box[3], pred_box[4], pred_box[5],  # size
                                        pred_rot[0], pred_rot[1], pred_rot[2]   # rot
                                    ]
                                    
                                    # é€‰æ‹©æœ€ä¼˜ç­‰æ•ˆbox
                                    equiv_boxes = equivalent_boxes[b][i]
                                    if len(equiv_boxes) > 0:
                                        best_box, min_loss = select_best_equivalent_representation(pred_box_for_equiv, equiv_boxes)
                                        
                                        # ğŸ” è°ƒè¯•ï¼šæ£€æŸ¥best_boxçš„ç»“æ„
                                        # print(f"     ğŸ” best_boxç»“æ„è°ƒè¯•:")
                                        # print(f"       best_boxç±»å‹: {type(best_box)}")
                                        # print(f"       best_boxé•¿åº¦: {len(best_box)}")
                                        # print(f"       best_boxå†…å®¹: {best_box}")
                                        
                                        # print(f"     æœ€ä¼˜ç­‰æ•ˆbox: pos=({best_box[0]:.3f}, {best_box[1]:.3f}, {best_box[2]:.3f}), size=({best_box[3]:.3f}, {best_box[4]:.3f}, {best_box[5]:.3f})")
                                        # print(f"     æœ€ä¼˜ç­‰æ•ˆæ—‹è½¬: roll={best_box[6]:.3f}, pitch={best_box[7]:.3f}, yaw={best_box[8]:.3f}")
                                        # print(f"     åŸå§‹ç­‰æ•ˆboxæ•°é‡: {len(equiv_boxes)}, ç­›é€‰åç­‰æ•ˆboxæ•°é‡: {len([eq for eq in equiv_boxes if abs(eq[6]) <= math.pi/3 and abs(eq[7]) <= math.pi/3 and abs(eq[8]) <= math.pi/3])}, æœ€å°loss: {min_loss:.6f}")
                                        
                                        # ğŸ” è°ƒè¯•ï¼šæ£€æŸ¥pred_boxå’Œbest_boxçš„ç»“æ„
                                        # print(f"     ğŸ” è¯¯å·®è®¡ç®—è°ƒè¯•:")
                                        # print(f"       pred_boxç±»å‹: {type(pred_box)}, é•¿åº¦: {len(pred_box)}")
                                        # print(f"       pred_boxå†…å®¹: {pred_box}")
                                        # print(f"       best_boxç±»å‹: {type(best_box)}, é•¿åº¦: {len(best_box)}")
                                        # print(f"       best_boxå†…å®¹: {best_box}")
                                        
                                        # ğŸ”§ ä¿®å¤ï¼špred_boxåªæœ‰6ä¸ªå…ƒç´ ï¼Œéœ€è¦æ­£ç¡®åˆ‡ç‰‡
                                        pred_pos_array = np.array(pred_box[:3])
                                        best_pos_array = np.array(best_box[:3])
                                        # print(f"       pred_pos_arrayå½¢çŠ¶: {pred_pos_array.shape}")
                                        # print(f"       best_pos_arrayå½¢çŠ¶: {best_pos_array.shape}")
                                        
                                        equiv_pos_error = np.sqrt(sum((pred_pos_array - best_pos_array)**2))
                                        
                                        # pred_boxåªæœ‰6ä¸ªå…ƒç´ ï¼Œbest_boxæœ‰9ä¸ªå…ƒç´ 
                                        pred_size_array = np.array(pred_box[3:])  # [l, w, h]
                                        best_size_array = np.array(best_box[3:6])  # [l, w, h] - åªå–å°ºå¯¸éƒ¨åˆ†
                                        # print(f"       pred_size_arrayå½¢çŠ¶: {pred_size_array.shape}")
                                        # print(f"       best_size_arrayå½¢çŠ¶: {best_size_array.shape}")
                                        
                                        equiv_size_error = np.sqrt(sum((pred_size_array - best_size_array)**2))
                                        
                                        # ğŸ” è°ƒè¯•ï¼šæ£€æŸ¥æ—‹è½¬éƒ¨åˆ†çš„å½¢çŠ¶
                                        # print(f"     ğŸ” æ—‹è½¬éƒ¨åˆ†è°ƒè¯•:")
                                        # print(f"       pred_rotç±»å‹: {type(pred_rot)}, é•¿åº¦: {len(pred_rot)}")
                                        # print(f"       pred_rotå†…å®¹: {pred_rot}")
                                        best_box_rot_slice = best_box[6:9]
                                        # print(f"       best_box[6:9]ç±»å‹: {type(best_box_rot_slice)}, é•¿åº¦: {len(best_box_rot_slice)}")
                                        # print(f"       best_box[6:9]å†…å®¹: {best_box_rot_slice}")
                                        
                                        # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿æ—‹è½¬éƒ¨åˆ†æ˜¯æ­£ç¡®çš„æ ¼å¼
                                        pred_rot_array = np.array(pred_rot)
                                        best_rot_array = np.array([best_box[6], best_box[7], best_box[8]])
                                        
                                        # print(f"       pred_rot_arrayå½¢çŠ¶: {pred_rot_array.shape}")
                                        # print(f"       best_rot_arrayå½¢çŠ¶: {best_rot_array.shape}")
                                        
                                        equiv_rot_error = np.sqrt(sum((pred_rot_array - best_rot_array)**2))
                                        
                                        # print(f"     ä¸æœ€ä¼˜ç­‰æ•ˆboxè¯¯å·®: pos={equiv_pos_error:.3f}, size={equiv_size_error:.3f}, rot={equiv_rot_error:.3f}")
                                except Exception as e:
                                    import traceback
                                    # print(f"     âš ï¸ ç­‰æ•ˆboxè°ƒè¯•å‡ºé”™: {e}")
                                    # print(f"     è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
                                    # traceback.print_exc()
                                    pass
                            
                            # è®¡ç®—å„ç»´åº¦è¯¯å·®
                            pos_error = np.sqrt(sum((np.array(pred_box[:3]) - np.array(gt_box[:3]))**2))
                            size_error = np.sqrt(sum((np.array(pred_box[3:]) - np.array(gt_box[3:]))**2))
                            rot_error = np.sqrt(sum((np.array(pred_rot) - np.array(gt_rot))**2))
                            
                            # print(f"     ä½ç½®è¯¯å·®: {pos_error:.3f}")
                            # print(f"     å°ºå¯¸è¯¯å·®: {size_error:.3f}")
                            # print(f"     æ—‹è½¬è¯¯å·®: {rot_error:.3f}")
                            # print(f"     IoU: {iou:.4f}")
                            
                            # å¦‚æœIoUå¼‚å¸¸é«˜ï¼Œé¢å¤–æ£€æŸ¥
                            # if iou > 0.5:
                            #     print(f"     ğŸš¨ å¼‚å¸¸é«˜IoUè­¦å‘Š!")
                            #     print(f"       å°ºå¯¸å·®å¼‚: l={abs(pred_box[3]-gt_box[3]):.3f}, w={abs(pred_box[4]-gt_box[4]):.3f}, h={abs(pred_box[5]-gt_box[5]):.3f}")
                            #     print(f"       ä½ç½®å·®å¼‚: x={abs(pred_box[0]-gt_box[0]):.3f}, y={abs(pred_box[1]-gt_box[1]):.3f}, z={abs(pred_box[2]-gt_box[2]):.3f}")
                            
                        except Exception as e:
                            # print(f"âš ï¸  è®¡ç®—box IoUæ—¶å‡ºé”™: {e}")
                            sample_ious.append(0.0)
                    
                    if sample_ious:
                        sample_mean_iou = sum(sample_ious) / len(sample_ious)
                        total_iou += sample_mean_iou
                        valid_samples += 1
                        # print(f"   æ ·æœ¬ {b} å¹³å‡IoU: {sample_mean_iou:.4f} (æœ‰æ•ˆIoUæ•°é‡: {len(sample_ious)})")
                    else:
                        # print(f"   æ ·æœ¬ {b} æ²¡æœ‰æœ‰æ•ˆçš„IoUè®¡ç®—")
                        pass
            
            if valid_samples == 0:
                return 0.0
            
            mean_iou = total_iou / valid_samples
            # print(f"\nğŸ¯ ç”ŸæˆOverall Mean IoU: {mean_iou:.4f} (from {valid_samples} samples)")
            return float(mean_iou)
            
        except Exception as e:
            if verbose:
                # print(f"è®¡ç®—ç”ŸæˆIoUæ—¶å‡ºé”™: {e}")
                pass
            return 0.0
    
    def _compute_box_iou(self, box1: List[float], box2: List[float], rot1: List[float] = None, rot2: List[float] = None) -> float:
        """
        è®¡ç®—ä¸¤ä¸ª3D boxçš„IoUï¼Œä½¿ç”¨OBBï¼ˆæœ‰å‘åŒ…å›´ç›’ï¼‰è®¡ç®—
        Args:
            box1: [x, y, z, l, w, h] - ç¬¬ä¸€ä¸ªboxçš„ä¸­å¿ƒåæ ‡å’Œå°ºå¯¸
            box2: [x, y, z, l, w, h] - ç¬¬äºŒä¸ªboxçš„ä¸­å¿ƒåæ ‡å’Œå°ºå¯¸ 
            rot1: [roll, pitch, yaw] - ç¬¬ä¸€ä¸ªboxçš„æ¬§æ‹‰è§’æ—‹è½¬ï¼ˆå¼§åº¦ï¼‰
            rot2: [roll, pitch, yaw] - ç¬¬äºŒä¸ªboxçš„æ¬§æ‹‰è§’æ—‹è½¬ï¼ˆå¼§åº¦ï¼‰
        Returns:
            IoUå€¼ (0.0-1.0)
        """
        # æ£€æŸ¥æ—‹è½¬ä¿¡æ¯æ˜¯å¦å®Œæ•´
        if rot1 is None or rot2 is None:
            raise ValueError("æ—‹è½¬ä¿¡æ¯ç¼ºå¤±ï¼Œæ— æ³•è®¡ç®—OBB IoU")
        
        # ä½¿ç”¨OBB IoUè®¡ç®—
        return self._compute_obb_iou(box1, box2, rot1, rot2)
    
    
    def _compute_obb_iou(self, box1: List[float], box2: List[float], rot1: List[float], rot2: List[float]) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªæœ‰å‘åŒ…å›´ç›’(OBB)çš„IoU
        Args:
            box1: [x, y, z, l, w, h] - ç¬¬ä¸€ä¸ªboxçš„ä¸­å¿ƒåæ ‡å’Œå°ºå¯¸
            box2: [x, y, z, l, w, h] - ç¬¬äºŒä¸ªboxçš„ä¸­å¿ƒåæ ‡å’Œå°ºå¯¸ 
            rot1: [roll, pitch, yaw] - ç¬¬ä¸€ä¸ªboxçš„æ¬§æ‹‰è§’æ—‹è½¬ï¼ˆå¼§åº¦ï¼‰
            rot2: [roll, pitch, yaw] - ç¬¬äºŒä¸ªboxçš„æ¬§æ‹‰è§’æ—‹è½¬ï¼ˆå¼§åº¦ï¼‰
        Returns:
            IoUå€¼ (0.0-1.0)
        """
        try:
            import numpy as np
            from scipy.spatial.transform import Rotation
            
            # ç¡®ä¿è¾“å…¥æ ¼å¼æ­£ç¡®
            if len(box1) != 6 or len(box2) != 6 or len(rot1) != 3 or len(rot2) != 3:
                # print(f"âš ï¸  OBBè¾“å…¥æ ¼å¼é”™è¯¯: box1={len(box1)}, box2={len(box2)}, rot1={len(rot1)}, rot2={len(rot2)}")
                return 0.0
            
            # æ£€æŸ¥è¾“å…¥å‚æ•°çš„æœ‰æ•ˆæ€§
            if np.any(np.isnan(box1)) or np.any(np.isinf(box1)):
                # print(f"âš ï¸  Box1åŒ…å«æ— æ•ˆå€¼: box1={box1}")
                return 0.0
            if np.any(np.isnan(box2)) or np.any(np.isinf(box2)):
                # print(f"âš ï¸  Box2åŒ…å«æ— æ•ˆå€¼: box2={box2}")
                return 0.0
            if np.any(np.isnan(rot1)) or np.any(np.isinf(rot1)):
                # print(f"âš ï¸  Rot1åŒ…å«æ— æ•ˆå€¼: rot1={rot1}")
                return 0.0
            if np.any(np.isnan(rot2)) or np.any(np.isinf(rot2)):
                # print(f"âš ï¸  Rot2åŒ…å«æ— æ•ˆå€¼: rot2={rot2}")
                return 0.0
            
            # æå–boxå‚æ•°
            center1 = np.array(box1[:3])  # [x, y, z]
            size1 = np.array(box1[3:])    # [l, w, h]
            center2 = np.array(box2[:3])  # [x, y, z]
            size2 = np.array(box2[3:])    # [l, w, h]
            
            # æ£€æŸ¥å°ºå¯¸æ˜¯å¦æœ‰æ•ˆï¼ˆé¿å…å°ºå¯¸ä¸º0çš„boxï¼‰
            if np.any(size1 <= 0) or np.any(size2 <= 0):
                # print(f"ğŸš¨ åœ¨OBB IoUè®¡ç®—ä¸­æ£€æµ‹åˆ°æ— æ•ˆå°ºå¯¸çš„box:")
                # print(f"  Box1: center={center1}, size={size1} (l={size1[0]:.6f}, w={size1[1]:.6f}, h={size1[2]:.6f})")
                # print(f"  Box2: center={center2}, size={size2} (l={size2[0]:.6f}, w={size2[1]:.6f}, h={size2[2]:.6f})")
                # print(f"  Rot1: {rot1}")
                # print(f"  Rot2: {rot2}")
                # print(f"  è¿”å›IoU=0.0")
                return 0.0
            
            # åˆ›å»ºæ—‹è½¬çŸ©é˜µ
            rot_matrix1 = Rotation.from_euler('xyz', rot1).as_matrix()
            rot_matrix2 = Rotation.from_euler('xyz', rot2).as_matrix()
            
            # è®¡ç®—OBBçš„8ä¸ªé¡¶ç‚¹
            def get_obb_vertices(center, size, rot_matrix):
                # å±€éƒ¨åæ ‡ç³»çš„8ä¸ªé¡¶ç‚¹
                half_size = size / 2
                vertices_local = np.array([
                    [-half_size[0], -half_size[1], -half_size[2]],
                    [ half_size[0], -half_size[1], -half_size[2]],
                    [ half_size[0],  half_size[1], -half_size[2]],
                    [-half_size[0],  half_size[1], -half_size[2]],
                    [-half_size[0], -half_size[1],  half_size[2]],
                    [ half_size[0], -half_size[1],  half_size[2]],
                    [ half_size[0],  half_size[1],  half_size[2]],
                    [-half_size[0],  half_size[1],  half_size[2]]
                ])
                
                # æ—‹è½¬å¹¶å¹³ç§»åˆ°ä¸–ç•Œåæ ‡ç³»
                vertices_world = vertices_local @ rot_matrix.T + center
                return vertices_world
            
            vertices1 = get_obb_vertices(center1, size1, rot_matrix1)
            vertices2 = get_obb_vertices(center2, size2, rot_matrix2)
            
            # ä½¿ç”¨å‡¸åŒ…è®¡ç®—äº¤é›†ä½“ç§¯
            from scipy.spatial import ConvexHull
            
            # è®¡ç®—ä¸¤ä¸ªOBBçš„å‡¸åŒ…
            try:
                # éªŒè¯é¡¶ç‚¹æ•°æ®çš„æœ‰æ•ˆæ€§
                if not self._validate_vertices(vertices1):
                    # print(f"âš ï¸  Box1é¡¶ç‚¹æ•°æ®æ— æ•ˆ:")
                    # print(f"  center1: {center1}")
                    # print(f"  size1: {size1}")
                    # print(f"  rot1: {rot1}")
                    # print(f"  vertices1 shape: {vertices1.shape}")
                    # print(f"  vertices1 sample: {vertices1[:3]}")
                    raise ValueError("Box1é¡¶ç‚¹æ•°æ®æ— æ•ˆï¼Œæ— æ³•è®¡ç®—OBB IoU")
                
                if not self._validate_vertices(vertices2):
                    # print(f"âš ï¸  Box2é¡¶ç‚¹æ•°æ®æ— æ•ˆ:")
                    # print(f"  center2: {center2}")
                    # print(f"  size2: {size2}")
                    # print(f"  rot2: {rot2}")
                    # print(f"  vertices2 shape: {vertices2.shape}")
                    # print(f"  vertices2 sample: {vertices2[:3]}")
                    raise ValueError("Box2é¡¶ç‚¹æ•°æ®æ— æ•ˆï¼Œæ— æ³•è®¡ç®—OBB IoU")
                
                hull1 = ConvexHull(vertices1)
                hull2 = ConvexHull(vertices2)
                
                # è®¡ç®—ä½“ç§¯
                volume1 = hull1.volume
                volume2 = hull2.volume
                
                # è®¡ç®—äº¤é›†ä½“ç§¯ï¼ˆç®€åŒ–æ–¹æ³•ï¼šä½¿ç”¨AABBè¿‘ä¼¼ï¼‰
                # è¿™é‡Œä½¿ç”¨ç®€åŒ–çš„æ–¹æ³•ï¼Œå®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„ç®—æ³•
                return self._compute_obb_intersection_volume(vertices1, vertices2, volume1, volume2)
                
            except Exception as e:
                raise RuntimeError(f"å‡¸åŒ…è®¡ç®—å‡ºé”™: {e}")
                
        except Exception as e:
            raise RuntimeError(f"OBB IoUè®¡ç®—å‡ºé”™: {e}")
    
    def _validate_vertices(self, vertices):
        """
        éªŒè¯é¡¶ç‚¹æ•°æ®çš„æœ‰æ•ˆæ€§
        Args:
            vertices: [8, 3] é¡¶ç‚¹åæ ‡
        Returns:
            bool: æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„ç‚¹
            if len(vertices) < 4:
                return False
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤ç‚¹
            unique_vertices = np.unique(vertices, axis=0)
            if len(unique_vertices) < 4:
                return False
            
            # æ£€æŸ¥æ˜¯å¦æœ‰NaNæˆ–Infå€¼
            if np.any(np.isnan(vertices)) or np.any(np.isinf(vertices)):
                return False
            
            # æ£€æŸ¥ç‚¹æ˜¯å¦å…±é¢ï¼ˆç®€åŒ–æ£€æŸ¥ï¼‰
            if len(unique_vertices) == 4:
                # å¦‚æœåªæœ‰4ä¸ªå”¯ä¸€ç‚¹ï¼Œæ£€æŸ¥æ˜¯å¦å…±é¢
                vectors = unique_vertices[1:] - unique_vertices[0]
                if np.linalg.matrix_rank(vectors) < 3:
                    return False
            
            return True
            
        except Exception:
            return False
    
    
    def _compute_obb_intersection_volume(self, vertices1: np.ndarray, vertices2: np.ndarray, volume1: float, volume2: float) -> float:
        """
        ä½¿ç”¨trimeshè®¡ç®—ä¸¤ä¸ªOBBçš„çœŸå®äº¤é›†ä½“ç§¯
        ä½¿ç”¨manifold3dè¿›è¡Œç²¾ç¡®çš„å¸ƒå°”è¿ç®—
        """
        try:
            import trimesh
            import numpy as np
            
            # åˆ›å»ºä¸¤ä¸ªOBBçš„trimeshå¯¹è±¡
            try:
                # åˆ›å»ºç¬¬ä¸€ä¸ªOBBçš„mesh
                mesh1 = trimesh.convex.convex_hull(vertices1)
                # åˆ›å»ºç¬¬äºŒä¸ªOBBçš„mesh  
                mesh2 = trimesh.convex.convex_hull(vertices2)
                
                # ä½¿ç”¨trimeshçš„å¸ƒå°”è¿ç®—è®¡ç®—äº¤é›†
                intersection = mesh1.intersection(mesh2)
                
                if intersection is None or intersection.volume <= 0:
                    return 0.0
                
                intersection_volume = intersection.volume
                
                # è®¡ç®—IoU
                union_volume = volume1 + volume2 - intersection_volume
                if union_volume <= 0:
                    return 0.0
                
                iou = intersection_volume / union_volume
                return max(0.0, min(1.0, iou))
                
            except Exception as e:
                print(f"âš ï¸  Trimeshå¸ƒå°”è¿ç®—è®¡ç®—å‡ºé”™: {e}")
                # å›é€€åˆ°AABBæ–¹æ³•
                return self._compute_aabb_iou(vertices1, vertices2, volume1, volume2)
            
        except Exception as e:
            print(f"âš ï¸  OBBäº¤é›†ä½“ç§¯è®¡ç®—å‡ºé”™: {e}")
            return 0.0
    
    
    
    def _compute_aabb_iou(self, vertices1: np.ndarray, vertices2: np.ndarray, volume1: float, volume2: float) -> float:
        """
        è®¡ç®—AABB IoUï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰
        """
        try:
            import numpy as np
            
            # è®¡ç®—AABBåŒ…å›´ç›’
            min1 = np.min(vertices1, axis=0)
            max1 = np.max(vertices1, axis=0)
            min2 = np.min(vertices2, axis=0)
            max2 = np.max(vertices2, axis=0)
            
            # è®¡ç®—äº¤é›†AABB
            inter_min = np.maximum(min1, min2)
            inter_max = np.minimum(max1, max2)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰äº¤é›†
            if np.any(inter_min >= inter_max):
                return 0.0
            
            # è®¡ç®—äº¤é›†ä½“ç§¯
            inter_volume = np.prod(inter_max - inter_min)
            
            # è®¡ç®—IoU
            union_volume = volume1 + volume2 - inter_volume
            if union_volume <= 0:
                return 0.0
            
            iou = inter_volume / union_volume
            return max(0.0, min(1.0, iou))
            
        except Exception as e:
            print(f"âš ï¸  AABB IoUè®¡ç®—å‡ºé”™: {e}")
            return 0.0
    
    
    def _quaternion_to_rotation_matrix(self, quat: List[float]) -> np.ndarray:
        """å°†å››å…ƒæ•°è½¬æ¢ä¸ºæ—‹è½¬çŸ©é˜µ"""
        import numpy as np
        
        if len(quat) != 4:
            return np.eye(3)
            
        qx, qy, qz, qw = quat
        
        # æ ‡å‡†åŒ–å››å…ƒæ•°
        norm = np.sqrt(qx**2 + qy**2 + qz**2 + qw**2)
        if norm == 0:
            return np.eye(3)
        qx, qy, qz, qw = qx/norm, qy/norm, qz/norm, qw/norm
        
        # å››å…ƒæ•°åˆ°æ—‹è½¬çŸ©é˜µçš„è½¬æ¢
        R = np.array([
            [1 - 2*(qy**2 + qz**2), 2*(qx*qy - qw*qz), 2*(qx*qz + qw*qy)],
            [2*(qx*qy + qw*qz), 1 - 2*(qx**2 + qz**2), 2*(qy*qz - qw*qx)],
            [2*(qx*qz - qw*qy), 2*(qy*qz + qw*qx), 1 - 2*(qx**2 + qy**2)]
        ])
        
        return R
    
    def _compute_axis_aligned_iou(self, box1: List[float], box2: List[float]) -> float:
        """è®¡ç®—axis-aligned boxçš„IoUï¼ˆåŸå§‹å®ç°ï¼‰"""
        x1, y1, z1, w1, h1, l1 = box1
        x2, y2, z2, w2, h2, l2 = box2
        
        # è®¡ç®—è¾¹ç•Œ
        x1_min, x1_max = x1 - w1/2, x1 + w1/2
        y1_min, y1_max = y1 - h1/2, y1 + h1/2
        z1_min, z1_max = z1 - l1/2, z1 + l1/2
        
        x2_min, x2_max = x2 - w2/2, x2 + w2/2
        y2_min, y2_max = y2 - h2/2, y2 + h2/2
        z2_min, z2_max = z2 - l2/2, z2 + l2/2
        
        # è®¡ç®—äº¤é›†
        inter_x = max(0, min(x1_max, x2_max) - max(x1_min, x2_min))
        inter_y = max(0, min(y1_max, y2_max) - max(y1_min, y2_min))
        inter_z = max(0, min(z1_max, z2_max) - max(z1_min, z2_min))
        
        inter_volume = inter_x * inter_y * inter_z
        
        # è®¡ç®—å¹¶é›†
        vol1 = w1 * h1 * l1
        vol2 = w2 * h2 * l2
        union_volume = vol1 + vol2 - inter_volume
        
        if union_volume <= 0:
            return 0.0
        
        return inter_volume / union_volume
    
    def _compute_oriented_box_iou_sampling(self, center1, size1, R1, center2, size2, R2, samples_per_dim=20):
        """
        ä½¿ç”¨é‡‡æ ·æ–¹æ³•è®¡ç®—æ—‹è½¬boxçš„IoU
        é€šè¿‡åœ¨3Dç©ºé—´ä¸­å¯†é›†é‡‡æ ·ç‚¹æ¥è¿‘ä¼¼è®¡ç®—äº¤é›†ä½“ç§¯
        """
        import numpy as np
        
        # è®¡ç®—ä¸¤ä¸ªboxçš„ä½“ç§¯
        vol1 = size1[0] * size1[1] * size1[2]
        vol2 = size2[0] * size2[1] * size2[2]
        
        if vol1 <= 0 or vol2 <= 0:
            return 0.0
        
        # ç¡®å®šé‡‡æ ·åŒºåŸŸï¼ˆä¸¤ä¸ªboxçš„åŒ…å›´ç›’ï¼‰
        corners1 = self._get_box_corners(center1, size1, R1)
        corners2 = self._get_box_corners(center2, size2, R2)
        
        all_corners = np.vstack([corners1, corners2])
        min_coords = np.min(all_corners, axis=0)
        max_coords = np.max(all_corners, axis=0)
        
        # æ‰©å±•é‡‡æ ·åŒºåŸŸä¸€ç‚¹ç‚¹ï¼Œç¡®ä¿åŒ…å«è¾¹ç•Œ
        margin = 0.01
        min_coords -= margin
        max_coords += margin
        
        # ç”Ÿæˆé‡‡æ ·ç‚¹
        x = np.linspace(min_coords[0], max_coords[0], samples_per_dim)
        y = np.linspace(min_coords[1], max_coords[1], samples_per_dim)
        z = np.linspace(min_coords[2], max_coords[2], samples_per_dim)
        
        # è®¡ç®—é‡‡æ ·ä½“ç§¯å…ƒç´ 
        dx = (max_coords[0] - min_coords[0]) / samples_per_dim
        dy = (max_coords[1] - min_coords[1]) / samples_per_dim
        dz = (max_coords[2] - min_coords[2]) / samples_per_dim
        dV = dx * dy * dz
        
        # æ£€æŸ¥æ¯ä¸ªé‡‡æ ·ç‚¹æ˜¯å¦åœ¨ä¸¤ä¸ªboxå†…
        intersection_count = 0
        union_count = 0
        
        for xi in x:
            for yi in y:
                for zi in z:
                    point = np.array([xi, yi, zi])
                    
                    in_box1 = self._point_in_oriented_box(point, center1, size1, R1)
                    in_box2 = self._point_in_oriented_box(point, center2, size2, R2)
                    
                    if in_box1 and in_box2:
                        intersection_count += 1
                    
                    if in_box1 or in_box2:
                        union_count += 1
        
        if union_count == 0:
            return 0.0
        
        # è®¡ç®—IoU
        intersection_volume = intersection_count * dV
        union_volume = vol1 + vol2 - intersection_volume
        
        if union_volume <= 0:
            return 0.0
        
        iou = intersection_volume / union_volume
        
        # è°ƒè¯•ä¿¡æ¯ï¼šå½“IoUå¼‚å¸¸æ—¶æ‰“å°è¯¦ç»†ä¿¡æ¯
        if iou < 0 or iou > 1:
            # print(f"âš ï¸  å¼‚å¸¸IoUå€¼: {iou:.6f}")
            # print(f"  intersection_count: {intersection_count}, dV: {dV:.6f}")
            # print(f"  vol1: {vol1:.6f}, vol2: {vol2:.6f}")
            # print(f"  intersection_volume: {intersection_volume:.6f}")
            # print(f"  union_volume: {union_volume:.6f}")
            # å¼ºåˆ¶é™åˆ¶åœ¨[0,1]èŒƒå›´å†…
            iou = max(0.0, min(1.0, iou))
        
        return iou
    
    def _get_box_corners(self, center, size, R):
        """è·å–æ—‹è½¬boxçš„8ä¸ªé¡¶ç‚¹åæ ‡"""
        import numpy as np
        
        # boxåœ¨å±€éƒ¨åæ ‡ç³»ä¸­çš„8ä¸ªé¡¶ç‚¹
        w, h, l = size
        local_corners = np.array([
            [-w/2, -h/2, -l/2],
            [+w/2, -h/2, -l/2],
            [+w/2, +h/2, -l/2],
            [-w/2, +h/2, -l/2],
            [-w/2, -h/2, +l/2],
            [+w/2, -h/2, +l/2],
            [+w/2, +h/2, +l/2],
            [-w/2, +h/2, +l/2],
        ])
        
        # è½¬æ¢åˆ°ä¸–ç•Œåæ ‡ç³»
        world_corners = (R @ local_corners.T).T + center
        return world_corners
    
    def _point_in_oriented_box(self, point, center, size, R):
        """æ£€æŸ¥ç‚¹æ˜¯å¦åœ¨æ—‹è½¬çš„boxå†…"""
        import numpy as np
        
        # å°†ç‚¹è½¬æ¢åˆ°boxçš„å±€éƒ¨åæ ‡ç³»
        local_point = R.T @ (point - center)
        
        # æ£€æŸ¥æ˜¯å¦åœ¨å±€éƒ¨åæ ‡ç³»çš„è¾¹ç•Œå†…
        w, h, l = size
        return (abs(local_point[0]) <= w/2 and 
                abs(local_point[1]) <= h/2 and 
                abs(local_point[2]) <= l/2)
    
    def _test_inference(self) -> Dict[str, float]:
        """åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæ¨ç†ï¼Œè®¡ç®—æŒ‡æ ‡å¹¶ä¿å­˜ç»“æœ"""
        if self.is_main_process:
            print("ğŸ§ª å¼€å§‹æµ‹è¯•é›†æ¨ç†...")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨
        test_loader = create_dataloader(
            data_root=self.config_loader.get('data.dataset.data_root'),
            stage="test",
            batch_size=1,  # æµ‹è¯•æ—¶ä½¿ç”¨batch_size=1
            max_boxes=self.config_loader.get('global.max_seq_len'),
            image_size=self.config_loader.get('global.image_size'),
            continuous_ranges=self.config_loader.get('data.dataset.continuous_ranges'),
            augmentation_config={},  # æµ‹è¯•æ—¶ä¸ä½¿ç”¨æ•°æ®å¢å¼º
            num_workers=0,  # æµ‹è¯•æ—¶ä½¿ç”¨å•è¿›ç¨‹
            pin_memory=False,
            prefetch_factor=2,
            persistent_workers=False
        )
        
        self.model.eval()
        
        # æµ‹è¯•ç»Ÿè®¡
        total_gen_iou = 0.0
        total_generated_boxes = 0.0
        total_gt_boxes = 0.0
        total_x_error = 0.0
        total_y_error = 0.0
        total_z_error = 0.0
        total_w_error = 0.0
        total_h_error = 0.0
        total_l_error = 0.0
        total_overall_error = 0.0
        num_batches = 0
        
        # ä¿å­˜æµ‹è¯•ç»“æœ
        test_results = []
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(test_loader):
                if self.is_main_process:
                    print(f"  ğŸ“Š å¤„ç†æµ‹è¯•æ ·æœ¬ {batch_idx + 1}/{len(test_loader)}")
                
                # å‡†å¤‡è¾“å…¥æ•°æ®
                rgbxyz = batch['image'].to(self.device)
                targets = {
                    'x': batch['x'].to(self.device),
                    'y': batch['y'].to(self.device),
                    'z': batch['z'].to(self.device),
                    'w': batch['w'].to(self.device),
                    'h': batch['h'].to(self.device),
                    'l': batch['l'].to(self.device),
                    'roll': batch['roll'].to(self.device),
                    'pitch': batch['pitch'].to(self.device),
                    'yaw': batch['yaw'].to(self.device),
                }
                
                # ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œç”Ÿæˆ
                if hasattr(self.model, 'module'):
                    model = self.model.module
                else:
                    model = self.model
                
                try:
                    # æ£€æŸ¥è¾“å…¥æ•°æ®çš„æœ‰æ•ˆæ€§
                    if torch.isnan(rgbxyz).any() or torch.isinf(rgbxyz).any():
                        if self.is_main_process:
                            print(f"  âš ï¸  è¾“å…¥æ•°æ®åŒ…å«NaNæˆ–Infå€¼ï¼Œè·³è¿‡æ­¤æ ·æœ¬")
                        continue
                    
                    # æ ¹æ®é…ç½®é€‰æ‹©æ¨ç†æ–¹æ³•
                    if self.use_incremental_inference:
                        # ä½¿ç”¨å¢é‡æ¨ç†è¿›è¡Œæµ‹è¯•ç”Ÿæˆï¼ˆæ›´é«˜æ•ˆï¼‰
                        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„max_seq_lenå‚æ•°
                        max_len = self.config_loader.get_global_config()['max_seq_len']
                        gen_results = model.generate_incremental(
                            image=rgbxyz,
                            max_seq_len=max_len,
                            temperature=self.incremental_temperature,
                            eos_threshold=self.eos_threshold
                        )
                    else:
                        # ä½¿ç”¨ä¼ ç»Ÿæ¨ç†æ–¹æ³•
                        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„max_seq_lenå‚æ•°
                        max_len = self.config_loader.get_global_config()['max_seq_len']
                        gen_results = model.generate(
                            image=rgbxyz,
                            max_seq_len=max_len,
                            temperature=1.0
                        )
                    
                    # æ£€æŸ¥ç”Ÿæˆç»“æœçš„æœ‰æ•ˆæ€§
                    if not gen_results or not isinstance(gen_results, dict):
                        if self.is_main_process:
                            print(f"  âš ï¸  ç”Ÿæˆç»“æœæ— æ•ˆï¼Œè·³è¿‡æ­¤æ ·æœ¬")
                        continue
                        
                except Exception as e:
                    if self.is_main_process:
                        print(f"  âŒ ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
                        print(f"  ğŸ“Š è¾“å…¥å›¾åƒå½¢çŠ¶: {rgbxyz.shape}")
                        print(f"  ğŸ“Š è¾“å…¥å›¾åƒèŒƒå›´: [{rgbxyz.min().item():.4f}, {rgbxyz.max().item():.4f}]")
                    continue
                
                # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„ç”ŸæˆæŒ‡æ ‡
                batch_size = targets['x'].size(0)
                batch_x_error = 0.0
                batch_y_error = 0.0
                batch_z_error = 0.0
                batch_w_error = 0.0
                batch_h_error = 0.0
                batch_l_error = 0.0
                batch_overall_error = 0.0
                
                for sample_idx in range(batch_size):
                    # æå–å•ä¸ªæ ·æœ¬çš„ç»“æœ
                    sample_gen_results = {}
                    sample_targets = {}
                    
                    for attr in ['x', 'y', 'z', 'w', 'h', 'l', 'roll', 'pitch', 'yaw']:
                        if attr in gen_results:
                            sample_gen_results[attr] = gen_results[attr][sample_idx:sample_idx+1]  # [1, seq_len]
                        if attr in targets:
                            sample_targets[attr] = targets[attr][sample_idx:sample_idx+1]  # [1, seq_len]
                    
                    # è®¡ç®—å•ä¸ªæ ·æœ¬çš„æŒ‡æ ‡
                    # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ä¸éªŒè¯é›†ç›¸åŒçš„ç­‰æ•ˆboxé€»è¾‘
                    if batch.get('equivalent_boxes'):
                        sample_equivalent_boxes = [batch['equivalent_boxes'][sample_idx]]
                    else:
                        sample_equivalent_boxes = None
                    sample_metrics = self._compute_generation_metrics(sample_gen_results, sample_targets, None, verbose=False, equivalent_boxes=sample_equivalent_boxes)
                    
                    # æ‰“å°æ¯ä¸ªæ ·æœ¬çš„ç»“æœ
                    if self.is_main_process:
                        actual_sample_idx = batch_idx * batch_size + sample_idx + 1
                        print(f"   Test Sample {actual_sample_idx}: Mean IoU = {sample_metrics['iou']:.4f} ({sample_metrics['num_generated_boxes']:.0f} boxes)")
                    
                    # ç´¯ç§¯ç»Ÿè®¡
                    total_gen_iou += sample_metrics['iou']
                    total_generated_boxes += sample_metrics['num_generated_boxes']
                    total_gt_boxes += sample_metrics['num_gt_boxes']
                    
                    # ç´¯ç§¯è¯¯å·®æŒ‡æ ‡
                    batch_x_error += sample_metrics.get('x_error', 0.0)
                    batch_y_error += sample_metrics.get('y_error', 0.0)
                    batch_z_error += sample_metrics.get('z_error', 0.0)
                    batch_w_error += sample_metrics.get('w_error', 0.0)
                    batch_h_error += sample_metrics.get('h_error', 0.0)
                    batch_l_error += sample_metrics.get('l_error', 0.0)
                    batch_overall_error += sample_metrics.get('overall_mean_error', 0.0)
                
                # ç´¯ç§¯åˆ°æ€»è¯¯å·® - ğŸ”§ ä¿®å¤ï¼šç›´æ¥ç´¯ç§¯ï¼Œä¸è¦å¤šé™¤batch_size
                total_x_error += batch_x_error
                total_y_error += batch_y_error
                total_z_error += batch_z_error
                total_w_error += batch_w_error
                total_h_error += batch_h_error
                total_l_error += batch_l_error
                total_overall_error += batch_overall_error
                
                # ä¿å­˜è¯¦ç»†ç»“æœ
                test_results.append({
                    'batch_idx': batch_idx,
                    'folder_name': batch['folder_name'][0] if 'folder_name' in batch else f'test_{batch_idx}',
                    'generation_results': {
                        'x': gen_results['x'].cpu().tolist() if 'x' in gen_results else [],
                        'y': gen_results['y'].cpu().tolist() if 'y' in gen_results else [],
                        'z': gen_results['z'].cpu().tolist() if 'z' in gen_results else [],
                        'w': gen_results['w'].cpu().tolist() if 'w' in gen_results else [],
                        'h': gen_results['h'].cpu().tolist() if 'h' in gen_results else [],
                        'l': gen_results['l'].cpu().tolist() if 'l' in gen_results else [],
                    },
                    'ground_truth': {
                        'x': targets['x'].cpu().tolist(),
                        'y': targets['y'].cpu().tolist(),
                        'z': targets['z'].cpu().tolist(),
                        'w': targets['w'].cpu().tolist(),
                        'h': targets['h'].cpu().tolist(),
                        'l': targets['l'].cpu().tolist(),
                        'roll': targets['roll'].cpu().tolist(),
                        'pitch': targets['pitch'].cpu().tolist(),
                        'yaw': targets['yaw'].cpu().tolist(),
                    },
                    'metrics': {
                        'iou': total_gen_iou / batch_size if batch_size > 0 else 0.0,
                        'num_generated_boxes': total_generated_boxes / batch_size if batch_size > 0 else 0.0,
                        'num_gt_boxes': total_gt_boxes / batch_size if batch_size > 0 else 0.0,
                        'x_error': batch_x_error / batch_size if batch_size > 0 else 0.0,
                        'y_error': batch_y_error / batch_size if batch_size > 0 else 0.0,
                        'z_error': batch_z_error / batch_size if batch_size > 0 else 0.0,
                        'w_error': batch_w_error / batch_size if batch_size > 0 else 0.0,
                        'h_error': batch_h_error / batch_size if batch_size > 0 else 0.0,
                        'l_error': batch_l_error / batch_size if batch_size > 0 else 0.0,
                        'overall_mean_error': batch_overall_error / batch_size if batch_size > 0 else 0.0
                    }
                })
                
                num_batches += 1
        
        # è®¡ç®—å¹³å‡å€¼
        avg_gen_iou = total_gen_iou / num_batches if num_batches > 0 else 0.0
        avg_generated_boxes = total_generated_boxes / num_batches if num_batches > 0 else 0.0
        avg_gt_boxes = total_gt_boxes / num_batches if num_batches > 0 else 0.0
        avg_x_error = total_x_error / num_batches if num_batches > 0 else 0.0
        avg_y_error = total_y_error / num_batches if num_batches > 0 else 0.0
        avg_z_error = total_z_error / num_batches if num_batches > 0 else 0.0
        avg_w_error = total_w_error / num_batches if num_batches > 0 else 0.0
        avg_h_error = total_h_error / num_batches if num_batches > 0 else 0.0
        avg_l_error = total_l_error / num_batches if num_batches > 0 else 0.0
        avg_overall_error = total_overall_error / num_batches if num_batches > 0 else 0.0
        
        # ä¿å­˜æµ‹è¯•ç»“æœåˆ°æ–‡ä»¶ - åªåœ¨ä¸»è¿›ç¨‹ä¿å­˜
        if self.is_main_process:
            test_results_file = self.output_dir / "test_results.json"
            with open(test_results_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'test_summary': {
                        'num_samples': num_batches,
                        'avg_generation_iou': avg_gen_iou,
                        'avg_generated_boxes': avg_generated_boxes,
                        'avg_gt_boxes': avg_gt_boxes,
                        'avg_overall_error': avg_overall_error,
                        'avg_x_error': avg_x_error,
                        'avg_y_error': avg_y_error,
                        'avg_z_error': avg_z_error,
                        'avg_w_error': avg_w_error,
                        'avg_h_error': avg_h_error,
                        'avg_l_error': avg_l_error,
                        'generation_rate': avg_generated_boxes / max(avg_gt_boxes, 1)
                    },
                    'detailed_results': test_results
                }, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {test_results_file}")
        
        # æ‰“å°æµ‹è¯•ç»“æœæ‘˜è¦ - åªåœ¨ä¸»è¿›ç¨‹æ‰“å°
        if self.is_main_process:
            print(f"\nğŸ§ª æµ‹è¯•é›†æ¨ç†ç»“æœæ‘˜è¦:")
            print(f"   æ ·æœ¬æ•°é‡: {num_batches}")
            print(f"   å¹³å‡ç”ŸæˆIoU: {avg_gen_iou:.4f}")
            print(f"   å¹³å‡ç”Ÿæˆç®±å­æ•°: {avg_generated_boxes:.1f}")
            print(f"   å¹³å‡GTç®±å­æ•°: {avg_gt_boxes:.1f}")
            print(f"   ç”Ÿæˆç‡: {avg_generated_boxes/max(avg_gt_boxes, 1):.2f}")
            print(f"   æ€»ä½“å¹³å‡è¯¯å·®: {avg_overall_error:.4f}")
            print(f"   Xè¯¯å·®: {avg_x_error:.4f} | Yè¯¯å·®: {avg_y_error:.4f} | Zè¯¯å·®: {avg_z_error:.4f}")
            print(f"   Wè¯¯å·®: {avg_w_error:.4f} | Hè¯¯å·®: {avg_h_error:.4f} | Lè¯¯å·®: {avg_l_error:.4f}")
        
        return {
            'generation_iou': avg_gen_iou,
            'avg_generated_boxes': avg_generated_boxes,
            'avg_gt_boxes': avg_gt_boxes,
            'avg_overall_error': avg_overall_error,
            'avg_x_error': avg_x_error,
            'avg_y_error': avg_y_error,
            'avg_z_error': avg_z_error,
            'avg_w_error': avg_w_error,
            'avg_h_error': avg_h_error,
            'avg_l_error': avg_l_error
        }
    
    def _save_checkpoint(self, is_best: bool = False, is_best_generation: bool = False):
        """ä¿å­˜checkpoint"""
        if not self.is_main_process:
            return
        checkpoint = {
            'model': self.model.module.state_dict() if hasattr(self.model, 'module') else self.model.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'scheduler': self.scheduler.state_dict() if self.scheduler else None,
            'scaler': self.scaler.state_dict() if self.scaler else None,
            'epoch': self.current_epoch,
            'current_phase_idx': self.current_phase_idx,
            'best_val_loss': self.best_val_loss,
            'best_generation_loss': self.best_generation_loss,
            'training_stats': self.training_stats,
            'model_config': self.model_config,
            'training_config': self.training_config,
            'metadata': {
                'total_epochs': sum(phase.epochs for phase in self.training_phases),
                'training_phases': [asdict(phase) for phase in self.training_phases],
                'device': str(self.device),
                'world_size': self.world_size,
                'local_rank': self.local_rank
            }
        }
        
        # åªä¿å­˜æœ€æ–°checkpoint (ç”¨äºresume)
        latest_path = self.checkpoint_dir / "checkpoint_latest.pt"
        torch.save(checkpoint, latest_path)
        
        # ä¿å­˜æœ€ä½³éªŒè¯æŸå¤±æ¨¡å‹
        if is_best:
            best_path = self.checkpoint_dir / "checkpoint_best_val.pt"
            torch.save(checkpoint, best_path)
            if self.is_main_process:
                print(f"âœ… æœ€ä½³éªŒè¯æ¨¡å‹å·²ä¿å­˜: {best_path}")
        
        # ä¿å­˜æœ€ä½³ç”Ÿæˆæ¨¡å‹
        if is_best_generation:
            best_gen_path = self.checkpoint_dir / "checkpoint_best_generation.pt"
            torch.save(checkpoint, best_gen_path)
            if self.is_main_process:
                print(f"âœ… æœ€ä½³ç”Ÿæˆæ¨¡å‹å·²ä¿å­˜: {best_gen_path}")
        
        if not (is_best or is_best_generation):
            if self.is_main_process:
                print(f"âœ… æœ€æ–°checkpointå·²ä¿å­˜: {latest_path}")
    
    def _load_checkpoint(self, checkpoint_path: str):
        """åŠ è½½checkpoint"""
        if self.is_main_process:
            print(f"ğŸ“‚ åŠ è½½checkpoint: {checkpoint_path}")
        
        # ğŸ”§ ä¿®å¤PyTorch 2.6çš„weights_onlyé—®é¢˜
        # ç”±äºè¿™æ˜¯æˆ‘ä»¬è‡ªå·±çš„checkpointæ–‡ä»¶ï¼Œè®¾ç½®weights_only=Falseæ˜¯å®‰å…¨çš„
        checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)
        
        # åŠ è½½æ¨¡å‹çŠ¶æ€
        if hasattr(self.model, 'module'):
            self.model.module.load_state_dict(checkpoint['model'])
        else:
            self.model.load_state_dict(checkpoint['model'])
        
        # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
        self.optimizer.load_state_dict(checkpoint['optimizer'])
        
        # åŠ è½½è°ƒåº¦å™¨çŠ¶æ€
        if self.scheduler and checkpoint['scheduler']:
            self.scheduler.load_state_dict(checkpoint['scheduler'])
        
        # åŠ è½½scalerçŠ¶æ€
        if self.scaler and checkpoint['scaler']:
            self.scaler.load_state_dict(checkpoint['scaler'])
        
        # æ¢å¤è®­ç»ƒçŠ¶æ€
        self.current_epoch = checkpoint['epoch'] + 1  # ä»ä¸‹ä¸€ä¸ªepochå¼€å§‹
        self.current_phase_idx = checkpoint.get('current_phase_idx', 0)
        self.best_val_loss = checkpoint.get('best_val_loss', float('inf'))
        self.best_generation_loss = checkpoint.get('best_generation_loss', float('inf'))
        self.training_stats = checkpoint.get('training_stats', [])
        
        # ğŸ”§ ä¿®å¤ï¼šé‡æ–°è®¡ç®—æ­£ç¡®çš„è®­ç»ƒé˜¶æ®µï¼ˆä»¥é˜²checkpointä¸­çš„phase_idxä¸å‡†ç¡®ï¼‰
        epoch_count = 0
        old_phase_idx = self.current_phase_idx
        for phase_idx, phase in enumerate(self.training_phases):
            if self.is_main_process:
                print(f"   æ£€æŸ¥é˜¶æ®µ{phase_idx} ({phase.name}): epochs {epoch_count}-{epoch_count + phase.epochs - 1}")
                print(f"     æ¡ä»¶: {self.current_epoch} < {epoch_count + phase.epochs} = {self.current_epoch < epoch_count + phase.epochs}")
            
            if self.current_epoch < epoch_count + phase.epochs:
                self.current_phase_idx = phase_idx
                if self.is_main_process:
                    print(f"     -> é€‰æ‹©é˜¶æ®µ{phase_idx} ({phase.name})")
                break
            epoch_count += phase.epochs
        
        if self.is_main_process:
            print(f"ğŸ”„ é˜¶æ®µç´¢å¼•: {old_phase_idx} -> {self.current_phase_idx}")
            print(f"âœ… å·²æ¢å¤åˆ°epoch {self.current_epoch}")
            print(f"ğŸ¯ å½“å‰è®­ç»ƒé˜¶æ®µ: {self.training_phases[self.current_phase_idx].name} (é˜¶æ®µ{self.current_phase_idx})")
            print(f"ğŸ“Š æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.4f}")
            print(f"ğŸ¯ æœ€ä½³ç”ŸæˆæŸå¤±: {self.best_generation_loss:.4f}")
    
    def _evaluate_best_model_on_test(self, is_best_val: bool = False, is_best_generation: bool = False):
        """åœ¨è·å¾—æœ€ä½³æ¨¡å‹æ—¶åœ¨testé›†ä¸Šè¿›è¡Œæ¨ç†å¹¶è®°å½•ç»“æœ"""
        if not self.is_main_process:
            return
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨æœ€ä½³æ¨¡å‹testæ¨ç†
        best_model_testing_config = self.training_config.get('optimizations', {}).get('best_model_testing', {})
        if not best_model_testing_config.get('enabled', True):
            return
        
        # åªæœ‰åœ¨è·å¾—æœ€ä½³æ¨¡å‹æ—¶æ‰è¿›è¡Œtestæ¨ç†
        if not (is_best_val or is_best_generation):
            return
            
        print(f"\nğŸ¯ æ£€æµ‹åˆ°{'æœ€ä½³éªŒè¯æ¨¡å‹' if is_best_val else ''}{'å’Œ' if is_best_val and is_best_generation else ''}{'æœ€ä½³ç”Ÿæˆæ¨¡å‹' if is_best_generation else ''}ï¼Œå¼€å§‹åœ¨testé›†ä¸Šè¿›è¡Œæ¨ç†...")
        
        # ä¿å­˜å½“å‰æ¨¡å‹çŠ¶æ€
        current_model_state = self.model.module.state_dict() if hasattr(self.model, 'module') else self.model.state_dict()
        
        try:
            # åœ¨testé›†ä¸Šè¿›è¡Œæ¨ç†
            test_results = self._test_inference()
            
            # è®°å½•testç»“æœåˆ°SwanLab
            if self.use_swanlab and best_model_testing_config.get('save_results', True):
                # ä¸ºæœ€ä½³éªŒè¯æ¨¡å‹è®°å½•testç»“æœ
                if is_best_val:
                    swanlab.log({
                        'test_best_val/generation_iou': test_results['generation_iou'],
                        'test_best_val/overall_mean_error': test_results.get('avg_overall_error', 0.0),
                        'test_best_val/x_error': test_results.get('avg_x_error', 0.0),
                        'test_best_val/y_error': test_results.get('avg_y_error', 0.0),
                        'test_best_val/z_error': test_results.get('avg_z_error', 0.0),
                        'test_best_val/w_error': test_results.get('avg_w_error', 0.0),
                        'test_best_val/h_error': test_results.get('avg_h_error', 0.0),
                        'test_best_val/l_error': test_results.get('avg_l_error', 0.0),
                        'test_best_val/roll_error': test_results.get('avg_roll_error', 0.0),
                        'test_best_val/pitch_error': test_results.get('avg_pitch_error', 0.0),
                        'test_best_val/yaw_error': test_results.get('avg_yaw_error', 0.0),
                        'test_best_val/avg_generated_boxes': test_results.get('avg_generated_boxes', 0.0),
                        'test_best_val/avg_gt_boxes': test_results.get('avg_gt_boxes', 0.0),
                        'test_best_val/generation_rate': test_results.get('avg_generated_boxes', 0.0) / max(test_results.get('avg_gt_boxes', 1), 1),
                        'test_best_val/epoch': self.current_epoch
                    }, step=self.current_epoch)
                
                # ä¸ºæœ€ä½³ç”Ÿæˆæ¨¡å‹è®°å½•testç»“æœ
                if is_best_generation:
                    swanlab.log({
                        'test_best_gen/generation_iou': test_results['generation_iou'],
                        'test_best_gen/overall_mean_error': test_results.get('avg_overall_error', 0.0),
                        'test_best_gen/x_error': test_results.get('avg_x_error', 0.0),
                        'test_best_gen/y_error': test_results.get('avg_y_error', 0.0),
                        'test_best_gen/z_error': test_results.get('avg_z_error', 0.0),
                        'test_best_gen/w_error': test_results.get('avg_w_error', 0.0),
                        'test_best_gen/h_error': test_results.get('avg_h_error', 0.0),
                        'test_best_gen/l_error': test_results.get('avg_l_error', 0.0),
                        'test_best_gen/roll_error': test_results.get('avg_roll_error', 0.0),
                        'test_best_gen/pitch_error': test_results.get('avg_pitch_error', 0.0),
                        'test_best_gen/yaw_error': test_results.get('avg_yaw_error', 0.0),
                        'test_best_gen/avg_generated_boxes': test_results.get('avg_generated_boxes', 0.0),
                        'test_best_gen/avg_gt_boxes': test_results.get('avg_gt_boxes', 0.0),
                        'test_best_gen/generation_rate': test_results.get('avg_generated_boxes', 0.0) / max(test_results.get('avg_gt_boxes', 1), 1),
                        'test_best_gen/epoch': self.current_epoch
                    }, step=self.current_epoch)
            
            # æ‰“å°testç»“æœ
            print(f"ğŸ§ª Testé›†æ¨ç†ç»“æœ (Epoch {self.current_epoch}):")
            print(f"   ç”ŸæˆIoU: {test_results['generation_iou']:.4f}")
            print(f"   æ€»ä½“å¹³å‡è¯¯å·®: {test_results.get('avg_overall_error', 0.0):.4f}")
            print(f"   Xè¯¯å·®: {test_results.get('avg_x_error', 0.0):.4f} | Yè¯¯å·®: {test_results.get('avg_y_error', 0.0):.4f} | Zè¯¯å·®: {test_results.get('avg_z_error', 0.0):.4f}")
            print(f"   Wè¯¯å·®: {test_results.get('avg_w_error', 0.0):.4f} | Hè¯¯å·®: {test_results.get('avg_h_error', 0.0):.4f} | Lè¯¯å·®: {test_results.get('avg_l_error', 0.0):.4f}")
            print(f"   å¹³å‡ç”Ÿæˆæ•°é‡: {test_results.get('avg_generated_boxes', 0.0):.1f} | å¹³å‡GTæ•°é‡: {test_results.get('avg_gt_boxes', 0.0):.1f}")
            print(f"   ç”Ÿæˆç‡: {test_results.get('avg_generated_boxes', 0.0) / max(test_results.get('avg_gt_boxes', 1), 1):.2f}")
            
        except Exception as e:
            print(f"âŒ Testé›†æ¨ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        
        finally:
            # ç¡®ä¿æ¨¡å‹çŠ¶æ€æ²¡æœ‰è¢«æ”¹å˜
            if hasattr(self.model, 'module'):
                self.model.module.load_state_dict(current_model_state)
            else:
                self.model.load_state_dict(current_model_state)

    def train(self):
        """å¼€å§‹è®­ç»ƒ"""
        # å¤šGPUåŒæ­¥ç‚¹ - ç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½å‡†å¤‡å¥½
        if self.world_size > 1 and torch.cuda.device_count() > 1:
            if torch.distributed.is_initialized():
                torch.distributed.barrier()
                if self.is_main_process:
                    print("âœ… æ‰€æœ‰è¿›ç¨‹åŒæ­¥å®Œæˆï¼Œå¼€å§‹è®­ç»ƒ...")
        
        if self.is_main_process:
            print("ğŸš€ å¼€å§‹åˆ†æ®µå¼è®­ç»ƒ...")
        
        total_epochs = sum(phase.epochs for phase in self.training_phases)
        
        # ğŸ”§ ä¿®å¤ï¼šä¸å†é‡æ–°è®¡ç®—é˜¶æ®µç´¢å¼•ï¼Œä½¿ç”¨_load_checkpointä¸­å·²æ­£ç¡®è®¡ç®—çš„å€¼
        for phase_idx in range(self.current_phase_idx, len(self.training_phases)):
            phase = self.training_phases[phase_idx]
            
            if self.is_main_process:
                print(f"\n{'='*60}")
                print(f"ğŸ¯ è®­ç»ƒé˜¶æ®µ: {phase.name}")
                print(f"ğŸ“ {phase.description}")
                print(f"â±ï¸  æŒç»­{phase.epochs}ä¸ªepochs")
                print(f"ğŸ² Teacher Forcing: {phase.teacher_forcing_ratio}")
                print(f"ğŸ“Š Scheduled Sampling: {phase.scheduled_sampling}")
                print(f"{'='*60}")
            
            # ä¸ºå½“å‰é˜¶æ®µåˆ›å»ºæŸå¤±å‡½æ•°
            loss_fn = self._create_loss_function(phase.name)
            loss_fn = loss_fn.to(self.device)
            
            # è®¡ç®—é˜¶æ®µå†…çš„èµ·å§‹epoch
            phase_start_epoch = sum(p.epochs for p in self.training_phases[:phase_idx])
            
            for epoch_in_phase in range(phase.epochs):
                if self.current_epoch > phase_start_epoch + epoch_in_phase:
                    continue  # è·³è¿‡å·²è®­ç»ƒçš„epoch
                
                # ğŸ”§ ä¿®å¤ï¼šè®¡ç®—æ­£ç¡®çš„é˜¶æ®µå†…epochä½ç½®
                actual_epoch_in_phase = self.current_epoch - phase_start_epoch + 1
                
                if self.is_main_process:
                    print(f"\n--- Epoch {self.current_epoch + 1}/{total_epochs} "
                          f"(Phase: {phase.name}, {actual_epoch_in_phase}/{phase.epochs}) ---")
                
                # è®­ç»ƒ
                train_stats = self._train_epoch(phase, epoch_in_phase, loss_fn)
                
                # éªŒè¯
                val_results = self._validate_epoch(loss_fn, phase)
                val_tf_loss = val_results['tf_total_loss']
                val_gen_iou = val_results['generation_iou']
                val_tf_iou = val_results['tf_mean_iou']
                
                # å®Œå–„ç»Ÿè®¡ä¿¡æ¯
                train_stats.val_loss = val_tf_loss
                train_stats.val_generation_loss = 1.0 - val_gen_iou  # å°†IoUè½¬æ¢ä¸ºæŸå¤±å½¢å¼ï¼ˆç”¨äºå…¼å®¹ç°æœ‰ä»£ç ï¼‰
                train_stats.val_mean_iou = val_tf_iou
                
                # åˆ¤æ–­æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
                is_best_val = val_tf_loss < self.best_val_loss
                # ä½¿ç”¨IoUåˆ¤æ–­æœ€ä½³ç”Ÿæˆæ¨¡å‹ï¼šæ›´é«˜çš„IoU = æ›´å¥½çš„æ¨¡å‹
                is_best_generation = val_gen_iou > (1.0 - self.best_generation_loss)
                
                if is_best_val:
                    self.best_val_loss = val_tf_loss
                if is_best_generation:
                    # æ›´æ–°æœ€ä½³ç”ŸæˆæŸå¤±ï¼ˆå­˜å‚¨ä¸º1-IoUçš„å½¢å¼ï¼Œä¿æŒä¸ç°æœ‰ä»£ç å…¼å®¹ï¼‰
                    self.best_generation_loss = 1.0 - val_gen_iou
                
                # SwanLabæ—¥å¿—è®°å½•
                if self.use_swanlab:
                    # è®­ç»ƒlossç»„ä»¶
                    swanlab.log({
                        'train/total_loss': train_stats.train_loss,
                        'train/classification_loss': train_stats.train_classification_loss,
                        'train/iou_loss': train_stats.train_iou_loss,
                        'train/delta_loss': train_stats.train_delta_loss,
                        'train/eos_loss': train_stats.train_eos_loss,
                        'train/mean_iou': train_stats.train_mean_iou,
                        
                        # ğŸ” æ·»åŠ æ—‹è½¬è§’åº¦æŸå¤±è®°å½•
                        'train/roll_cls_loss': train_stats.train_roll_cls_loss,
                        'train/pitch_cls_loss': train_stats.train_pitch_cls_loss,
                        'train/yaw_cls_loss': train_stats.train_yaw_cls_loss,
                        'train/roll_delta_loss': train_stats.train_roll_delta_loss,
                        'train/pitch_delta_loss': train_stats.train_pitch_delta_loss,
                        'train/yaw_delta_loss': train_stats.train_yaw_delta_loss,
                        
                        # Teacher ForcingéªŒè¯lossç»„ä»¶  
                        'val/tf_total_loss': val_results['tf_total_loss'],
                        'val/tf_classification_loss': val_results['tf_classification_loss'],
                        'val/tf_iou_loss': val_results['tf_iou_loss'],
                        'val/tf_delta_loss': val_results['tf_delta_loss'],
                        'val/tf_eos_loss': val_results['tf_eos_loss'],
                        'val/tf_mean_iou': val_results['tf_mean_iou'],
                        
                        # AutoregressiveéªŒè¯lossç»„ä»¶
                        'val/ar_mean_iou': val_results['generation_iou'],
                        # ç”Ÿæˆç»Ÿè®¡
                        'val/generation_rate': val_results['avg_generated_boxes'] / max(val_results['avg_gt_boxes'], 1),
                        # ç»´åº¦è¯¯å·®æŒ‡æ ‡
                        'val/overall_mean_error': val_results.get('avg_overall_error', 0.0),
                        'val/x_error': val_results.get('avg_x_error', 0.0),
                        'val/y_error': val_results.get('avg_y_error', 0.0),
                        'val/z_error': val_results.get('avg_z_error', 0.0),
                        'val/w_error': val_results.get('avg_w_error', 0.0),
                        'val/h_error': val_results.get('avg_h_error', 0.0),
                        'val/l_error': val_results.get('avg_l_error', 0.0),
                        'val/roll_error': val_results.get('avg_roll_error', 0.0),
                        'val/pitch_error': val_results.get('avg_pitch_error', 0.0),
                        'val/yaw_error': val_results.get('avg_yaw_error', 0.0),
                        
                        # è®­ç»ƒå‚æ•°
                        'train/teacher_forcing_ratio': train_stats.teacher_forcing_ratio,
                        'train/learning_rate': train_stats.learning_rate,
                        'train/adaptive_cls_weight': train_stats.adaptive_cls_weight,
                        'train/adaptive_delta_weight': train_stats.adaptive_delta_weight,
                        
                        # epochä¿¡æ¯
                        # 'epoch': self.current_epoch,
                        # 'phase_idx': phase_idx,
                        'epoch_time': train_stats.epoch_time
                    }, step=self.current_epoch)  # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ stepå‚æ•°ç¡®ä¿resumeæ—¶æ­£ç¡®ç»­æ¥

                # æ§åˆ¶å°è¾“å‡ºè¯¦ç»†ä¿¡æ¯
                # æ§åˆ¶å°è¾“å‡ºè¯¦ç»†ä¿¡æ¯ - åªåœ¨ä¸»è¿›ç¨‹æ‰“å°
                if self.is_main_process:
                    print(f"ğŸ“Š è®­ç»ƒæŸå¤±è¯¦æƒ…:")
                    print(f"   æ€»æŸå¤±: {train_stats.train_loss:.4f}")
                    print(f"   åˆ†ç±»æŸå¤±: {train_stats.train_classification_loss:.4f}")
                    print(f"   IoUæŸå¤±: {train_stats.train_iou_loss:.4f}")
                    print(f"   DeltaæŸå¤±: {train_stats.train_delta_loss:.4f}")
                    print(f"ğŸ¯ éªŒè¯æŸå¤±è¯¦æƒ…:")
                    print(f"   TFæ€»æŸå¤±: {val_results['tf_total_loss']:.4f}")
                    print(f"   TFåˆ†ç±»: {val_results['tf_classification_loss']:.4f} | TF IoU: {val_results['tf_iou_loss']:.4f} | TF Delta: {val_results['tf_delta_loss']:.4f} | TF EOS: {val_results['tf_eos_loss']:.4f}")
                    print(f"   TF mean IoU: {val_results['tf_mean_iou']:.4f}")
                    print(f"ğŸ“Š ç”Ÿæˆæ¨¡å¼è¯¦æƒ…:")
                    print(f"   ç”ŸæˆIoUè¯„ä¼°: {val_results['generation_iou']:.4f}")
                    print(f"ğŸ“ ç»´åº¦è¯¯å·®è¯¦æƒ…:")
                    print(f"   æ€»ä½“å¹³å‡è¯¯å·®: {val_results.get('avg_overall_error', 0.0):.4f}")
                    print(f"   Xè¯¯å·®: {val_results.get('avg_x_error', 0.0):.4f} | Yè¯¯å·®: {val_results.get('avg_y_error', 0.0):.4f} | Zè¯¯å·®: {val_results.get('avg_z_error', 0.0):.4f}")
                    print(f"   Wè¯¯å·®: {val_results.get('avg_w_error', 0.0):.4f} | Hè¯¯å·®: {val_results.get('avg_h_error', 0.0):.4f} | Lè¯¯å·®: {val_results.get('avg_l_error', 0.0):.4f}")
                    print(f"   æ—‹è½¬è¯¯å·®: Roll={val_results.get('avg_roll_error', 0.0):.4f} | Pitch={val_results.get('avg_pitch_error', 0.0):.4f} | Yaw={val_results.get('avg_yaw_error', 0.0):.4f}")
                    print(f"ğŸ“¦ ç®±å­ç»Ÿè®¡:")
                    print(f"   å¹³å‡ç”Ÿæˆæ•°é‡: {val_results['avg_generated_boxes']:.1f} | å¹³å‡GTæ•°é‡: {val_results['avg_gt_boxes']:.1f} | ç”Ÿæˆç‡: {val_results['avg_generated_boxes']/max(val_results['avg_gt_boxes'], 1):.2f}")
                    print(f"âš–ï¸  æƒé‡è¯¦æƒ…:")
                    print(f"   è‡ªé€‚åº”åˆ†ç±»æƒé‡: {train_stats.adaptive_cls_weight:.3f}")
                    print(f"   è‡ªé€‚åº”Deltaæƒé‡: {train_stats.adaptive_delta_weight:.3f}")
                    print(f"   TFæ¯”ä¾‹: {train_stats.teacher_forcing_ratio:.3f} | å­¦ä¹ ç‡: {train_stats.learning_rate:.2e}")
                
                # ä¿å­˜checkpoint
                if (self.current_epoch + 1) % self.training_config.get('save_interval', 10) == 0:
                    self._save_checkpoint(is_best_val, is_best_generation)
                elif is_best_val or is_best_generation:
                    self._save_checkpoint(is_best_val, is_best_generation)
                
                # åœ¨è·å¾—æœ€ä½³æ¨¡å‹æ—¶åœ¨testé›†ä¸Šè¿›è¡Œæ¨ç†
                self._evaluate_best_model_on_test(is_best_val, is_best_generation)
                
                self.current_epoch += 1
        
        if self.is_main_process:
            print("\nğŸ‰ è®­ç»ƒå®Œæˆ!")
            print(f"ğŸ“Š æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.4f}")
            print(f"ğŸ¯ æœ€ä½³ç”ŸæˆæŸå¤±: {self.best_generation_loss:.4f}")
        
        # ä¿å­˜æœ€ç»ˆcheckpoint
        if self.is_main_process:
            self._save_checkpoint()
        
        # åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæ¨ç†ï¼ˆè®­ç»ƒç»“æŸæ—¶çš„æœ€ç»ˆè¯„ä¼°ï¼‰
        if self.is_main_process:
            print("\nğŸ§ª è®­ç»ƒç»“æŸï¼Œå¼€å§‹åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆæ¨ç†...")
        test_results = self._test_inference()
        
        # è®°å½•æµ‹è¯•ç»“æœåˆ°SwanLab
        if self.use_swanlab:
            swanlab.log({
                'test/generation_iou': test_results['generation_iou'],
                'test/overall_mean_error': test_results.get('avg_overall_error', 0.0),
                'test/x_error': test_results.get('avg_x_error', 0.0),
                'test/y_error': test_results.get('avg_y_error', 0.0),
                'test/z_error': test_results.get('avg_z_error', 0.0),
                'test/w_error': test_results.get('avg_w_error', 0.0),
                'test/h_error': test_results.get('avg_h_error', 0.0),
                'test/l_error': test_results.get('avg_l_error', 0.0),
                'test/roll_error': test_results.get('avg_roll_error', 0.0),
                'test/pitch_error': test_results.get('avg_pitch_error', 0.0),
                'test/yaw_error': test_results.get('avg_yaw_error', 0.0),
                'test/avg_generated_boxes': test_results.get('avg_generated_boxes', 0.0),
                'test/avg_gt_boxes': test_results.get('avg_gt_boxes', 0.0),
                'test/generation_rate': test_results.get('avg_generated_boxes', 0.0) / max(test_results.get('avg_gt_boxes', 1), 1)
            }, step=self.current_epoch)  # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ stepå‚æ•°
            swanlab.finish()


def setup_distributed(local_rank: int, world_size: int):
    """è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒ"""
    # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰ç¯å¢ƒå˜é‡è®¾ç½®
    if 'MASTER_ADDR' not in os.environ:
        os.environ['MASTER_ADDR'] = 'localhost'
    if 'MASTER_PORT' not in os.environ:
        os.environ['MASTER_PORT'] = '12355'
    
    # æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
    if not torch.cuda.is_available():
        print(f"âš ï¸  CUDAä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ")
        return False
    
    # æ£€æŸ¥GPUæ•°é‡
    if torch.cuda.device_count() < world_size:
        print(f"âš ï¸  GPUæ•°é‡({torch.cuda.device_count()})å°‘äºworld_size({world_size})ï¼Œæ— æ³•è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ")
        return False
    
    try:
        dist.init_process_group(
            backend='nccl',
            rank=local_rank,
            world_size=world_size
        )
        print(f"âœ… åˆ†å¸ƒå¼è®­ç»ƒåˆå§‹åŒ–æˆåŠŸ (Rank {local_rank}/{world_size})")
        return True
    except Exception as e:
        print(f"âŒ åˆ†å¸ƒå¼è®­ç»ƒåˆå§‹åŒ–å¤±è´¥: {e}")
        return False


def cleanup_distributed():
    """æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒ"""
    if dist.is_initialized():
        dist.destroy_process_group()


def main(
    config_dir: str = ".",
    experiment_name: str = "primitive_3d_exp",
    resume_from: Optional[str] = None,
    local_rank: int = 0,
    world_size: int = 1,
    validation_samples: List[int] = None
):
    """ä¸»è®­ç»ƒå‡½æ•°"""
    
    # è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒ
    distributed_success = False
    if world_size > 1:
        distributed_success = setup_distributed(local_rank, world_size)
        if not distributed_success:
            print(f"âš ï¸  åˆ†å¸ƒå¼è®­ç»ƒè®¾ç½®å¤±è´¥ï¼Œå›é€€åˆ°å•GPUæ¨¡å¼")
            world_size = 1
            local_rank = 0
    
    try:
        # è®¾ç½®éšæœºç§å­
        torch.manual_seed(42 + local_rank)
        np.random.seed(42 + local_rank)
        random.seed(42 + local_rank)
        
        # åŠ è½½ç»Ÿä¸€é…ç½® - åªåœ¨ä¸»è¿›ç¨‹æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        config_loader = ConfigLoader()
        config_loader.load_unified_config(os.path.join(config_dir, "training_config.yaml"))
        
        if local_rank == 0:
            print(f"âœ… æˆåŠŸåŠ è½½ç»Ÿä¸€é…ç½®æ–‡ä»¶: {os.path.join(config_dir, 'training_config.yaml')}")
            # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
            global_config = config_loader.get_global_config()
            print(f"ğŸ“Š é…ç½®ç‰ˆæœ¬: {global_config.get('version', 'unknown')}")
            
            # æ˜¾ç¤ºè®­ç»ƒé˜¶æ®µä¿¡æ¯
            phases_config = config_loader.get_training_phases()
            total_epochs = sum(phase.get('epochs', 0) for phase in phases_config.values())
            print(f"ğŸ¯ è®­ç»ƒé˜¶æ®µ: {len(phases_config)}ä¸ªé˜¶æ®µï¼Œæ€»è®¡{total_epochs}ä¸ªepoch")
            for phase_name, phase_config in phases_config.items():
                print(f"  - {phase_name}: {phase_config.get('epochs', 0)} epochs")
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = AdvancedTrainer(
            config_loader=config_loader,
            experiment_name=experiment_name,
            resume_from=resume_from,
            local_rank=local_rank,
            world_size=world_size,
            use_swanlab=True,
            validation_samples=validation_samples or [0, 1, 2, 3, 4]
        )
        
        # å¼€å§‹è®­ç»ƒ
        trainer.train()
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        raise
    finally:
        # æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒ
        if distributed_success:
            cleanup_distributed()


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="3D Primitive Detection Training")
    parser.add_argument("--config-dir", default=".", help="é…ç½®æ–‡ä»¶ç›®å½•")
    parser.add_argument("--experiment-name", default="primitive_3d_exp", help="å®éªŒåç§°")
    parser.add_argument("--resume-from", default=None, help="æ¢å¤è®­ç»ƒçš„checkpointè·¯å¾„")
    parser.add_argument("--local-rank", type=int, default=0, help="æœ¬åœ°GPUæ’å")
    parser.add_argument("--world-size", type=int, default=1, help="æ€»GPUæ•°é‡")
    parser.add_argument("--validation-samples", nargs='+', type=int, default=[0, 1, 2, 3, 4], 
                       help="éªŒè¯å¯è§†åŒ–æ ·æœ¬ç´¢å¼•")
    
    args = parser.parse_args()
    
    main(
        config_dir=args.config_dir,
        experiment_name=args.experiment_name,
        resume_from=args.resume_from,
        local_rank=args.local_rank,
        world_size=args.world_size,
        validation_samples=args.validation_samples
    ) 