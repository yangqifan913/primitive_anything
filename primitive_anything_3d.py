# -*- coding: utf-8 -*-
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import Tensor
import numpy as np
import math
from typing import List, Tuple, Dict, Optional
from einops import rearrange, repeat, pack
from x_transformers import Decoder
from x_transformers.autoregressive_wrapper import eval_decorator
import torchvision.models as models
from gateloop_transformer import SimpleGateLoopLayer
from torch.amp import autocast
from torch.utils.checkpoint import checkpoint
from dataclasses import dataclass

class FiLM(nn.Module):
    """Feature-wise Linear Modulation layer - ä¸PrimitiveAnythingæºç ä¿æŒä¸€è‡´"""
    def __init__(self, dim, dim_out=None):
        super().__init__()
        dim_out = dim_out or dim
        
        self.to_gamma = nn.Linear(dim, dim_out, bias=False)
        self.to_beta = nn.Linear(dim, dim_out)
        
        self.gamma_mult = nn.Parameter(torch.zeros(1,))
        self.beta_mult = nn.Parameter(torch.zeros(1,))
        
    def forward(self, x, cond):
        """
        Args:
            x: [B, seq_len, dim] - è¾“å…¥ç‰¹å¾
            cond: [B, cond_dim] - æ¡ä»¶ç‰¹å¾
        Returns:
            modulated_x: [B, seq_len, dim_out] - è°ƒåˆ¶åçš„ç‰¹å¾
        """
        gamma, beta = self.to_gamma(cond), self.to_beta(cond)
        gamma, beta = tuple(rearrange(t, 'b d -> b 1 d') for t in (gamma, beta))
        
        # åˆå§‹åŒ–åˆ°æ’ç­‰æ˜ å°„
        gamma = (1 + self.gamma_mult * gamma.tanh())
        beta = beta.tanh() * self.beta_mult
        
        # ç»å…¸FiLM
        return x * gamma + beta

class GateLoopBlock(nn.Module):
    """é—¨æ§å¾ªç¯å— - å‚è€ƒPrimitiveAnythingå®ç°ï¼Œæ”¯æŒç¼“å­˜"""
    def __init__(self, dim, depth=2, use_heinsen=False):
        super().__init__()
        self.gateloops = nn.ModuleList([])

        for _ in range(depth):
            gateloop = SimpleGateLoopLayer(dim=dim, use_heinsen=use_heinsen)
            self.gateloops.append(gateloop)

    def forward(self, x, cache=None):
        """
        å‚è€ƒPrimitiveAnythingçš„GateLoopBlockå®ç°
        
        Args:
            x: è¾“å…¥tensor [B, seq_len, dim]
            cache: é—¨æ§å¾ªç¯çš„ç¼“å­˜åˆ—è¡¨
            
        Returns:
            x: è¾“å‡ºtensor
            new_caches: æ›´æ–°åçš„ç¼“å­˜åˆ—è¡¨
        """
        received_cache = cache is not None and len(cache) > 0

        if x.numel() == 0:  # ç©ºtensoræ£€æŸ¥
            return x, cache

        if received_cache:
            # å¦‚æœæœ‰ç¼“å­˜ï¼Œåˆ†ç¦»ä¹‹å‰çš„åºåˆ—å’Œæ–°token
            prev, x = x[:, :-1], x[:, -1:]

        cache = cache if cache is not None else []
        cache_iter = iter(cache)

        new_caches = []
        for gateloop in self.gateloops:
            layer_cache = next(cache_iter, None)
            # æ£€æŸ¥gateloopæ˜¯å¦æ”¯æŒcacheå’Œreturn_cacheå‚æ•°
            if hasattr(gateloop, 'forward') and 'cache' in gateloop.forward.__code__.co_varnames:
                try:
                    out, new_cache = gateloop(x, cache=layer_cache, return_cache=True)
                    new_caches.append(new_cache)
                except TypeError:
                    # å¦‚æœä¸æ”¯æŒreturn_cacheï¼Œä½¿ç”¨æ™®é€šå‰å‘ä¼ æ’­
                    out = gateloop(x)
                    new_caches.append(None)
            else:
                # æ™®é€šå‰å‘ä¼ æ’­
                out = gateloop(x)
                new_caches.append(None)
            
            x = x + out

        if received_cache:
            # å¦‚æœæœ‰ç¼“å­˜ï¼Œå°†ä¹‹å‰çš„åºåˆ—ä¸æ–°å¤„ç†çš„tokenè¿æ¥
            x = torch.cat((prev, x), dim=-2)

        return x, new_caches


class EnhancedFPN(nn.Module):
    """è¶…è½»é‡ç‰ˆFeature Pyramid Network - å¤§å¹…é™ä½å†…å­˜å ç”¨"""
    def __init__(self, in_channels, out_channels=32, attention_heads=2, attention_layers=None, 
                 smooth_conv_kernel=3, smooth_conv_padding=1, backbone_channels=None):  # æ”¯æŒé…ç½®åŒ–
        super().__init__()
        self.out_channels = out_channels
        
        # ä½¿ç”¨ä¼ å…¥çš„backbone_channelsï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
        if backbone_channels is None:
            backbone_channels = [256, 512, 1024, 2048]  # ResNet50çš„é»˜è®¤é€šé“æ•°
        
        # ä¾§è¾¹è¿æ¥å±‚ - å°†ä¸åŒå±‚çš„ç‰¹å¾ç»Ÿä¸€åˆ°ç›¸åŒé€šé“æ•°
        self.lateral_convs = nn.ModuleList([
            nn.Conv2d(backbone_channels[0], out_channels, 1),   # layer1
            nn.Conv2d(backbone_channels[1], out_channels, 1),   # layer2
            nn.Conv2d(backbone_channels[2], out_channels, 1),    # layer3
            nn.Conv2d(backbone_channels[3], out_channels, 1),   # layer4
        ])
        
        # ç®€åŒ–çš„å¹³æ»‘å±‚ - ä½¿ç”¨é…ç½®ä¸­çš„å‚æ•°
        self.smooth_convs = nn.ModuleList([
            nn.Conv2d(out_channels, out_channels, smooth_conv_kernel, padding=smooth_conv_padding)
            for _ in range(4)
        ])
        
        # ç§»é™¤é¢å¤–çš„å·ç§¯å±‚
        self.extra_convs = nn.ModuleList([
            nn.Identity() for _ in range(4)
        ])
        
        # æ³¨æ„åŠ›æœºåˆ¶é…ç½®åŒ–
        if attention_layers is None:
            attention_layers = [2, 3]  # é»˜è®¤åœ¨layer3å’Œlayer4ä½¿ç”¨æ³¨æ„åŠ›
        self.attention_layers = attention_layers
        self.attention_blocks = nn.ModuleList([
            nn.MultiheadAttention(out_channels, num_heads=attention_heads, batch_first=True)
            for _ in range(len(attention_layers))
        ])
        
    def forward(self, features):
        """
        Args:
            features: List of tensors from different layers
                     [layer1_feat, layer2_feat, layer3_feat, layer4_feat]
        Returns:
            fpn_features: List of FPN features at different scales
        """
        # ä»é«˜å±‚åˆ°ä½å±‚å¤„ç†
        laterals = []
        for i, (feat, lateral_conv) in enumerate(zip(features, self.lateral_convs)):
            lateral = lateral_conv(feat)
            laterals.append(lateral)
        
        # è‡ªé¡¶å‘ä¸‹è·¯å¾„
        for i in range(len(laterals) - 2, -1, -1):
            # ä¸Šé‡‡æ ·é«˜å±‚ç‰¹å¾
            upsampled = F.interpolate(
                laterals[i + 1], 
                size=laterals[i].shape[-2:], 
                mode='nearest'
            )
            # æ·»åŠ ä¾§è¾¹è¿æ¥
            laterals[i] = laterals[i] + upsampled
        
        # å¹³æ»‘å¤„ç†å’Œæ³¨æ„åŠ›
        fpn_features = []
        attention_idx = 0  # æ³¨æ„åŠ›æ¨¡å—çš„ç´¢å¼•
        
        for i, (lateral, smooth_conv, extra_conv) in enumerate(
            zip(laterals, self.smooth_convs, self.extra_convs)
        ):
            # å¹³æ»‘å¤„ç†
            smoothed = F.relu(smooth_conv(lateral))
            # é¢å¤–å·ç§¯ï¼ˆç°åœ¨æ˜¯Identityï¼‰
            extra = extra_conv(smoothed)
            
            # å¯é…ç½®çš„æ³¨æ„åŠ›æœºåˆ¶
            if i in self.attention_layers and attention_idx < len(self.attention_blocks):
                b, c, h, w = extra.shape
                extra_flat = extra.view(b, c, h*w).permute(0, 2, 1)  # [B, H*W, C]
                attended, _ = self.attention_blocks[attention_idx](extra_flat, extra_flat, extra_flat)
                attended = attended.permute(0, 2, 1).view(b, c, h, w)  # [B, C, H, W]
                fpn_features.append(attended)
                attention_idx += 1
            else:
                # ä¸ä½¿ç”¨æ³¨æ„åŠ›çš„å±‚ç›´æ¥è¾“å‡º
                fpn_features.append(extra)
        
        return fpn_features

class DeepVisualProcessor(nn.Module):
    """ç®€åŒ–ç‰ˆè§†è§‰ç‰¹å¾å¤„ç†å™¨"""
    def __init__(self, in_channels, out_channels):
        super().__init__()
        
        # ç®€åŒ–çš„å·ç§¯ç½‘ç»œ
        self.conv_layers = nn.Sequential(
            # å•å±‚å¤„ç†
            nn.Conv2d(in_channels, out_channels, 3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
        )
        
        # æ®‹å·®è¿æ¥
        self.shortcut = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()
        
    def forward(self, x):
        residual = self.shortcut(x)
        out = self.conv_layers(x)
        return out + residual

class ImageEncoder(nn.Module):
    """RGBå›¾åƒç¼–ç å™¨ - åªå¤„ç†RGBæ•°æ®ï¼Œæ”¯æŒå¤šç§ResNet backbone + ç®€åŒ–FPN + ç®€åŒ–å¤„ç†å™¨"""
    def __init__(self, input_channels=3, output_dim=256, use_fpn=True, backbone="resnet50", pretrained=True, 
                 fpn_output_channels=128, fpn=None, conv1=None, **kwargs):
        super().__init__()
        self.use_fpn = use_fpn
        self.backbone = backbone
        self.pretrained = pretrained
        self.fpn_output_channels = fpn_output_channels
        self.fpn_config = fpn or {}
        self.conv1_config = conv1 or {}
        
        # æ ¹æ®é…ç½®é€‰æ‹©backbone
        if backbone.lower() == "resnet18":
            if pretrained:
                self.backbone_model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
            else:
                self.backbone_model = models.resnet18(weights=None)
            self.backbone_channels = [64, 128, 256, 512]  # ResNet18çš„é€šé“æ•°
        elif backbone.lower() == "resnet34":
            if pretrained:
                self.backbone_model = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)
            else:
                self.backbone_model = models.resnet34(weights=None)
            self.backbone_channels = [64, 128, 256, 512]  # ResNet34çš„é€šé“æ•°
        elif backbone.lower() == "resnet50":
            if pretrained:
                self.backbone_model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)
            else:
                self.backbone_model = models.resnet50(weights=None)
            self.backbone_channels = [256, 512, 1024, 2048]  # ResNet50çš„é€šé“æ•°
        elif backbone.lower() == "resnet101":
            if pretrained:
                self.backbone_model = models.resnet101(weights=models.ResNet101_Weights.IMAGENET1K_V1)
            else:
                self.backbone_model = models.resnet101(weights=None)
            self.backbone_channels = [256, 512, 1024, 2048]  # ResNet101çš„é€šé“æ•°
        else:
            raise ValueError(f"Unsupported backbone: {backbone}. Supported: resnet18, resnet34, resnet50, resnet101")
        
        # ç¡®ä¿è¾“å…¥é€šé“æ•°ä¸º3ï¼ˆRGBï¼‰
        assert input_channels == 3, "ImageEncoderåªå¤„ç†RGBæ•°æ®ï¼Œinput_channelså¿…é¡»ä¸º3"
        
        # ç®€åŒ–FPNæ¨¡å—
        if use_fpn:
            # æ ¹æ®backboneé€‰æ‹©FPNçš„è¾“å…¥é€šé“æ•°
            if backbone.lower() in ["resnet18", "resnet34"]:
                fpn_in_channels = 512  # ResNet18/34çš„æœ€åä¸€å±‚é€šé“æ•°
            else:  # resnet50, resnet101
                fpn_in_channels = 2048  # ResNet50/101çš„æœ€åä¸€å±‚é€šé“æ•°
            
            # ä½¿ç”¨é…ç½®ä¸­çš„FPNå‚æ•°
            fpn_out_channels = self.fpn_config.get('output_channels', 32)
            attention_heads = self.fpn_config.get('attention_heads', 2)
            attention_layers = self.fpn_config.get('attention_layers', [2, 3])
            smooth_conv_kernel = self.fpn_config.get('smooth_conv_kernel', 3)
            smooth_conv_padding = self.fpn_config.get('smooth_conv_padding', 1)
            
            self.fpn = EnhancedFPN(
                in_channels=fpn_in_channels, 
                out_channels=fpn_out_channels,
                attention_heads=attention_heads,
                attention_layers=attention_layers,
                smooth_conv_kernel=smooth_conv_kernel,
                smooth_conv_padding=smooth_conv_padding,
                backbone_channels=self.backbone_channels
            )
            self.feature_dim = fpn_out_channels
        else:
            # æ ¹æ®backboneé€‰æ‹©ç‰¹å¾ç»´åº¦
            if backbone.lower() in ["resnet18", "resnet34"]:
                self.feature_dim = 512
            else:  # resnet50, resnet101
                self.feature_dim = 2048
        
        # ç®€åŒ–æ·±å±‚å¤„ç†å™¨
        self.deep_processor = DeepVisualProcessor(self.feature_dim, output_dim)
        
        # ç©ºé—´ç‰¹å¾æŠ•å½±å±‚
        self.spatial_projection = nn.Conv2d(output_dim, output_dim, kernel_size=1)
        
        print(f"Enhanced ImageEncoder: {backbone.upper()} + {'Ultra-Lightweight FPN (32ch)' if use_fpn else 'No FPN'} + SimpleProcessor -> {self.feature_dim} -> {output_dim} (spatial features)")
        
        # æ‰“å°FPNé…ç½®ä¿¡æ¯
        if use_fpn and self.fpn_config:
            print(f"FPNé…ç½®: attention_layers={self.fpn_config.get('attention_layers', [2, 3])}, "
                  f"attention_heads={self.fpn_config.get('attention_heads', 2)}, "
                  f"smooth_conv_kernel={self.fpn_config.get('smooth_conv_kernel', 3)}")
        
    def forward(self, x, return_2d_features=False):
        # è·å–ResNetçš„å·ç§¯ç‰¹å¾å›¾
        x = self.backbone_model.conv1(x)
        x = self.backbone_model.bn1(x)
        x = self.backbone_model.relu(x)
        x = self.backbone_model.maxpool(x)
        
        # æå–ä¸åŒå±‚çš„ç‰¹å¾
        layer1_feat = self.backbone_model.layer1(x)      # [B, C1, H/4, W/4]
        layer2_feat = self.backbone_model.layer2(layer1_feat)  # [B, C2, H/8, W/8]
        layer3_feat = self.backbone_model.layer3(layer2_feat)  # [B, C3, H/16, W/16]
        layer4_feat = self.backbone_model.layer4(layer3_feat)  # [B, C4, H/32, W/32]
        
        if self.use_fpn:
            # ä½¿ç”¨ç®€åŒ–FPNå¤„ç†å¤šå°ºåº¦ç‰¹å¾
            fpn_features = self.fpn([layer1_feat, layer2_feat, layer3_feat, layer4_feat])
            
            # é€‰æ‹©æœ€ç»†ç²’åº¦çš„ç‰¹å¾ï¼ˆæœ€é«˜åˆ†è¾¨ç‡ï¼‰
            output = fpn_features[0]  # [B, 64, H/4, W/4]
        else:
            # ä¸ä½¿ç”¨FPNï¼Œç›´æ¥ä½¿ç”¨æœ€åä¸€å±‚ç‰¹å¾
            output = layer4_feat  # [B, 512, H/32, W/32]
        
        # ç®€åŒ–æ·±å±‚å¤„ç†
        output = self.deep_processor(output)  # [B, output_dim, H, W]
        
        # æŠ•å½±åˆ°ç›®æ ‡ç»´åº¦
        output = self.spatial_projection(output)  # [B, output_dim, H, W]
        
        if return_2d_features:
            return output  # è¿”å›2Dç‰¹å¾å›¾ [B, output_dim, H, W]
        
        # å°†ç©ºé—´ç‰¹å¾å±•å¹³ä¸ºåºåˆ— [batch_size, H*W, output_dim]
        batch_size, channels, height, width = output.shape
        output = output.permute(0, 2, 3, 1).contiguous()  # [B, H, W, channels]
        output = output.view(batch_size, height * width, channels)  # [B, H*W, channels]
        
        return output


class PointCloudEncoder(nn.Module):
    """ç‚¹äº‘ç¼–ç å™¨ - ä½¿ç”¨PointTransformerå¤„ç†ç‚¹äº‘æ•°æ®"""
    def __init__(self, output_dim=256, point_cloud_encoder_config=None):
        super().__init__()
        self.output_dim = output_dim
        
        # å¯¼å…¥PointTransformer
        from point_transformer import PointTransformerV3
        
        # åˆ›å»ºç‚¹äº‘ç¼–ç å™¨
        final_point_cloud_config = point_cloud_encoder_config.copy()
        
        # ç¡®ä¿å‚æ•°ç±»å‹æ­£ç¡®ï¼ˆå…ƒç»„è€Œä¸æ˜¯åˆ—è¡¨ï¼‰
        if 'order' in final_point_cloud_config and isinstance(final_point_cloud_config['order'], list):
            final_point_cloud_config['order'] = tuple(final_point_cloud_config['order'])
        if 'stride' in final_point_cloud_config and isinstance(final_point_cloud_config['stride'], list):
            final_point_cloud_config['stride'] = tuple(final_point_cloud_config['stride'])
        if 'enc_depths' in final_point_cloud_config and isinstance(final_point_cloud_config['enc_depths'], list):
            final_point_cloud_config['enc_depths'] = tuple(final_point_cloud_config['enc_depths'])
        if 'enc_channels' in final_point_cloud_config and isinstance(final_point_cloud_config['enc_channels'], list):
            final_point_cloud_config['enc_channels'] = tuple(final_point_cloud_config['enc_channels'])
        if 'enc_num_head' in final_point_cloud_config and isinstance(final_point_cloud_config['enc_num_head'], list):
            final_point_cloud_config['enc_num_head'] = tuple(final_point_cloud_config['enc_num_head'])
        if 'enc_patch_size' in final_point_cloud_config and isinstance(final_point_cloud_config['enc_patch_size'], list):
            final_point_cloud_config['enc_patch_size'] = tuple(final_point_cloud_config['enc_patch_size'])
        
        self.point_cloud_encoder = PointTransformerV3(**final_point_cloud_config)
        
        # æ·»åŠ æŠ•å½±å±‚å°†PointTransformerV3çš„è¾“å‡ºæ˜ å°„åˆ°æœŸæœ›çš„è¾“å‡ºç»´åº¦
        transformer_output_dim = final_point_cloud_config.get('enc_channels', (32, 64, 128, 256, 512))[-1]
        self.output_projection = nn.Linear(transformer_output_dim, output_dim)
        
        print(f"PointCloudEncoder: PointTransformer({transformer_output_dim}) -> Linear -> {output_dim} (point cloud features)")
        
        # æ‰“å°ç‚¹äº‘ç¼–ç å™¨é…ç½®ä¿¡æ¯
        if point_cloud_encoder_config:
            print(f"PointCloudé…ç½®:")
            print(f"  åŸºç¡€å‚æ•°: in_channels={final_point_cloud_config.get('in_channels', 3)}, "
                  f"num_classes={final_point_cloud_config.get('num_classes', output_dim)}")
            print(f"  ç¼–ç å™¨: depths={final_point_cloud_config.get('enc_depths', [2, 2, 2, 6, 2])}, "
                  f"channels={final_point_cloud_config.get('enc_channels', [32, 64, 128, 256, 512])}")
            print(f"  æ³¨æ„åŠ›: heads={final_point_cloud_config.get('enc_num_head', [2, 4, 8, 16, 32])}, "
                  f"mlp_ratio={final_point_cloud_config.get('mlp_ratio', 4)}")
            print(f"  Dropout: attn_drop={final_point_cloud_config.get('attn_drop', 0.0)}, "
                  f"drop_path={final_point_cloud_config.get('drop_path', 0.3)}")
        
    def forward(self, data_dict):
        """
        å¤„ç†ç‚¹äº‘æ•°æ®ï¼Œç›´æ¥ä¼ é€’data_dictç»™PointTransformerV3
        
        Args:
            data_dict: Dict - ç‚¹äº‘æ•°æ®å­—å…¸ï¼ŒåŒ…å«coord, grid_coord, offset, featç­‰å­—æ®µ
            
        Returns:
            features: List[Tensor] - ç‚¹äº‘ç‰¹å¾åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯[n_feat, output_dim]
            pixel_coords_out: List[List[Tensor]] - åƒç´ åæ ‡åˆ—è¡¨ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…å«n_featä¸ªç‰¹å¾å¯¹åº”çš„åƒç´ åæ ‡
        """
        # åˆ†ç¦»pixel_coordsï¼Œä¸è¾“å…¥åˆ°point_cloud_encoderä¸­
        pixel_coords = data_dict.get('pixel_coords', None)
        # data_dict_for_encoder = {k: v for k, v in data_dict.items() if k != 'pixel_coords'}
        # ç›´æ¥é€šè¿‡ç‚¹äº‘ç¼–ç å™¨å¤„ç†æ•´ä¸ªbatchï¼ˆä¸åŒ…å«pixel_coordsï¼‰
        point, orig2cur = self.point_cloud_encoder(data_dict)
        # æå–ç‰¹å¾ - æŒ‰featureåˆ†ç»„ï¼Œæ¯ä¸ªfeatureå¯¹åº”å¤šä¸ªpixel
        features = []
        pixel_coords_out = []
        
        # æ ¹æ®batchä¿¡æ¯åˆ†å‰²ç‰¹å¾
        from utils.utils import offset2batch
        original_batch = offset2batch(data_dict['offset'])
        batch_size = len(data_dict['offset'])  # offsetçš„é•¿åº¦å°±æ˜¯batch size
        
        for i in range(batch_size):
            # æ‰¾åˆ°å±äºç¬¬iä¸ªæ ·æœ¬çš„åŸå§‹ç‚¹
            sample_mask = original_batch == i
            # ä½¿ç”¨orig2curæ˜ å°„åˆ°å¤„ç†åçš„ç‰¹å¾
            sample_indices = orig2cur[sample_mask]  # [N_i] - æ¯ä¸ªåŸå§‹ç‚¹å¯¹åº”çš„featureç´¢å¼•
            sample_pixel_coords = pixel_coords[sample_mask]  # [N_i, 2]
            # æŒ‰featureåˆ†ç»„ï¼šæ¯ä¸ªfeatureå¯¹åº”å¤šä¸ªpixel
            unique_features, inverse_indices = torch.unique(sample_indices, return_inverse=True)
            # ä¸ºæ¯ä¸ªunique featureæ”¶é›†å¯¹åº”çš„pixelåæ ‡
            feature_pixels_list = []
            for feat_idx in unique_features:
                # æ‰¾åˆ°æ˜ å°„åˆ°å½“å‰featureçš„æ‰€æœ‰åŸå§‹ç‚¹
                point_mask = (sample_indices == feat_idx)
                feature_pixels = sample_pixel_coords[point_mask]  # [m, 2] - mä¸ªpixel
                feature_pixels_list.append(feature_pixels)
            # è·å–å¯¹åº”çš„ç‰¹å¾å‘é‡å¹¶æŠ•å½±åˆ°æœŸæœ›çš„è¾“å‡ºç»´åº¦
            sample_feat = point.feat[unique_features]  # [n_feat, transformer_output_dim]
            sample_feat = self.output_projection(sample_feat)  # [n_feat, output_dim]
            features.append(sample_feat)
            pixel_coords_out.append(feature_pixels_list)
        
        return features, pixel_coords_out
    
    # def get_feature_pixel_mapping(self, features, original_indices, pixel_coords_out):
    #     """
    #     å»ºç«‹ç‚¹äº‘ç‰¹å¾å’ŒåŸå›¾åƒç´ çš„å¯¹åº”å…³ç³»
        
    #     Args:
    #         features: List[Tensor] - ç‚¹äº‘ç‰¹å¾åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯[N_i, output_dim]
    #         original_indices: List[Tensor] - åŸå§‹ç´¢å¼•åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯[N_i]ï¼Œè¡¨ç¤ºæ¯ä¸ªç‚¹å¯¹åº”å“ªä¸ªfeature
    #         pixel_coords_out: List[Tensor] - åƒç´ åæ ‡åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯[N_i, 2]
            
    #     Returns:
    #         mapping: List[Dict] - æ¯ä¸ªæ ·æœ¬çš„å¯¹åº”å…³ç³»å­—å…¸åˆ—è¡¨
    #                 æ¯ä¸ªå­—å…¸çš„keyæ˜¯featureç´¢å¼•ï¼Œvalueæ˜¯åŒ…å«è¯¥featureçš„æ‰€æœ‰ç‚¹çš„ä¿¡æ¯
    #                 æ ¼å¼: {feature_idx: {'feature': Tensor, 'points': List[Dict]}}
    #     """
    #     mapping = []
        
    #     for i in range(len(features)):
    #         pc_feat_i = features[i]           # [N_i, output_dim]
    #         orig_idx_i = original_indices[i]  # [N_i]
    #         pixel_coord_i = pixel_coords_out[i]  # [N_i, 2]
            
    #         # æŒ‰featureç´¢å¼•åˆ†ç»„
    #         feature_groups = {}
            
    #         for j in range(pc_feat_i.shape[0]):
    #             feature_idx = orig_idx_i[j].item()
                
    #             if feature_idx not in feature_groups:
    #                 feature_groups[feature_idx] = {
    #                     'feature': pc_feat_i[j],  # è¿™ä¸ªfeatureçš„ç‰¹å¾å‘é‡
    #                     'points': []  # å±äºè¿™ä¸ªfeatureçš„æ‰€æœ‰ç‚¹
    #                 }
                
    #             # æ·»åŠ è¿™ä¸ªç‚¹çš„ä¿¡æ¯
    #             feature_groups[feature_idx]['points'].append({
    #                 'point_idx': j,  # åœ¨åŸå§‹ç‚¹äº‘ä¸­çš„ç´¢å¼•
    #                 'pixel_coord': pixel_coord_i[j],  # [2] - åƒç´ åæ ‡
    #                 'point_coord': pc_feat_i[j]  # [output_dim] - è¿™ä¸ªç‚¹çš„ç‰¹å¾ï¼ˆä¸featureç›¸åŒï¼‰
    #             })
            
    #         mapping.append(feature_groups)
        
    #     return mapping


class DualModalEncoder(nn.Module):
    """åŒæ¨¡æ€ç¼–ç å™¨ - èåˆRGBå›¾åƒå’Œç‚¹äº‘æ•°æ®"""
    def __init__(self, 
                 image_encoder_config=None,
                 point_cloud_encoder_config=None,
                 fusion_dim=256):
        super().__init__()
        self.fusion_dim = fusion_dim
        
        # RGBå›¾åƒç¼–ç å™¨
        # å¦‚æœä¼ å…¥äº†å®Œæ•´çš„image_encoder_configï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™ä½¿ç”¨åŸºæœ¬å‚æ•°
        if image_encoder_config is not None:
            # ä½¿ç”¨ä¼ å…¥çš„å®Œæ•´é…ç½®ï¼Œä½†ç¡®ä¿input_channelså’Œoutput_dimæ­£ç¡®
            final_image_encoder_config = image_encoder_config.copy()
            final_image_encoder_config['input_channels'] = 3
            final_image_encoder_config['output_dim'] = fusion_dim // 2
        else:
            # å‘åå…¼å®¹ï¼šä½¿ç”¨åŸºæœ¬å‚æ•°æ„å»ºé…ç½®
            final_image_encoder_config = {
                'input_channels': 3,
                'output_dim': fusion_dim // 2
            }
        
        self.image_encoder = ImageEncoder(**final_image_encoder_config)
        
        # ç‚¹äº‘ç¼–ç å™¨
        self.point_cloud_encoder = PointCloudEncoder(
            output_dim=fusion_dim // 2,  # å„å ä¸€åŠç»´åº¦
            point_cloud_encoder_config=point_cloud_encoder_config
        )
        
        # èåˆå±‚
        self.fusion_layer = nn.Linear(fusion_dim, fusion_dim)
        
        print(f"DualModalEncoder: RGB({fusion_dim//2}) + PointCloud({fusion_dim//2}) -> {fusion_dim}")
        
    def forward(self, rgb_images, point_cloud_data):
        """
        å¤„ç†RGBå›¾åƒå’Œç‚¹äº‘æ•°æ®
        
        Args:
            rgb_images: Tensor - RGBå›¾åƒ [B, 3, H, W]
            point_cloud_data: Dict - ç‚¹äº‘æ•°æ®å­—å…¸ï¼ŒåŒ…å«coord, grid_coord, offset, featç­‰å­—æ®µ
            
        Returns:
            fused_features: List[Tensor] - èåˆåçš„ç‰¹å¾åˆ—è¡¨
            original_indices: List[Tensor] - åŸå§‹ç´¢å¼•åˆ—è¡¨
            pixel_coords_out: List[Tensor] - åƒç´ åæ ‡åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯[N_i, 2]
        """
        # å¤„ç†RGBå›¾åƒï¼ˆè·å–2Dç‰¹å¾ä»¥ä¾¿åƒç´ ç´¢å¼•æ˜ å°„ï¼‰
        rgb_2d = self.image_encoder(rgb_images, return_2d_features=True)  # [B, C, Hf, Wf]
        B, C, Hf, Wf = rgb_2d.shape
        # åŒæ—¶ä¿ç•™å±•å¹³åçš„åºåˆ—è¡¨ç¤ºç”¨äºç´¢å¼•æ”¶é›†
        rgb_seq = rgb_2d.permute(0, 2, 3, 1).contiguous().view(B, Hf * Wf, C)  # [B, Hf*Wf, C]

        # å¤„ç†ç‚¹äº‘æ•°æ®
        pc_features, pixel_coords_out = self.point_cloud_encoder(point_cloud_data)  # List[Tensor]

        # èåˆç‰¹å¾ - åŸºäºpc featureå¯¹åº”çš„åƒç´ é›†åˆï¼ŒèšåˆRGBåä¸PCæ‹¼æ¥
        fused_pc_features = []  # List[Tensor] with shape [N_feat_i, fusion_dim]
        H_img, W_img = rgb_images.shape[-2], rgb_images.shape[-1]

        for i, pc_feat in enumerate(pc_features):
            # pc_feat: [N_feat_i, output_dim] - ç¬¬iä¸ªæ ·æœ¬çš„PC featuresï¼ˆPointTransformerV3çš„è¾“å‡ºï¼‰
            # original_indices[i]: [N_feat_i] - æ¯ä¸ªPC featureå¯¹åº”çš„åŸå§‹ç‚¹ç´¢å¼•
            
            # å½“å‰æ ·æœ¬çš„RGBç‰¹å¾
            rgb_feat_seq = rgb_seq[i]  # [Hf*Wf, C]

            # å½“å‰æ ·æœ¬çš„åƒç´ åæ ‡ï¼ˆæ¯ä¸ªfeatureå¯¹åº”çš„åƒç´ åæ ‡åˆ—è¡¨ï¼‰
            feature_pixels_list = pixel_coords_out[i]  # List[Tensor] - æ¯ä¸ªfeatureå¯¹åº”çš„åƒç´ åæ ‡
            
            # å¯¹æ¯ä¸ªPC featureï¼Œèšåˆå…¶å¯¹åº”çš„æ‰€æœ‰åƒç´ çš„RGBç‰¹å¾
            fused_feat_list = []
            
            for feat_idx, feature_pixels in enumerate(feature_pixels_list):
                # è·å–å½“å‰PC featureçš„ç‰¹å¾å‘é‡
                current_pc_feat = pc_feat[feat_idx].unsqueeze(0)  # [1, output_dim]
                # feature_pixels: [m, 2] - è¯¥featureå¯¹åº”çš„mä¸ªåƒç´ åæ ‡
                
                # è®¡ç®—åƒç´ åœ¨ç‰¹å¾å›¾ä¸Šçš„ç´¢å¼•
                x_pix = feature_pixels[:, 0].to(pc_feat.device).float()  # [m]
                y_pix = feature_pixels[:, 1].to(pc_feat.device).float()  # [m]
                
                # å°†åŸå›¾åƒç´ åæ ‡æ˜ å°„åˆ°ç‰¹å¾å›¾åæ ‡
                x_feat = (x_pix * (Wf / float(W_img))).floor().clamp(0, Wf - 1).long()
                y_feat = (y_pix * (Hf / float(H_img))).floor().clamp(0, Hf - 1).long()
                feat_idx_linear = y_feat * Wf + x_feat  # [m] - åœ¨ç‰¹å¾å›¾ä¸­çš„çº¿æ€§ç´¢å¼•
                # è·å–è¯¥featureå¯¹åº”çš„æ‰€æœ‰åƒç´ çš„RGBç‰¹å¾
                feature_rgb_feat = rgb_feat_seq[feat_idx_linear]  # [m, C_rgb]
                # å¯¹å±äºè¯¥featureçš„æ‰€æœ‰åƒç´ çš„RGBç‰¹å¾å–å¹³å‡
                rgb_feat = feature_rgb_feat.mean(dim=0, keepdim=True)  # [1, C_rgb]
                # æ‹¼æ¥å½“å‰PC featureä¸å¯¹åº”çš„RGBç‰¹å¾
                combined = torch.cat([current_pc_feat, rgb_feat], dim=-1)  # [1, output_dim + C_rgb]
                fused_feat = self.fusion_layer(combined)  # [1, fusion_dim]
                fused_feat_list.append(fused_feat)
            
            fused_feat = torch.cat(fused_feat_list, dim=0)  # [N_feat_i, fusion_dim]
            fused_pc_features.append(fused_feat)
            # print(len(fused_pc_features))

        return fused_pc_features
    
    # def get_feature_pixel_mapping(self, fused_features, original_indices, pixel_coords_out):
    #     """
    #     å»ºç«‹èåˆç‰¹å¾å’ŒåŸå›¾åƒç´ çš„å¯¹åº”å…³ç³»
        
    #     Args:
    #         fused_features: List[Tensor] - èåˆç‰¹å¾åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯[H*W, fusion_dim]
    #         original_indices: List[Tensor] - åŸå§‹ç´¢å¼•åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯[N_i]ï¼Œè¡¨ç¤ºæ¯ä¸ªç‚¹å¯¹åº”å“ªä¸ªfeature
    #         pixel_coords_out: List[Tensor] - åƒç´ åæ ‡åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯[N_i, 2]
            
    #     Returns:
    #         mapping: List[Dict] - æ¯ä¸ªæ ·æœ¬çš„å¯¹åº”å…³ç³»å­—å…¸åˆ—è¡¨
    #                 æ¯ä¸ªå­—å…¸çš„keyæ˜¯featureç´¢å¼•ï¼Œvalueæ˜¯åŒ…å«è¯¥featureçš„æ‰€æœ‰ç‚¹çš„ä¿¡æ¯
    #                 æ ¼å¼: {feature_idx: {'fused_feature': Tensor, 'points': List[Dict]}}
    #     """
    #     mapping = []
        
    #     for i in range(len(fused_features)):
    #         fused_feat_i = fused_features[i]       # [H*W, fusion_dim]
    #         orig_idx_i = original_indices[i]       # [N_i]
    #         pixel_coord_i = pixel_coords_out[i]    # [N_i, 2]
            
    #         # æŒ‰featureç´¢å¼•åˆ†ç»„
    #         feature_groups = {}
            
    #         for j in range(orig_idx_i.shape[0]):
    #             feature_idx = orig_idx_i[j].item()
                
    #             if feature_idx not in feature_groups:
    #                 feature_groups[feature_idx] = {
    #                     'fused_feature': fused_feat_i[feature_idx],  # è¿™ä¸ªfeatureçš„èåˆç‰¹å¾
    #                     'points': []  # å±äºè¿™ä¸ªfeatureçš„æ‰€æœ‰ç‚¹
    #                 }
                
    #             # æ·»åŠ è¿™ä¸ªç‚¹çš„ä¿¡æ¯
    #             feature_groups[feature_idx]['points'].append({
    #                 'point_idx': j,  # åœ¨åŸå§‹ç‚¹äº‘ä¸­çš„ç´¢å¼•
    #                 'pixel_coord': pixel_coord_i[j],  # [2] - åƒç´ åæ ‡
    #             })
            
    #         mapping.append(feature_groups)
        
    #     return mapping

@dataclass
class IncrementalState:
    """å¢é‡ç”ŸæˆçŠ¶æ€ - å‚è€ƒPrimitiveAnythingå®ç°"""
    current_sequence: torch.Tensor  # [B, current_len, embed_dim]
    fused_embed: torch.Tensor      # [B, max_feat_len, fusion_dim] - èåˆåçš„ç‰¹å¾
    fused_cond: torch.Tensor       # [B, fusion_cond_dim] - èåˆåçš„æ¡ä»¶
    stopped_samples: torch.Tensor  # [B] å¸ƒå°”å€¼ï¼Œæ ‡è®°å“ªäº›æ ·æœ¬å·²åœæ­¢
    current_step: int              # å½“å‰æ­¥æ•°
    mask: torch.Tensor            # [B, max_feat_len] - æ ‡è®°æœ‰æ•ˆç‰¹å¾ä½ç½®çš„mask
    
    # å¤šçº§KVç¼“å­˜ç”¨äºçœŸæ­£çš„å¢é‡è§£ç ï¼ˆå‚è€ƒPrimitiveAnythingï¼‰
    decoder_cache: Optional[object] = None  # Transformer decoderçš„cache
    gateloop_cache: Optional[List] = None   # é—¨æ§å¾ªç¯å—çš„cache
    
    # ç”Ÿæˆç»“æœè·Ÿè¸ª
    generated_boxes: Optional[Dict[str, List]] = None
    
    def __post_init__(self):
        if self.gateloop_cache is None:
            self.gateloop_cache = []
        if self.generated_boxes is None:
            # è¿™é‡Œä¼šåœ¨initialize_incremental_generationä¸­æ­£ç¡®è®¾ç½®
            pass

class PrimitiveTransformer3D(nn.Module):
    """3DåŸºæœ¬ä½“å˜æ¢å™¨ - æ”¯æŒRGBXYZè¾“å…¥å’Œ3Dç®±å­ç”Ÿæˆ"""
    def __init__(
        self,
        *,
        # ç¦»æ•£åŒ–å‚æ•° - 3Dåæ ‡
        num_discrete_x = 128,
        num_discrete_y = 128,
        num_discrete_z = 128,  # æ–°å¢zåæ ‡
        num_discrete_w = 64,
        num_discrete_h = 64,
        num_discrete_l = 64,  # æ–°å¢lengthç»´åº¦
        
        # è¿ç»­èŒƒå›´ - 3Dåæ ‡
        continuous_range_x = [0.5, 2.5],
        continuous_range_y = [-2, 2],
        continuous_range_z = [-1.5, 1.5],  # æ–°å¢zèŒƒå›´
        continuous_range_w = [0.3, 0.7],
        continuous_range_h = [0.3, 0.7],
        continuous_range_l = [0.3, 0.7],  # æ–°å¢lengthèŒƒå›´
        
        # åµŒå…¥ç»´åº¦ - 3D
        dim_x_embed = 64,
        dim_y_embed = 64,
        dim_z_embed = 64,  # æ–°å¢zåµŒå…¥
        dim_w_embed = 32,
        dim_h_embed = 32,
        dim_l_embed = 32,  # æ–°å¢lengthåµŒå…¥
        
        # æ¨¡å‹å‚æ•°
        dim = 512,
        max_primitive_len = 10,
        attn_depth = 6,
        attn_dim_head = 64,
        attn_heads = 8,
        attn_dropout = 0.0,  # æ³¨æ„åŠ›dropout
        ff_dropout = 0.0,    # å‰é¦ˆdropout
        
        # åŒæ¨¡æ€ç¼–ç å™¨é…ç½®
        image_encoder_dim = 512,
        point_cloud_encoder_dim = 256,
        fusion_dim = 512,
        use_fpn = True,
        backbone = "resnet50",
        pretrained = True,
        image_encoder_config = None,
        point_cloud_encoder_config = None,
        
        # å…¶ä»–å‚æ•°
        shape_cond_with_cat = False,
        condition_on_image = True,
        gateloop_depth = 2,
        gateloop_use_heinsen = False,
        
        pad_id = -1,
    ):
        super().__init__()
        
        # 3Dç¦»æ•£åŒ–å‚æ•°
        self.num_discrete_x = num_discrete_x
        self.num_discrete_y = num_discrete_y
        self.num_discrete_z = num_discrete_z  # æ–°å¢
        self.num_discrete_w = num_discrete_w
        self.num_discrete_h = num_discrete_h
        self.num_discrete_l = num_discrete_l  # æ–°å¢
        
        # 3Dè¿ç»­èŒƒå›´
        self.continuous_range_x = continuous_range_x
        self.continuous_range_y = continuous_range_y
        self.continuous_range_z = continuous_range_z  # æ–°å¢
        self.continuous_range_w = continuous_range_w
        self.continuous_range_h = continuous_range_h
        self.continuous_range_l = continuous_range_l  # æ–°å¢
        
        # å…¶ä»–å‚æ•°
        self.shape_cond_with_cat = shape_cond_with_cat
        self.condition_on_image = condition_on_image
        self.gateloop_depth = gateloop_depth
        self.gateloop_use_heinsen = gateloop_use_heinsen
        
        # èåˆç‰¹å¾æ¡ä»¶æŠ•å½±å±‚
        if shape_cond_with_cat:
            self.image_cond_proj = nn.Linear(fusion_dim, dim)
        else:
            self.image_cond_proj = None
        
        # èåˆç‰¹å¾æ¡ä»¶åŒ–å±‚
        if condition_on_image:
            self.fused_film_cond = FiLM(dim, dim)
            self.fused_cond_proj_film = nn.Linear(fusion_dim, self.fused_film_cond.to_gamma.in_features)
        else:
            self.fused_film_cond = None
            self.fused_cond_proj_film = None
        
        # é—¨æ§å¾ªç¯å—
        if gateloop_depth > 0:
            self.gateloop_block = GateLoopBlock(dim, depth=gateloop_depth, use_heinsen=gateloop_use_heinsen)
        else:
            self.gateloop_block = None
        
        # åŒæ¨¡æ€ç¼–ç å™¨ - åˆ†åˆ«å¤„ç†RGBå’Œç‚¹äº‘æ•°æ®
        # å¦‚æœä¼ å…¥äº†å®Œæ•´çš„image_encoder_configï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™ä½¿ç”¨åŸºæœ¬å‚æ•°
        if image_encoder_config is not None:
            # ä½¿ç”¨ä¼ å…¥çš„å®Œæ•´é…ç½®ï¼Œä½†ç¡®ä¿output_dimæ­£ç¡®
            final_image_encoder_config = image_encoder_config.copy()
            final_image_encoder_config['output_dim'] = fusion_dim // 2
        else:
            # å‘åå…¼å®¹ï¼šä½¿ç”¨åŸºæœ¬å‚æ•°æ„å»ºé…ç½®
            final_image_encoder_config = {
                'use_fpn': use_fpn,
                'backbone': backbone,
                'pretrained': pretrained,
                'output_dim': fusion_dim // 2
            }
        
        self.dual_modal_encoder = DualModalEncoder(
            image_encoder_config=final_image_encoder_config,
            point_cloud_encoder_config=point_cloud_encoder_config,
            fusion_dim=fusion_dim
        )
        
        # 3DåµŒå…¥å±‚
        self.x_embed = nn.Embedding(num_discrete_x, dim_x_embed)
        self.y_embed = nn.Embedding(num_discrete_y, dim_y_embed)
        self.z_embed = nn.Embedding(num_discrete_z, dim_z_embed)  # æ–°å¢
        self.w_embed = nn.Embedding(num_discrete_w, dim_w_embed)
        self.h_embed = nn.Embedding(num_discrete_h, dim_h_embed)
        self.l_embed = nn.Embedding(num_discrete_l, dim_l_embed)  # æ–°å¢
        
        # æŠ•å½±å±‚ - æ›´æ–°æ€»ç»´åº¦
        total_embed_dim = dim_x_embed + dim_y_embed + dim_z_embed + dim_w_embed + dim_h_embed + dim_l_embed
        self.project_in = nn.Linear(total_embed_dim, dim)
        
        # è¿ç»­å€¼åˆ°embeddingçš„è½¬æ¢å±‚ï¼ˆç”¨äºå±æ€§é—´ä¾èµ–ï¼‰
        self.continuous_to_x_embed = nn.Linear(1, dim_x_embed)
        self.continuous_to_y_embed = nn.Linear(1, dim_y_embed)
        self.continuous_to_z_embed = nn.Linear(1, dim_z_embed)
        self.continuous_to_w_embed = nn.Linear(1, dim_w_embed)
        self.continuous_to_h_embed = nn.Linear(1, dim_h_embed)
        self.continuous_to_l_embed = nn.Linear(1, dim_l_embed)
        
        # è§£ç å™¨
        self.decoder = Decoder(
            dim=dim,
            depth=attn_depth,
            heads=attn_heads,
            attn_dim_head=attn_dim_head,
            attn_flash=True,
            attn_dropout=attn_dropout,  # ä½¿ç”¨æ³¨æ„åŠ›dropout
            ff_dropout=ff_dropout,      # ä½¿ç”¨å‰é¦ˆdropout
            cross_attend=True,
            cross_attn_dim_context=fusion_dim,
        )
        
        # 3Dé¢„æµ‹å¤´ - æ·»åŠ zåæ ‡å’Œlength
        self.to_x_logits = nn.Sequential(
            nn.Linear(dim, dim),
            nn.ReLU(),
            nn.Linear(dim, num_discrete_x),
        )

        self.to_y_logits = nn.Sequential(
            nn.Linear(dim + dim_x_embed, dim),
            nn.ReLU(),
            nn.Linear(dim, num_discrete_y),
        )
        
        # æ–°å¢zåæ ‡é¢„æµ‹å¤´
        self.to_z_logits = nn.Sequential(
            nn.Linear(dim + dim_x_embed + dim_y_embed, dim),
            nn.ReLU(),
            nn.Linear(dim, num_discrete_z),
        )

        self.to_w_logits = nn.Sequential(
            nn.Linear(dim + dim_x_embed + dim_y_embed + dim_z_embed, dim),
            nn.ReLU(),
            nn.Linear(dim, num_discrete_w),
        )

        self.to_h_logits = nn.Sequential(
            nn.Linear(dim + dim_x_embed + dim_y_embed + dim_z_embed + dim_w_embed, dim),
            nn.ReLU(),
            nn.Linear(dim, num_discrete_h),
        )
        
        # æ–°å¢lengthé¢„æµ‹å¤´
        self.to_l_logits = nn.Sequential(
            nn.Linear(dim + dim_x_embed + dim_y_embed + dim_z_embed + dim_w_embed + dim_h_embed, dim),
            nn.ReLU(),
            nn.Linear(dim, num_discrete_l),
        )
        
        # 3D Deltaé¢„æµ‹å¤´
        self.to_x_delta = nn.Sequential(
            nn.Linear(dim, dim),
            nn.ReLU(),
            nn.Linear(dim, 1),
        )
        
        self.to_y_delta = nn.Sequential(
            nn.Linear(dim + dim_x_embed, dim),
            nn.ReLU(),
            nn.Linear(dim, 1),
        )
        
        # æ–°å¢z deltaé¢„æµ‹å¤´
        self.to_z_delta = nn.Sequential(
            nn.Linear(dim + dim_x_embed + dim_y_embed, dim),
            nn.ReLU(),
            nn.Linear(dim, 1),
        )
        
        self.to_w_delta = nn.Sequential(
            nn.Linear(dim + dim_x_embed + dim_y_embed + dim_z_embed, dim),
            nn.ReLU(),
            nn.Linear(dim, 1),
        )
        
        self.to_h_delta = nn.Sequential(
            nn.Linear(dim + dim_x_embed + dim_y_embed + dim_z_embed + dim_w_embed, dim),
            nn.ReLU(),
            nn.Linear(dim, 1),
        )
        
        # æ–°å¢length deltaé¢„æµ‹å¤´
        self.to_l_delta = nn.Sequential(
            nn.Linear(dim + dim_x_embed + dim_y_embed + dim_z_embed + dim_w_embed + dim_h_embed, dim),
            nn.ReLU(),
            nn.Linear(dim, 1),
        )
        
        # EOSé¢„æµ‹ç½‘ç»œ
        self.to_eos_logits = nn.Sequential(
            nn.Linear(dim, dim),
            nn.ReLU(),
            nn.Linear(dim, 1),
        )
        
        # ç‰¹æ®Štoken
        self.sos_token = nn.Parameter(torch.randn(1, dim))
        self.pad_id = pad_id
        self.max_seq_len = max_primitive_len
        
        # ä½ç½®ç¼–ç 
        self.pos_embed = nn.Parameter(torch.randn(1, max_primitive_len + 1, dim))
        
        print(f"3D PrimitiveTransformer: RGBXYZ(6ch) Input + 3D Box Generation (x,y,z,w,h,l)")
    
    def continuous_from_discrete(self, discrete_values, num_bins, value_range):
        """å°†ç¦»æ•£å€¼è½¬æ¢ä¸ºè¿ç»­å€¼"""
        min_val, max_val = value_range
        return min_val + (discrete_values.float() / (num_bins - 1)) * (max_val - min_val)
    
    def get_continuous_embed(self, attr_name, continuous_value):
        """ä»è¿ç»­å€¼è·å–embedding"""
        # å°†è¿ç»­å€¼reshapeä¸º[B, 1]æˆ–[B, seq_len, 1]
        if continuous_value.dim() == 1:
            continuous_value = continuous_value.unsqueeze(-1)
        elif continuous_value.dim() == 2:
            continuous_value = continuous_value.unsqueeze(-1)
        
        if attr_name == 'x':
            return self.continuous_to_x_embed(continuous_value)
        elif attr_name == 'y':
            return self.continuous_to_y_embed(continuous_value)
        elif attr_name == 'z':
            return self.continuous_to_z_embed(continuous_value)
        elif attr_name == 'w':
            return self.continuous_to_w_embed(continuous_value)
        elif attr_name == 'h':
            return self.continuous_to_h_embed(continuous_value)
        elif attr_name == 'l':
            return self.continuous_to_l_embed(continuous_value)
        else:
            raise ValueError(f"Unknown attribute: {attr_name}")
    
    def predict_attribute_with_continuous_embed(self, step_embed, attr_name, prev_embeds=None, use_gumbel=None, temperature=1.0):
        """é¢„æµ‹å±æ€§å¹¶è¿”å›è¿ç»­å€¼å’Œembedding - æ”¯æŒå¯å¾®åˆ†é‡‡æ ·"""
        # æ„å»ºè¾“å…¥
        if prev_embeds is None:
            input_embed = step_embed
        else:
            input_embed = torch.cat([step_embed] + prev_embeds, dim=-1)
        
        # è·å–é¢„æµ‹å¤´å’Œå‚æ•°
        if attr_name == 'x':
            logits_head = self.to_x_logits
            delta_head = self.to_x_delta
            num_bins = self.num_discrete_x
            value_range = self.continuous_range_x
        elif attr_name == 'y':
            logits_head = self.to_y_logits
            delta_head = self.to_y_delta
            num_bins = self.num_discrete_y
            value_range = self.continuous_range_y
        elif attr_name == 'z':
            logits_head = self.to_z_logits
            delta_head = self.to_z_delta
            num_bins = self.num_discrete_z
            value_range = self.continuous_range_z
        elif attr_name == 'w':
            logits_head = self.to_w_logits
            delta_head = self.to_w_delta
            num_bins = self.num_discrete_w
            value_range = self.continuous_range_w
        elif attr_name == 'h':
            logits_head = self.to_h_logits
            delta_head = self.to_h_delta
            num_bins = self.num_discrete_h
            value_range = self.continuous_range_h
        elif attr_name == 'l':
            logits_head = self.to_l_logits
            delta_head = self.to_l_delta
            num_bins = self.num_discrete_l
            value_range = self.continuous_range_l
        else:
            raise ValueError(f"Unknown attribute: {attr_name}")
        
        # é¢„æµ‹
        logits = logits_head(input_embed)
        delta = torch.tanh(delta_head(input_embed).squeeze(-1)) * 0.5
        
        # å†³å®šä½¿ç”¨å“ªç§é‡‡æ ·æ–¹å¼
        if use_gumbel is None:
            use_gumbel = self.training  # è®­ç»ƒæ—¶ä½¿ç”¨Gumbel Softmaxï¼Œæ¨ç†æ—¶ä½¿ç”¨argmax
        
        if use_gumbel:
            # ä½¿ç”¨Gumbel Softmaxè¿›è¡Œå¯å¾®åˆ†é‡‡æ ·
            continuous_base = self._differentiable_discrete_to_continuous(
                logits, num_bins, value_range, temperature
            )
            # ç”¨äºè¿”å›çš„ç¦»æ•£é¢„æµ‹ï¼ˆä¸å‚ä¸æ¢¯åº¦ä¼ æ’­ï¼‰
            discrete_pred = torch.argmax(logits, dim=-1)
        else:
            # æ¨ç†æ—¶ä½¿ç”¨argmaxï¼ˆä¸éœ€è¦æ¢¯åº¦ï¼‰
            discrete_pred = torch.argmax(logits, dim=-1)
            continuous_base = self.continuous_from_discrete(discrete_pred, num_bins, value_range)
        
        # åŠ ä¸Šdeltaä¿®æ­£ - ğŸ”§ ä¿®å¤ï¼šdeltaåº”è¯¥æŒ‰bin_widthç¼©æ”¾
        if use_gumbel:
            # Gumbel Softmaxæƒ…å†µä¸‹ï¼Œéœ€è¦è®¡ç®—ç­‰æ•ˆçš„bin_width
            min_val, max_val = value_range
            bin_width = (max_val - min_val) / (num_bins - 1)
            continuous_value = continuous_base + delta * bin_width
        else:
            # argmaxæƒ…å†µä¸‹ï¼ŒåŒæ ·ä½¿ç”¨bin_widthç¼©æ”¾
            min_val, max_val = value_range
            bin_width = (max_val - min_val) / (num_bins - 1)
            continuous_value = continuous_base + delta * bin_width
        
        # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´æ€§ï¼ˆé’ˆå¯¹æ··åˆç²¾åº¦è®­ç»ƒï¼‰
        if continuous_value.dtype != delta.dtype:
            continuous_value = continuous_value.to(dtype=delta.dtype)
        
        # è·å–embedding
        embed = self.get_continuous_embed(attr_name, continuous_value)
        
        return logits, delta, continuous_value, embed
    
    def _differentiable_discrete_to_continuous(self, logits, num_bins, value_range, temperature=1.0):
        """ä½¿ç”¨Gumbel Softmaxè¿›è¡Œå¯å¾®åˆ†çš„ç¦»æ•£åˆ°è¿ç»­è½¬æ¢ - å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬"""
        # è·å–è¾“å…¥çš„æ•°æ®ç±»å‹å’Œè®¾å¤‡
        input_dtype = logits.dtype
        input_device = logits.device
        
        # åˆ›å»ºè¿ç»­å€¼çš„ç¦»æ•£ç½‘æ ¼ï¼ˆå¤ç”¨ç¼“å­˜ï¼‰
        min_val, max_val = value_range
        cache_key = (num_bins, min_val, max_val, input_device, input_dtype)
        
        # ç®€å•çš„ç¼“å­˜æœºåˆ¶é¿å…é‡å¤åˆ›å»º
        if not hasattr(self, '_discrete_values_cache'):
            self._discrete_values_cache = {}
        
        if cache_key not in self._discrete_values_cache:
            self._discrete_values_cache[cache_key] = torch.linspace(
                min_val, max_val, num_bins, device=input_device, dtype=input_dtype
            )
        discrete_values = self._discrete_values_cache[cache_key]
        
        # ä½¿ç”¨æ›´å†…å­˜å‹å¥½çš„è®¡ç®—æ–¹å¼
        # é¿å…åˆ›å»ºå¤§çš„ä¸­é—´tensor
        if temperature <= 0.1:
            # ä½æ¸©åº¦æ—¶ç›´æ¥ç”¨argmaxè¿‘ä¼¼ï¼ˆèŠ‚çœå†…å­˜ï¼‰
            indices = torch.argmax(logits, dim=-1)
            continuous_value = discrete_values[indices]
        else:
            # æ­£å¸¸Gumbel Softmax - ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
            # åœ¨æ··åˆç²¾åº¦ä¸‹ï¼Œlogitså¯èƒ½æ˜¯Halfç±»å‹
            if input_dtype == torch.half:
                # å¯¹äºHalfç²¾åº¦ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
                with torch.cuda.amp.autocast(enabled=False):
                    # è½¬æ¢ä¸ºfloat32è¿›è¡Œè®¡ç®—
                    logits_fp32 = logits.float()
                    discrete_values_fp32 = discrete_values.float()
                    
                    gumbel_weights = F.gumbel_softmax(logits_fp32, tau=temperature, hard=False, dim=-1)
                    continuous_value = torch.mv(gumbel_weights, discrete_values_fp32)
                    
                    # è½¬æ¢å›åŸå§‹ç²¾åº¦
                    continuous_value = continuous_value.to(dtype=input_dtype)
            else:
                # æ­£å¸¸ç²¾åº¦è®¡ç®—
                gumbel_weights = F.gumbel_softmax(logits, tau=temperature, hard=False, dim=-1)
                continuous_value = torch.mv(gumbel_weights, discrete_values)
        
        return continuous_value
    
    def discretize(self, value, num_discrete, continuous_range):
        """å°†è¿ç»­å€¼ç¦»æ•£åŒ–"""
        min_val, max_val = continuous_range
        discrete = ((value - min_val) / (max_val - min_val) * (num_discrete - 1)).clamp(0, num_discrete - 1).long()
        return discrete
    
    def undiscretize(self, discrete, num_discrete, continuous_range):
        """å°†ç¦»æ•£å€¼è½¬æ¢å›è¿ç»­å€¼"""
        min_val, max_val = continuous_range
        continuous = discrete.float() / (num_discrete - 1) * (max_val - min_val) + min_val
        return continuous
    
    def masked_mean_pooling(self, features, mask):
        """
        ä½¿ç”¨maskè¿›è¡Œå¹³å‡æ± åŒ–ï¼Œå¿½ç•¥å¡«å……çš„ä½ç½®
        
        Args:
            features: [batch_size, seq_len, feature_dim]
            mask: [batch_size, seq_len] - Trueè¡¨ç¤ºæœ‰æ•ˆä½ç½®ï¼ŒFalseè¡¨ç¤ºå¡«å……ä½ç½®
            
        Returns:
            pooled_features: [batch_size, feature_dim]
        """
        # å°†maskæ‰©å±•åˆ°featureç»´åº¦
        mask_expanded = mask.unsqueeze(-1).expand_as(features)  # [batch_size, seq_len, feature_dim]
        
        # å°†å¡«å……ä½ç½®è®¾ä¸º0
        masked_features = features * mask_expanded.float()
        
        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æœ‰æ•ˆé•¿åº¦
        valid_lengths = mask.sum(dim=1, keepdim=True).float()  # [batch_size, 1]
        
        # é¿å…é™¤é›¶
        valid_lengths = torch.clamp(valid_lengths, min=1.0)
        
        # è®¡ç®—maskedå¹³å‡
        pooled_features = masked_features.sum(dim=1) / valid_lengths  # [batch_size, feature_dim]
        
        return pooled_features
    
    def encode_primitive(self, x, y, z, w, h, l, primitive_mask):
        """ç¼–ç 3DåŸºæœ¬ä½“å‚æ•°"""
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„æ¡†
        if x.numel() == 0 or y.numel() == 0 or z.numel() == 0 or w.numel() == 0 or h.numel() == 0 or l.numel() == 0:
            batch_size = x.shape[0] if x.numel() > 0 else 1
            dim = self.project_in.out_features
            empty_embed = torch.zeros(batch_size, 0, dim, device=x.device)
            empty_discrete = (torch.zeros(batch_size, 0, dtype=torch.long, device=x.device),
                            torch.zeros(batch_size, 0, dtype=torch.long, device=x.device),
                            torch.zeros(batch_size, 0, dtype=torch.long, device=x.device),
                            torch.zeros(batch_size, 0, dtype=torch.long, device=x.device),
                            torch.zeros(batch_size, 0, dtype=torch.long, device=x.device),
                            torch.zeros(batch_size, 0, dtype=torch.long, device=x.device))
            return empty_embed, empty_discrete
        
        # 3Dç¦»æ•£åŒ–
        discrete_x = self.discretize(x, self.num_discrete_x, self.continuous_range_x)
        discrete_y = self.discretize(y, self.num_discrete_y, self.continuous_range_y)
        discrete_z = self.discretize(z, self.num_discrete_z, self.continuous_range_z)  # æ–°å¢
        discrete_w = self.discretize(w, self.num_discrete_w, self.continuous_range_w)
        discrete_h = self.discretize(h, self.num_discrete_h, self.continuous_range_h)
        discrete_l = self.discretize(l, self.num_discrete_l, self.continuous_range_l)  # æ–°å¢
        
        # 3DåµŒå…¥
        x_embed = self.x_embed(discrete_x)
        y_embed = self.y_embed(discrete_y)
        z_embed = self.z_embed(discrete_z)  # æ–°å¢
        w_embed = self.w_embed(discrete_w)
        h_embed = self.h_embed(discrete_h)
        l_embed = self.l_embed(discrete_l)  # æ–°å¢
        
        # ç»„åˆ3Dç‰¹å¾
        primitive_embed, _ = pack([x_embed, y_embed, z_embed, w_embed, h_embed, l_embed], 'b np *')
        primitive_embed = self.project_in(primitive_embed)
        
        # ä½¿ç”¨primitive_maskå°†æ— æ•ˆä½ç½®çš„embeddingè®¾ç½®ä¸º0
        primitive_embed = primitive_embed.masked_fill(~primitive_mask.unsqueeze(-1), 0.)
        
        return primitive_embed, (discrete_x, discrete_y, discrete_z, discrete_w, discrete_h, discrete_l)
    
    def forward(
        self,
        *,
        x: Tensor,
        y: Tensor,
        z: Tensor,  # æ–°å¢zåæ ‡
        w: Tensor,
        h: Tensor,
        l: Tensor,  # æ–°å¢length
        rgb_image: Tensor,  # RGBå›¾åƒ [B, 3, H, W]
        coords: List[Tensor],  # åæ ‡åˆ—è¡¨
        grid_coords: List[Tensor],  # ç½‘æ ¼åæ ‡åˆ—è¡¨
        offsets: List[Tensor],  # åç§»é‡åˆ—è¡¨
        feats: List[Tensor],  # ç‰¹å¾åˆ—è¡¨
        pixel_coords: List[Tensor] = None,  # åƒç´ åæ ‡åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯[N_i, 2]
    ):
        """3Då‰å‘ä¼ æ’­"""
        # åˆ›å»º3D mask
        primitive_mask = (x != self.pad_id) & (y != self.pad_id) & (z != self.pad_id) & (w != self.pad_id) & (h != self.pad_id) & (l != self.pad_id)
        
        # ç¼–ç 3DåŸºæœ¬ä½“
        codes, discrete_coords = self.encode_primitive(x, y, z, w, h, l, primitive_mask)

        # æ„å»ºç‚¹äº‘æ•°æ®å­—å…¸
        point_cloud_data = {
            'coord': coords,
            'grid_coord': grid_coords,
            'offset': offsets,
            'feat': feats,
            'pixel_coords': pixel_coords,
        }
 
            
        
        # ä½¿ç”¨åŒæ¨¡æ€ç¼–ç å™¨å¤„ç†RGBå›¾åƒå’Œç‚¹äº‘æ•°æ®
        fused_embed = self.dual_modal_encoder(rgb_image, point_cloud_data)
        
        # å°†èåˆç‰¹å¾è½¬æ¢ä¸ºç»Ÿä¸€çš„æ ¼å¼
        # å¤„ç†å˜é•¿ç‚¹äº‘ç‰¹å¾ï¼šå¡«å……åˆ°æœ€å¤§é•¿åº¦å¹¶åˆ›å»ºmask
        if len(fused_embed) > 0:
            # æ‰¾åˆ°æœ€å¤§ç‰¹å¾æ•°é‡
            max_feat_len = max(feat.shape[0] for feat in fused_embed)
            fusion_dim = fused_embed[0].shape[1]
            batch_size = len(fused_embed)
            
            # å¡«å……åˆ°ç»Ÿä¸€é•¿åº¦å¹¶åˆ›å»ºmask
            padded_features = []
            mask = torch.zeros(batch_size, max_feat_len, dtype=torch.bool, device=fused_embed[0].device)
            
            for i, feat in enumerate(fused_embed):
                feat_len = feat.shape[0]
                if feat_len < max_feat_len:
                    # ç”¨é›¶å¡«å……
                    padding = torch.zeros(max_feat_len - feat_len, fusion_dim, 
                                        device=feat.device, dtype=feat.dtype)
                    padded_feat = torch.cat([feat, padding], dim=0)
                else:
                    padded_feat = feat
                padded_features.append(padded_feat)
                
                # è®¾ç½®maskï¼šæœ‰æ•ˆä½ç½®ä¸ºTrueï¼Œå¡«å……ä½ç½®ä¸ºFalse
                mask[i, :feat_len] = True
            
            # å †å æˆbatch
            fused_embed = torch.stack(padded_features, dim=0)  # [batch_size, max_feat_len, fusion_dim]
        else:
            # ç©ºbatchçš„æƒ…å†µ
            fused_embed = torch.empty(0, 0, 0)
            mask = torch.empty(0, 0, dtype=torch.bool)
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        batch_size, seq_len, _ = codes.shape
        device = codes.device
        
        
        # æ„å»ºè¾“å…¥åºåˆ—
        history = codes
        sos = repeat(self.sos_token, 'n d -> b n d', b=batch_size)
        
        primitive_codes, packed_sos_shape = pack([sos, history], 'b * d')
        seq_len = primitive_codes.shape[1]
        pos_embed = self.pos_embed[:, :seq_len, :]
        primitive_codes = primitive_codes + pos_embed
        
        # èåˆç‰¹å¾æ¡ä»¶åŒ–å¤„ç†ï¼ˆRGB+ç‚¹äº‘èåˆç‰¹å¾ï¼‰
        if self.condition_on_image and self.fused_film_cond is not None:
            # ä½¿ç”¨maskedå¹³å‡æ± åŒ–è€Œä¸æ˜¯ç®€å•å¹³å‡
            pooled_fused_embed = self.masked_mean_pooling(fused_embed, mask)  # [batch_size, fusion_dim]
            fused_cond = self.fused_cond_proj_film(pooled_fused_embed)  # æŠ•å½±åˆ°æ¡ä»¶ç»´åº¦
            primitive_codes = self.fused_film_cond(primitive_codes, fused_cond)  # FiLMè°ƒåˆ¶
        
        # é—¨æ§å¾ªç¯å—å¤„ç†
        if self.gateloop_block is not None:
            primitive_codes, gateloop_cache = self.gateloop_block(primitive_codes)
        
        # å˜æ¢å™¨è§£ç  - ç¦ç”¨gradient checkpointingä»¥é¿å…ä¸Scheduled Samplingå†²çª
        attended_codes = self.decoder(
            primitive_codes,
            context=fused_embed,
            context_mask=mask,  # æ·»åŠ maskæ¥å¿½ç•¥å¡«å……çš„ä½ç½®
        )

        return attended_codes
    
    def forward_with_predictions(
        self,
        *,
        x: Tensor,
        y: Tensor,
        z: Tensor,
        w: Tensor,
        h: Tensor,
        l: Tensor,
        rgb_image: Tensor,
        coords: List[Tensor],
        grid_coords: List[Tensor],
        offsets: List[Tensor],
        feats: List[Tensor],
        pixel_coords: List[Tensor] = None
    ):
        """å¸¦é¢„æµ‹è¾“å‡ºçš„å‰å‘ä¼ æ’­ï¼Œç”¨äºè®­ç»ƒ"""
        # å…ˆè°ƒç”¨æ ‡å‡†å‰å‘ä¼ æ’­è·å–attended_codes
        attended_codes = self.forward(
            x=x, y=y, z=z, w=w, h=h, l=l, rgb_image=rgb_image, coords=coords, grid_coords=grid_coords, offsets=offsets, feats=feats, pixel_coords=pixel_coords
        )
        
        # attended_codes shape: [batch_size, seq_len, model_dim]
        batch_size, seq_len, _ = attended_codes.shape
        
        # ä¸ºæ¯ä¸ªåºåˆ—ä½ç½®è®¡ç®—é¢„æµ‹
        all_logits = {f'{attr}_logits': [] for attr in ['x', 'y', 'z', 'w', 'h', 'l']}
        all_deltas = {f'{attr}_delta': [] for attr in ['x', 'y', 'z', 'w', 'h', 'l']}
        all_continuous = {f'{attr}_continuous': [] for attr in ['x', 'y', 'z', 'w', 'h', 'l']}
        eos_logits_list = []
        
        for t in range(seq_len):
            step_embed = attended_codes[:, t, :]  # [batch_size, model_dim]
            
            # ç´¯ç§¯çš„embedç”¨äºåç»­å±æ€§é¢„æµ‹
            x_embed = y_embed = z_embed = w_embed = h_embed = None
            
            # é¢„æµ‹xåæ ‡ - ä½¿ç”¨è¿ç»­å€¼embedding
            x_logits, x_delta, x_continuous, x_embed = self.predict_attribute_with_continuous_embed(step_embed, 'x', prev_embeds=None, use_gumbel=None, temperature=1.0)
            
            # é¢„æµ‹yåæ ‡ - ä½¿ç”¨è¿ç»­å€¼embedding
            y_logits, y_delta, y_continuous, y_embed = self.predict_attribute_with_continuous_embed(step_embed, 'y', prev_embeds=[x_embed], use_gumbel=None, temperature=1.0)
            
            # é¢„æµ‹zåæ ‡ - ä½¿ç”¨è¿ç»­å€¼embedding
            z_logits, z_delta, z_continuous, z_embed = self.predict_attribute_with_continuous_embed(step_embed, 'z', prev_embeds=[x_embed, y_embed], use_gumbel=None, temperature=1.0)
            
            # é¢„æµ‹w - ä½¿ç”¨è¿ç»­å€¼embedding
            w_logits, w_delta, w_continuous, w_embed = self.predict_attribute_with_continuous_embed(step_embed, 'w', prev_embeds=[x_embed, y_embed, z_embed], use_gumbel=None, temperature=1.0)
            
            # é¢„æµ‹h - ä½¿ç”¨è¿ç»­å€¼embedding
            h_logits, h_delta, h_continuous, h_embed = self.predict_attribute_with_continuous_embed(step_embed, 'h', prev_embeds=[x_embed, y_embed, z_embed, w_embed], use_gumbel=None, temperature=1.0)
            
            # é¢„æµ‹l - ä½¿ç”¨è¿ç»­å€¼embedding
            l_logits, l_delta, l_continuous, l_embed = self.predict_attribute_with_continuous_embed(step_embed, 'l', prev_embeds=[x_embed, y_embed, z_embed, w_embed, h_embed], use_gumbel=None, temperature=1.0)
            
            # é¢„æµ‹EOS
            eos_logit = self.to_eos_logits(step_embed).squeeze(-1)
            
            # æ”¶é›†ç»“æœ
            all_logits['x_logits'].append(x_logits)
            all_logits['y_logits'].append(y_logits)
            all_logits['z_logits'].append(z_logits)
            all_logits['w_logits'].append(w_logits)
            all_logits['h_logits'].append(h_logits)
            all_logits['l_logits'].append(l_logits)
            
            all_deltas['x_delta'].append(x_delta)
            all_deltas['y_delta'].append(y_delta)
            all_deltas['z_delta'].append(z_delta)
            all_deltas['w_delta'].append(w_delta)
            all_deltas['h_delta'].append(h_delta)
            all_deltas['l_delta'].append(l_delta)
            
            all_continuous['x_continuous'].append(x_continuous)
            all_continuous['y_continuous'].append(y_continuous)
            all_continuous['z_continuous'].append(z_continuous)
            all_continuous['w_continuous'].append(w_continuous)
            all_continuous['h_continuous'].append(h_continuous)
            all_continuous['l_continuous'].append(l_continuous)
            
            eos_logits_list.append(eos_logit)
        
        # ç»„è£…æœ€ç»ˆè¾“å‡º
        logits_dict = {}
        delta_dict = {}
        continuous_dict = {}
        
        for attr in ['x', 'y', 'z', 'w', 'h', 'l']:
            logits_dict[f'{attr}_logits'] = torch.stack(all_logits[f'{attr}_logits'], dim=1)
            delta_dict[f'{attr}_delta'] = torch.stack(all_deltas[f'{attr}_delta'], dim=1)
            continuous_dict[f'{attr}_continuous'] = torch.stack(all_continuous[f'{attr}_continuous'], dim=1)
        
        eos_logits = torch.stack(eos_logits_list, dim=1)
        
        return {
            'logits_dict': logits_dict,
            'delta_dict': delta_dict,
            'continuous_dict': continuous_dict,
            'eos_logits': eos_logits
        }
    
    @eval_decorator
    @torch.no_grad()
    def generate(
        self,
        rgb_image: Tensor,  # RGBå›¾åƒ [B, 3, H, W]
        coords: List[Tensor],  # åæ ‡åˆ—è¡¨
        grid_coords: List[Tensor],  # ç½‘æ ¼åæ ‡åˆ—è¡¨
        offsets: List[Tensor],  # åç§»é‡åˆ—è¡¨
        feats: List[Tensor],  # ç‰¹å¾åˆ—è¡¨
        max_seq_len: Optional[int] = None,
        temperature: float = 1.0,
        eos_threshold: float = 0.5,
        debug: bool = False
    ):
        """3D autoregressiveç”Ÿæˆ"""
        max_seq_len = max_seq_len or self.max_seq_len
        batch_size = rgb_image.shape[0]
        device = rgb_image.device
        
        # å‡†å¤‡ç‚¹äº‘æ•°æ®
        point_cloud_data = {
            'coord': torch.cat(coords, dim=0),
            'grid_coord': torch.cat(grid_coords, dim=0), 
            'offset': torch.cat(offsets, dim=0),
            'feat': torch.cat(feats, dim=0),
            'pixel_coords': torch.cat(pixel_coords, dim=0) if pixel_coords else None
        }
        
        # ä½¿ç”¨åŒæ¨¡æ€ç¼–ç å™¨å¤„ç†RGBå›¾åƒå’Œç‚¹äº‘æ•°æ®
        fused_embed = self.dual_modal_encoder(rgb_image, point_cloud_data)
        
        # å°†èåˆç‰¹å¾è½¬æ¢ä¸ºç»Ÿä¸€çš„æ ¼å¼å¹¶åˆ›å»ºmask
        if len(fused_embed) > 0:
            # æ‰¾åˆ°æœ€å¤§ç‰¹å¾æ•°é‡
            max_feat_len = max(feat.shape[0] for feat in fused_embed)
            fusion_dim = fused_embed[0].shape[1]
            batch_size = len(fused_embed)
            
            # å¡«å……åˆ°ç»Ÿä¸€é•¿åº¦å¹¶åˆ›å»ºmask
            padded_features = []
            mask = torch.zeros(batch_size, max_feat_len, dtype=torch.bool, device=fused_embed[0].device)
            
            for i, feat in enumerate(fused_embed):
                feat_len = feat.shape[0]
                if feat_len < max_feat_len:
                    # ç”¨é›¶å¡«å……
                    padding = torch.zeros(max_feat_len - feat_len, fusion_dim, 
                                        device=feat.device, dtype=feat.dtype)
                    padded_feat = torch.cat([feat, padding], dim=0)
                else:
                    padded_feat = feat
                padded_features.append(padded_feat)
                
                # è®¾ç½®maskï¼šæœ‰æ•ˆä½ç½®ä¸ºTrueï¼Œå¡«å……ä½ç½®ä¸ºFalse
                mask[i, :feat_len] = True
            
            # å †å æˆbatch
            fused_embed = torch.stack(padded_features, dim=0)  # [batch_size, max_feat_len, fusion_dim]
        else:
            # ç©ºbatchçš„æƒ…å†µ
            fused_embed = torch.empty(0, 0, 0)
            mask = torch.empty(0, 0, dtype=torch.bool)
        
        
        # ä¸ºæ¯ä¸ªæ ·æœ¬ç‹¬ç«‹è·Ÿè¸ª3Dç”Ÿæˆç»“æœ
        generated_results = {
            'x': [[] for _ in range(batch_size)],
            'y': [[] for _ in range(batch_size)],
            'z': [[] for _ in range(batch_size)],  # æ–°å¢zåæ ‡
            'w': [[] for _ in range(batch_size)],
            'h': [[] for _ in range(batch_size)],
            'l': [[] for _ in range(batch_size)]   # æ–°å¢length
        }
        
        # è·Ÿè¸ªæ¯ä¸ªæ ·æœ¬æ˜¯å¦å·²ç»åœæ­¢ç”Ÿæˆ
        stopped_samples = torch.zeros(batch_size, dtype=torch.bool, device=image.device)
        
        # åˆå§‹åºåˆ—ï¼šåªæœ‰SOS token
        current_sequence = repeat(self.sos_token, 'n d -> b n d', b=batch_size)
        
        for step in range(max_seq_len):
            # å¦‚æœæ‰€æœ‰æ ·æœ¬éƒ½åœæ­¢äº†ï¼Œæå‰ç»“æŸ
            if torch.all(stopped_samples):
                break
            
            primitive_codes = current_sequence
            seq_len = primitive_codes.shape[1]
            pos_embed = self.pos_embed[:, :seq_len, :]
            primitive_codes = primitive_codes + pos_embed
            
            # èåˆç‰¹å¾æ¡ä»¶åŒ–å¤„ç†ï¼ˆRGB+ç‚¹äº‘èåˆç‰¹å¾ï¼‰
            if self.condition_on_image and self.fused_film_cond is not None:
                # ä½¿ç”¨maskedå¹³å‡æ± åŒ–è€Œä¸æ˜¯ç®€å•å¹³å‡
                pooled_fused_embed = self.masked_mean_pooling(fused_embed, mask)  # [batch_size, fusion_dim]
                fused_cond = self.fused_cond_proj_film(pooled_fused_embed)  # æŠ•å½±åˆ°æ¡ä»¶ç»´åº¦
                primitive_codes = self.fused_film_cond(primitive_codes, fused_cond)  # FiLMè°ƒåˆ¶
            
            # é—¨æ§å¾ªç¯å—å¤„ç†
            if self.gateloop_block is not None:
                primitive_codes, gateloop_cache = self.gateloop_block(primitive_codes)
            
            # é€šè¿‡decoderè·å–attended codes
            attended_codes = self.decoder(
                primitive_codes,
                context=fused_embed,
            )
            
            # ç”¨æœ€åä¸€ä¸ªä½ç½®é¢„æµ‹ä¸‹ä¸€ä¸ªtoken
            next_embed = attended_codes[:, -1]
            
            # é¢„æµ‹3Dåæ ‡å’Œå°ºå¯¸ - æŒ‰é¡ºåºï¼šx, y, z, w, h, l
            # é¢„æµ‹xåæ ‡ - ä½¿ç”¨è¿ç»­å€¼embedding
            x_logits = self.to_x_logits(next_embed)
            x_delta = torch.tanh(self.to_x_delta(next_embed).squeeze(-1)) * 0.5
            if temperature == 0:
                next_x_discrete = x_logits.argmax(dim=-1)
            else:
                x_probs = F.softmax(x_logits / temperature, dim=-1)
                next_x_discrete = torch.multinomial(x_probs, 1).squeeze(-1)
            
            # è®¡ç®—xçš„è¿ç»­å€¼ç”¨äºåç»­é¢„æµ‹
            x_continuous_base = self.continuous_from_discrete(next_x_discrete, self.num_discrete_x, self.continuous_range_x)
            x_continuous = x_continuous_base + x_delta
            x_embed = self.get_continuous_embed('x', x_continuous)
            
            # é¢„æµ‹yåæ ‡ - ä½¿ç”¨è¿ç»­å€¼embedding
            y_input = torch.cat([next_embed, x_embed], dim=-1)
            y_logits = self.to_y_logits(y_input)
            y_delta = torch.tanh(self.to_y_delta(y_input).squeeze(-1)) * 0.5
            
            if temperature == 0:
                next_y_discrete = y_logits.argmax(dim=-1)
            else:
                y_probs = F.softmax(y_logits / temperature, dim=-1)
                next_y_discrete = torch.multinomial(y_probs, 1).squeeze(-1)
            
            # è®¡ç®—yçš„è¿ç»­å€¼ç”¨äºåç»­é¢„æµ‹
            y_continuous_base = self.continuous_from_discrete(next_y_discrete, self.num_discrete_y, self.continuous_range_y)
            y_continuous = y_continuous_base + y_delta
            y_embed = self.get_continuous_embed('y', y_continuous)
            
            # é¢„æµ‹zåæ ‡ - ä½¿ç”¨è¿ç»­å€¼embedding
            z_input = torch.cat([next_embed, x_embed, y_embed], dim=-1)
            z_logits = self.to_z_logits(z_input)
            z_delta = torch.tanh(self.to_z_delta(z_input).squeeze(-1)) * 0.5
            
            if temperature == 0:
                next_z_discrete = z_logits.argmax(dim=-1)
            else:
                z_probs = F.softmax(z_logits / temperature, dim=-1)
                next_z_discrete = torch.multinomial(z_probs, 1).squeeze(-1)
            
            # è®¡ç®—zçš„è¿ç»­å€¼ç”¨äºåç»­é¢„æµ‹
            z_continuous_base = self.continuous_from_discrete(next_z_discrete, self.num_discrete_z, self.continuous_range_z)
            z_continuous = z_continuous_base + z_delta
            z_embed = self.get_continuous_embed('z', z_continuous)
            
            # é¢„æµ‹wï¼ˆå®½åº¦ï¼‰- ä½¿ç”¨è¿ç»­å€¼embedding
            w_input = torch.cat([next_embed, x_embed, y_embed, z_embed], dim=-1)
            w_logits = self.to_w_logits(w_input)
            w_delta = torch.tanh(self.to_w_delta(w_input).squeeze(-1)) * 0.5
            
            if temperature == 0:
                next_w_discrete = w_logits.argmax(dim=-1)
            else:
                w_probs = F.softmax(w_logits / temperature, dim=-1)
                next_w_discrete = torch.multinomial(w_probs, 1).squeeze(-1)
            
            # è®¡ç®—wçš„è¿ç»­å€¼ç”¨äºåç»­é¢„æµ‹
            w_continuous_base = self.continuous_from_discrete(next_w_discrete, self.num_discrete_w, self.continuous_range_w)
            w_continuous = w_continuous_base + w_delta
            w_embed = self.get_continuous_embed('w', w_continuous)
            
            # é¢„æµ‹hï¼ˆé«˜åº¦ï¼‰- ä½¿ç”¨è¿ç»­å€¼embedding
            h_input = torch.cat([next_embed, x_embed, y_embed, z_embed, w_embed], dim=-1)
            h_logits = self.to_h_logits(h_input)
            h_delta = torch.tanh(self.to_h_delta(h_input).squeeze(-1)) * 0.5
            
            if temperature == 0:
                next_h_discrete = h_logits.argmax(dim=-1)
            else:
                h_probs = F.softmax(h_logits / temperature, dim=-1)
                next_h_discrete = torch.multinomial(h_probs, 1).squeeze(-1)
            
            # è®¡ç®—hçš„è¿ç»­å€¼ç”¨äºåç»­é¢„æµ‹
            h_continuous_base = self.continuous_from_discrete(next_h_discrete, self.num_discrete_h, self.continuous_range_h)
            h_continuous = h_continuous_base + h_delta
            h_embed = self.get_continuous_embed('h', h_continuous)
            
            # é¢„æµ‹lï¼ˆé•¿åº¦ï¼‰- ä½¿ç”¨è¿ç»­å€¼embedding
            l_input = torch.cat([next_embed, x_embed, y_embed, z_embed, w_embed, h_embed], dim=-1)
            l_logits = self.to_l_logits(l_input)
            l_delta = torch.tanh(self.to_l_delta(l_input).squeeze(-1)) * 0.5
            
            if temperature == 0:
                next_l_discrete = l_logits.argmax(dim=-1)
            else:
                l_probs = F.softmax(l_logits / temperature, dim=-1)
                next_l_discrete = torch.multinomial(l_probs, 1).squeeze(-1)
            
            # è®¡ç®—lçš„è¿ç»­å€¼
            l_continuous_base = self.continuous_from_discrete(next_l_discrete, self.num_discrete_l, self.continuous_range_l)
            l_continuous = l_continuous_base + l_delta
            
            # ä½¿ç”¨å·²ç»è®¡ç®—å¥½çš„è¿ç»­å€¼ï¼ˆåŒ…å«deltaï¼‰
            x_center_pred = x_continuous
            y_center_pred = y_continuous
            z_center_pred = z_continuous
            w_center_pred = w_continuous
            h_center_pred = h_continuous
            l_center_pred = l_continuous

            # è¿ç»­å€¼å·²ç»åœ¨ä¸Šé¢è®¡ç®—å¥½äº†ï¼ˆåŒ…å«deltaï¼‰ï¼Œç›´æ¥ä½¿ç”¨å¹¶åº”ç”¨èŒƒå›´é™åˆ¶
            x_continuous = x_center_pred.clamp(self.continuous_range_x[0], self.continuous_range_x[1])
            y_continuous = y_center_pred.clamp(self.continuous_range_y[0], self.continuous_range_y[1])
            z_continuous = z_center_pred.clamp(self.continuous_range_z[0], self.continuous_range_z[1])
            w_continuous = w_center_pred.clamp(self.continuous_range_w[0], self.continuous_range_w[1])
            h_continuous = h_center_pred.clamp(self.continuous_range_h[0], self.continuous_range_h[1])
            l_continuous = l_center_pred.clamp(self.continuous_range_l[0], self.continuous_range_l[1])
            
            # åªä¸ºæœªåœæ­¢çš„æ ·æœ¬ä¿å­˜3Dç»“æœ
            for i in range(batch_size):
                if not stopped_samples[i]:
                    generated_results['x'][i].append(x_continuous[i])
                    generated_results['y'][i].append(y_continuous[i])
                    generated_results['z'][i].append(z_continuous[i])
                    generated_results['w'][i].append(w_continuous[i])
                    generated_results['h'][i].append(h_continuous[i])
                    generated_results['l'][i].append(l_continuous[i])
            
            # é¢„æµ‹EOS
            eos_logits = self.to_eos_logits(next_embed).squeeze(-1)
            eos_prob = torch.sigmoid(eos_logits)
            new_stopped = eos_prob > eos_threshold
            stopped_samples = stopped_samples | new_stopped
            if debug:
                print(f"Step {step}: EOS probs = {eos_prob.tolist()}, stopped(next) = {stopped_samples.tolist()}")
            
            # ç¼–ç 3Dé¢„æµ‹ç»“æœå¹¶æ·»åŠ åˆ°åºåˆ—
            pred_embed, _ = self.encode_primitive(
                x_continuous.unsqueeze(0), y_continuous.unsqueeze(0), z_continuous.unsqueeze(0),
                w_continuous.unsqueeze(0), h_continuous.unsqueeze(0), l_continuous.unsqueeze(0),
                torch.ones_like(x_continuous, dtype=torch.bool).unsqueeze(0)
            )

            pred_embed = pred_embed.transpose(0, 1)
            current_sequence = torch.cat([current_sequence, pred_embed], dim=1)
        
        # å°†3Dç»“æœè½¬æ¢ä¸ºå¼ é‡æ ¼å¼
        max_len = max(len(generated_results['x'][i]) for i in range(batch_size))
        
        if max_len == 0:
            return None
        
        # åˆ›å»º3Dç»“æœå¼ é‡
        result = {
            'x': torch.zeros(batch_size, max_len, device=device),
            'y': torch.zeros(batch_size, max_len, device=device),
            'z': torch.zeros(batch_size, max_len, device=device),
            'w': torch.zeros(batch_size, max_len, device=device),
            'h': torch.zeros(batch_size, max_len, device=device),
            'l': torch.zeros(batch_size, max_len, device=device),
        }
        
        for i in range(batch_size):
            seq_len = len(generated_results['x'][i])
            if seq_len > 0:
                result['x'][i, :seq_len] = torch.stack(generated_results['x'][i])
                result['y'][i, :seq_len] = torch.stack(generated_results['y'][i])
                result['z'][i, :seq_len] = torch.stack(generated_results['z'][i])
                result['w'][i, :seq_len] = torch.stack(generated_results['w'][i])
                result['h'][i, :seq_len] = torch.stack(generated_results['h'][i])
                result['l'][i, :seq_len] = torch.stack(generated_results['l'][i])
        
        return result
    
    # ======================== å¢é‡æ¨ç†ç›¸å…³ä»£ç  ========================
    
    def initialize_incremental_generation(
        self,
        rgb_image: Tensor,
        coords: Tensor,
        grid_coords: Tensor,
        offsets: Tensor,
        feats: Tensor,
        pixel_coords: Tensor = None,
        max_seq_len: Optional[int] = None,
        temperature: float = 1.0,
        eos_threshold: float = 0.5
    ) -> IncrementalState:
        """
        åˆå§‹åŒ–å¢é‡ç”ŸæˆçŠ¶æ€
        
        Args:
            rgb_image: [B, 3, H, W] RGBè¾“å…¥å›¾åƒ
            coords: List[Tensor] åæ ‡åˆ—è¡¨
            grid_coords: List[Tensor] ç½‘æ ¼åæ ‡åˆ—è¡¨
            offsets: List[Tensor] åç§»é‡åˆ—è¡¨
            feats: List[Tensor] ç‰¹å¾åˆ—è¡¨
            max_seq_len: æœ€å¤§åºåˆ—é•¿åº¦
            temperature: é‡‡æ ·æ¸©åº¦
            eos_threshold: EOSé˜ˆå€¼
            
        Returns:
            state: åˆå§‹åŒ–çš„å¢é‡çŠ¶æ€
        """
        batch_size = rgb_image.shape[0]
        max_seq_len = max_seq_len or self.max_seq_len
        device = rgb_image.device
        
        # 1. ä½¿ç”¨åŒæ¨¡æ€ç¼–ç å™¨å¤„ç†RGBå›¾åƒå’Œç‚¹äº‘æ•°æ®ï¼ˆåªè®¡ç®—ä¸€æ¬¡ï¼‰
        with torch.no_grad():
            # å‡†å¤‡ç‚¹äº‘æ•°æ®
            point_cloud_data = {
                'coord': coords,
                'grid_coord': grid_coords, 
                'offset': offsets,
                'feat': feats,
                'pixel_coords': pixel_coords
            }
            
            fused_embed = self.dual_modal_encoder(rgb_image, point_cloud_data)
            
            # å°†èåˆç‰¹å¾è½¬æ¢ä¸ºç»Ÿä¸€çš„æ ¼å¼å¹¶åˆ›å»ºmask
            if len(fused_embed) > 0:
                # æ‰¾åˆ°æœ€å¤§ç‰¹å¾æ•°é‡
                max_feat_len = max(feat.shape[0] for feat in fused_embed)
                fusion_dim = fused_embed[0].shape[1]
                batch_size = len(fused_embed)
                
                # å¡«å……åˆ°ç»Ÿä¸€é•¿åº¦å¹¶åˆ›å»ºmask
                padded_features = []
                mask = torch.zeros(batch_size, max_feat_len, dtype=torch.bool, device=fused_embed[0].device)
                
                for i, feat in enumerate(fused_embed):
                    feat_len = feat.shape[0]
                    if feat_len < max_feat_len:
                        # ç”¨é›¶å¡«å……
                        padding = torch.zeros(max_feat_len - feat_len, fusion_dim, 
                                            device=feat.device, dtype=feat.dtype)
                        padded_feat = torch.cat([feat, padding], dim=0)
                    else:
                        padded_feat = feat
                    padded_features.append(padded_feat)
                    
                    # è®¾ç½®maskï¼šæœ‰æ•ˆä½ç½®ä¸ºTrueï¼Œå¡«å……ä½ç½®ä¸ºFalse
                    mask[i, :feat_len] = True
                
                # å †å æˆbatch
                fused_embed = torch.stack(padded_features, dim=0)  # [batch_size, max_feat_len, fusion_dim]
            else:
                # ç©ºbatchçš„æƒ…å†µ
                fused_embed = torch.empty(0, 0, 0)
                mask = torch.empty(0, 0, dtype=torch.bool)
            
            # å‡†å¤‡èåˆç‰¹å¾æ¡ä»¶åŒ–ï¼ˆRGB+ç‚¹äº‘èåˆç‰¹å¾ï¼‰
            fused_cond = None
            if self.condition_on_image and self.fused_film_cond is not None:
                # ä½¿ç”¨maskedå¹³å‡æ± åŒ–è€Œä¸æ˜¯ç®€å•å¹³å‡
                pooled_fused_embed = self.masked_mean_pooling(fused_embed, mask)  # [batch_size, fusion_dim]
                fused_cond = self.fused_cond_proj_film(pooled_fused_embed)  # æŠ•å½±åˆ°æ¡ä»¶ç»´åº¦
        
        # 2. åˆå§‹åŒ–åºåˆ—çŠ¶æ€
        current_sequence = repeat(self.sos_token, 'n d -> b n d', b=batch_size)
        
        # 3. åˆå§‹åŒ–ç”Ÿæˆç»“æœè·Ÿè¸ª
        generated_boxes = {
            'x': [[] for _ in range(batch_size)],
            'y': [[] for _ in range(batch_size)],
            'z': [[] for _ in range(batch_size)],
            'w': [[] for _ in range(batch_size)],
            'h': [[] for _ in range(batch_size)],
            'l': [[] for _ in range(batch_size)]
        }
        
        stopped_samples = torch.zeros(batch_size, dtype=torch.bool, device=device)
        
        # 4. åˆ›å»ºçŠ¶æ€å¯¹è±¡
        state = IncrementalState(
            current_sequence=current_sequence,
            fused_embed=fused_embed,
            fused_cond=fused_cond,
            stopped_samples=stopped_samples,
            current_step=0,
            mask=mask,
            decoder_cache=None,
            gateloop_cache=[],
            generated_boxes=generated_boxes
        )
        
        return state
    
    def generate_next_box_incremental(
        self, 
        state: IncrementalState, 
        temperature: float = 1.0,
        eos_threshold: float = 0.5
    ) -> Tuple[Optional[Dict], bool]:
        """
        å¢é‡ç”Ÿæˆä¸‹ä¸€ä¸ªboxï¼Œå‚è€ƒPrimitiveAnythingçš„å®ç°ä½¿ç”¨çœŸæ­£çš„KVç¼“å­˜
        
        Args:
            state: å¢é‡ç”ŸæˆçŠ¶æ€ï¼ˆåŒ…å«å¤šçº§ç¼“å­˜ï¼‰
            temperature: é‡‡æ ·æ¸©åº¦
            eos_threshold: EOSé˜ˆå€¼
            
        Returns:
            box_prediction: é¢„æµ‹çš„boxå±æ€§å­—å…¸ï¼Œå¦‚æœåœæ­¢åˆ™ä¸ºNone
            all_stopped: æ˜¯å¦æ‰€æœ‰æ ·æœ¬éƒ½åœæ­¢ç”Ÿæˆ
        """
        if torch.all(state.stopped_samples):
            return None, True
        
        batch_size = state.current_sequence.shape[0]
        device = state.current_sequence.device
        current_len = state.current_sequence.shape[1]
        
        # å‚è€ƒPrimitiveAnythingçš„forwardæ–¹æ³•ç»“æ„
        if state.current_step == 0:
            # ç¬¬ä¸€æ­¥ï¼šå®Œæ•´å‰å‘ä¼ æ’­ï¼Œåˆå§‹åŒ–æ‰€æœ‰ç¼“å­˜
            primitive_codes = state.current_sequence  # [B, current_len, dim]
            
            # æ·»åŠ ä½ç½®ç¼–ç 
            pos_embed = self.pos_embed[:, :current_len, :]
            primitive_codes = primitive_codes + pos_embed
            
            # èåˆç‰¹å¾æ¡ä»¶åŒ–ï¼ˆRGB+ç‚¹äº‘ï¼‰
            if state.fused_cond is not None:
                primitive_codes = self.fused_film_cond(primitive_codes, state.fused_cond)
            
            # é—¨æ§å¾ªç¯å—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if self.gateloop_block is not None:
                primitive_codes, gateloop_cache = self.gateloop_block(primitive_codes, cache=None)
                state.gateloop_cache = gateloop_cache if gateloop_cache is not None else []
            
            # Transformerè§£ç ï¼ˆåˆå§‹åŒ–decoderç¼“å­˜ï¼‰
            attended_codes, decoder_cache = self.decoder(
                primitive_codes,
                context=state.fused_embed,
                context_mask=state.mask,  # æ·»åŠ mask
                cache=None,  # ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼Œæ— ç¼“å­˜
                return_hiddens=True  # è¿”å›ä¸­é—´çŠ¶æ€ç”¨äºç¼“å­˜
            )
            
            # ä¿å­˜decoderç¼“å­˜
            state.decoder_cache = decoder_cache
            
        else:
            # åç»­æ­¥éª¤ï¼šåªå¤„ç†æ–°æ·»åŠ çš„tokenï¼Œä½¿ç”¨ç¼“å­˜ï¼ˆçœŸæ­£çš„å¢é‡ï¼ï¼‰
            new_token = state.current_sequence[:, -1:, :]  # [B, 1, dim] - åªæœ‰æœ€æ–°çš„token
            
            # ğŸ”§ ä¿®å¤ï¼šæ£€æŸ¥ä½ç½®ç¼–ç è¾¹ç•Œï¼Œé˜²æ­¢ç´¢å¼•è¶…å‡ºèŒƒå›´
            pos_index = current_len - 1
            if pos_index >= self.pos_embed.shape[1]:
                print(f"âš ï¸  Position index {pos_index} exceeds pos_embed size {self.pos_embed.shape[1]}")
                print(f"   Using last available position {self.pos_embed.shape[1] - 1}")
                pos_index = self.pos_embed.shape[1] - 1
            
            # æ·»åŠ ä½ç½®ç¼–ç ï¼ˆåªå¯¹æ–°tokenï¼‰
            pos_embed = self.pos_embed[:, pos_index:pos_index+1, :]
            primitive_codes = new_token + pos_embed
            
            # èåˆç‰¹å¾æ¡ä»¶åŒ–ï¼ˆRGB+ç‚¹äº‘ï¼Œåªå¯¹æ–°tokenï¼‰
            if state.fused_cond is not None:
                primitive_codes = self.fused_film_cond(primitive_codes, state.fused_cond)
            
            # é—¨æ§å¾ªç¯å—å¢é‡è®¡ç®—
            if self.gateloop_block is not None:
                primitive_codes, new_gateloop_cache = self.gateloop_block(
                    primitive_codes, 
                    cache=state.gateloop_cache
                )
                state.gateloop_cache = new_gateloop_cache if new_gateloop_cache is not None else state.gateloop_cache
            
            # çœŸæ­£çš„å¢é‡Transformerè§£ç ï¼
            attended_codes, new_decoder_cache = self.decoder(
                primitive_codes,  # åªæœ‰æ–°token [B, 1, dim]
                context=state.fused_embed,
                context_mask=state.mask,  # æ·»åŠ mask
                cache=state.decoder_cache,  # ä½¿ç”¨ä¹‹å‰çš„decoderç¼“å­˜
                return_hiddens=True
            )
            
            # æ›´æ–°decoderç¼“å­˜
            state.decoder_cache = new_decoder_cache
        
        # é¢„æµ‹ä¸‹ä¸€ä¸ªtokenï¼ˆåªéœ€è¦æœ€åä¸€ä¸ªä½ç½®ï¼‰
        # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ å®‰å…¨æ£€æŸ¥ï¼Œé˜²æ­¢attended_codesä¸ºç©º
        if attended_codes.shape[1] == 0:
            print(f"âŒ Error: attended_codes shape is {attended_codes.shape}")
            print(f"   current_step: {state.current_step}")
            print(f"   current_sequence shape: {state.current_sequence.shape}")
            print(f"   primitive_codes shape: {primitive_codes.shape}")
            print(f"   fused_embed shape: {state.fused_embed.shape}")
            if state.current_step == 0:
                print("   This is step 0 (initialization)")
            else:
                print(f"   This is step {state.current_step} (incremental)")
            raise RuntimeError("attended_codes is empty - this shouldn't happen")
        
        next_embed = attended_codes[:, -1, :]
        
        # æŒ‰é¡ºåºé¢„æµ‹å„ä¸ªå±æ€§
        box_prediction = {}
        
        # ğŸ”§ ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨predict_attribute_with_continuous_embedçš„è¾“å‡ºï¼ˆä¸è®­ç»ƒé€»è¾‘ä¸€è‡´ï¼‰
        # é¢„æµ‹xåæ ‡
        x_logits, x_delta, x_continuous, x_embed = self.predict_attribute_with_continuous_embed(
            next_embed, 'x', prev_embeds=None, use_gumbel=False, temperature=temperature
        )
        # âœ… ç›´æ¥ä½¿ç”¨x_continuousï¼Œä¸è®­ç»ƒé€»è¾‘ä¸€è‡´
        box_prediction['x'] = x_continuous
        
        # é¢„æµ‹yåæ ‡  
        y_logits, y_delta, y_continuous, y_embed = self.predict_attribute_with_continuous_embed(
            next_embed, 'y', prev_embeds=[x_embed], use_gumbel=False, temperature=temperature
        )
        # âœ… ç›´æ¥ä½¿ç”¨y_continuousï¼Œä¸è®­ç»ƒé€»è¾‘ä¸€è‡´
        box_prediction['y'] = y_continuous
        
        # é¢„æµ‹zåæ ‡
        z_logits, z_delta, z_continuous, z_embed = self.predict_attribute_with_continuous_embed(
            next_embed, 'z', prev_embeds=[x_embed, y_embed], use_gumbel=False, temperature=temperature
        )
        # âœ… ç›´æ¥ä½¿ç”¨z_continuousï¼Œä¸è®­ç»ƒé€»è¾‘ä¸€è‡´
        box_prediction['z'] = z_continuous
        
        # é¢„æµ‹wï¼ˆå®½åº¦ï¼‰
        w_logits, w_delta, w_continuous, w_embed = self.predict_attribute_with_continuous_embed(
            next_embed, 'w', prev_embeds=[x_embed, y_embed, z_embed], use_gumbel=False, temperature=temperature
        )
        # âœ… ç›´æ¥ä½¿ç”¨w_continuousï¼Œä¸è®­ç»ƒé€»è¾‘ä¸€è‡´
        box_prediction['w'] = w_continuous
        
        # é¢„æµ‹hï¼ˆé«˜åº¦ï¼‰
        h_logits, h_delta, h_continuous, h_embed = self.predict_attribute_with_continuous_embed(
            next_embed, 'h', prev_embeds=[x_embed, y_embed, z_embed, w_embed], use_gumbel=False, temperature=temperature
        )
        # âœ… ç›´æ¥ä½¿ç”¨h_continuousï¼Œä¸è®­ç»ƒé€»è¾‘ä¸€è‡´
        box_prediction['h'] = h_continuous
        
        # é¢„æµ‹lï¼ˆé•¿åº¦ï¼‰
        l_logits, l_delta, l_continuous, l_embed = self.predict_attribute_with_continuous_embed(
            next_embed, 'l', prev_embeds=[x_embed, y_embed, z_embed, w_embed, h_embed], use_gumbel=False, temperature=temperature
        )
        # âœ… ç›´æ¥ä½¿ç”¨l_continuousï¼Œä¸è®­ç»ƒé€»è¾‘ä¸€è‡´
        box_prediction['l'] = l_continuous
        
        # EOSé¢„æµ‹
        eos_logits = self.to_eos_logits(next_embed).squeeze(-1)  # [B]
        eos_probs = torch.sigmoid(eos_logits)
        
        # æ›´æ–°åœæ­¢çŠ¶æ€
        new_stops = eos_probs > eos_threshold
        state.stopped_samples = state.stopped_samples | new_stops
        
        # ä¿å­˜ç”Ÿæˆç»“æœï¼ˆåªä¸ºæœªåœæ­¢çš„æ ·æœ¬ï¼‰
        for i in range(batch_size):
            if not state.stopped_samples[i]:
                for attr in ['x', 'y', 'z', 'w', 'h', 'l']:
                    # ä¿å­˜tensorè€Œä¸æ˜¯floatï¼Œä»¥ä¾¿åç»­stackæ“ä½œ
                    state.generated_boxes[attr][i].append(box_prediction[attr][i:i+1])  # ä¿æŒtensorå½¢çŠ¶
        
        # ğŸ”§ ä¿®å¤Bug: æ›´æ–°current_sequenceä»¥ä¾¿ä¸‹ä¸€æ­¥ä½¿ç”¨
        # æ„å»ºä¸‹ä¸€æ­¥çš„è¾“å…¥embedding
        next_embeds = []
        for attr in ['x', 'y', 'z', 'w', 'h', 'l']:
            continuous_val = box_prediction[attr]
            # è·å–å¯¹åº”çš„ç¦»æ•£åŒ–å‚æ•°
            num_discrete = getattr(self, f'num_discrete_{attr}')
            continuous_range = getattr(self, f'continuous_range_{attr}')
            
            # ç¦»æ•£åŒ–è¿ç»­å€¼
            attr_discrete = self.discretize(continuous_val, num_discrete, continuous_range)
            attr_embed = getattr(self, f'{attr}_embed')(attr_discrete)
            next_embeds.append(attr_embed)
        
        # ç»„åˆæ‰€æœ‰å±æ€§çš„embedding
        combined_embed = torch.cat(next_embeds, dim=-1)  # [B, total_embed_dim]
        projected_embed = self.project_in(combined_embed).unsqueeze(1)  # [B, 1, model_dim]
        
        # æ›´æ–°å½“å‰åºåˆ—ï¼ˆè¿™æ˜¯å…³é”®çš„ä¿®å¤ï¼ï¼‰
        state.current_sequence = torch.cat([state.current_sequence, projected_embed], dim=1)
        
        # æ›´æ–°æ­¥æ•°
        state.current_step += 1
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ ·æœ¬éƒ½åœæ­¢
        all_stopped = torch.all(state.stopped_samples)
        
        return box_prediction, all_stopped
    
    def _sample_discrete(self, logits: Tensor, temperature: float) -> Tensor:
        """ç¦»æ•£é‡‡æ ·"""
        if temperature == 0:
            return logits.argmax(dim=-1)
        else:
            probs = F.softmax(logits / temperature, dim=-1)
            return torch.multinomial(probs, 1).squeeze(-1)
    
    def _compute_continuous_value_from_discrete_delta(self, discrete: Tensor, delta: Tensor, attr: str) -> Tensor:
        """ä»ç¦»æ•£å€¼å’Œdeltaè®¡ç®—è¿ç»­å€¼"""
        # è·å–å±æ€§é…ç½®
        if attr == 'x':
            num_bins, value_range = self.num_discrete_x, self.continuous_range_x
        elif attr == 'y':
            num_bins, value_range = self.num_discrete_y, self.continuous_range_y
        elif attr == 'z':
            num_bins, value_range = self.num_discrete_z, self.continuous_range_z
        elif attr == 'w':
            num_bins, value_range = self.num_discrete_w, self.continuous_range_w
        elif attr == 'h':
            num_bins, value_range = self.num_discrete_h, self.continuous_range_h
        elif attr == 'l':
            num_bins, value_range = self.num_discrete_l, self.continuous_range_l
        else:
            raise ValueError(f"Unknown attribute: {attr}")
        
        # è®¡ç®—è¿ç»­å€¼
        continuous_base = self.continuous_from_discrete(discrete, num_bins, value_range)
        continuous_value = continuous_base + delta
        
        # åº”ç”¨èŒƒå›´é™åˆ¶
        continuous_value = continuous_value.clamp(value_range[0], value_range[1])
        
        return continuous_value
    
    @eval_decorator
    @torch.no_grad()
    def generate_incremental(
        self,
        rgb_image: Tensor,
        coords: Tensor,
        grid_coords: Tensor,
        offsets: Tensor,
        feats: Tensor,
        pixel_coords: Tensor = None,
        max_seq_len: Optional[int] = None,
        temperature: float = 1.0,
        eos_threshold: float = 0.5,
        return_state: bool = False
    ) -> Dict[str, Tensor]:
        """
        å®Œæ•´çš„å¢é‡ç”Ÿæˆæµç¨‹
        
        Args:
            rgb_image: [B, 3, H, W] RGBè¾“å…¥å›¾åƒ
            coords: List[Tensor] åæ ‡åˆ—è¡¨
            grid_coords: List[Tensor] ç½‘æ ¼åæ ‡åˆ—è¡¨
            offsets: List[Tensor] åç§»é‡åˆ—è¡¨
            feats: List[Tensor] ç‰¹å¾åˆ—è¡¨
            max_seq_len: æœ€å¤§åºåˆ—é•¿åº¦
            temperature: é‡‡æ ·æ¸©åº¦
            eos_threshold: EOSé˜ˆå€¼
            return_state: æ˜¯å¦è¿”å›æœ€ç»ˆçŠ¶æ€
            
        Returns:
            results: ç”Ÿæˆçš„å®Œæ•´åºåˆ—
        """
        batch_size = rgb_image.shape[0]
        max_seq_len = max_seq_len or self.max_seq_len
        device = rgb_image.device
        
        # åˆå§‹åŒ–ç”ŸæˆçŠ¶æ€
        state = self.initialize_incremental_generation(rgb_image, coords, grid_coords, offsets, feats, pixel_coords, max_seq_len, temperature, eos_threshold)
        
        # é€æ­¥ç”Ÿæˆ
        for step in range(max_seq_len):
            # ğŸ”§ ä¿®å¤ï¼šæ£€æŸ¥åºåˆ—é•¿åº¦ï¼Œé˜²æ­¢è¶…è¿‡ä½ç½®ç¼–ç èŒƒå›´
            current_seq_len = state.current_sequence.shape[1]
            if current_seq_len >= self.pos_embed.shape[1]:
                print(f"âš ï¸  Sequence length {current_seq_len} would exceed pos_embed size {self.pos_embed.shape[1]}")
                print(f"   Stopping generation early at step {step}")
                break
                
            box_prediction, all_stopped = self.generate_next_box_incremental(state, temperature, eos_threshold)
            
            if all_stopped or box_prediction is None:
                break
        
        # è½¬æ¢ç»“æœä¸ºå¼ é‡æ ¼å¼
        results = self._convert_incremental_results_to_tensors(state.generated_boxes, batch_size, device)
        
        if return_state:
            return results, state
        else:
            return results
    
    def _convert_incremental_results_to_tensors(self, generated_boxes: Dict, batch_size: int, device: torch.device) -> Dict[str, Tensor]:
        """å°†å¢é‡ç”Ÿæˆç»“æœè½¬æ¢ä¸ºå¼ é‡æ ¼å¼"""
        max_len = max(len(generated_boxes['x'][i]) for i in range(batch_size))
        
        if max_len == 0:
            return None
        
        result = {}
        for attr in ['x', 'y', 'z', 'w', 'h', 'l']:
            result[attr] = torch.zeros(batch_size, max_len, device=device)
            
            for i in range(batch_size):
                seq_len = len(generated_boxes[attr][i])
                if seq_len > 0:
                    # è¿æ¥tensoråˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯[1]å½¢çŠ¶çš„tensor
                    result[attr][i, :seq_len] = torch.cat(generated_boxes[attr][i], dim=0)
        
        # æ·»åŠ é•¿åº¦ä¿¡æ¯ï¼Œæ–¹ä¾¿åç»­ç»Ÿè®¡
        result['generated_lengths'] = torch.tensor([len(generated_boxes['x'][i]) for i in range(batch_size)], device=device)
        
        return result 