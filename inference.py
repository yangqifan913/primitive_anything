#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
3Dç‰©ä½“æ£€æµ‹æ¨ç†è„šæœ¬
åŠ è½½ä¿å­˜çš„checkpoint.pthï¼Œå¯¹æŒ‡å®šæ•°æ®è¿›è¡Œæ¨ç†ï¼Œå¹¶ä¿å­˜ä¸ºJSONæ ¼å¼
"""

import argparse
import json
import os
import sys
import time
from pathlib import Path
from typing import Dict, List, Any, Optional
import warnings

import torch
import torch.nn.functional as F
import numpy as np
from torch.utils.data import DataLoader
from tqdm import tqdm

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from config_loader import ConfigLoader
from primitive_anything_3d import PrimitiveTransformer3D
from dataloader_3d import Box3DDataset, create_dataloader


class InferenceEngine:
    """3Dç‰©ä½“æ£€æµ‹æ¨ç†å¼•æ“"""
    
    def __init__(
        self,
        config_path: str,
        checkpoint_path: str,
        device: str = "cuda",
        use_incremental: bool = True,
        temperature: float = None,  # æ”¹ä¸ºNoneï¼Œä»é…ç½®æ–‡ä»¶è¯»å–
        eos_threshold: float = 0.5
    ):
        """
        åˆå§‹åŒ–æ¨ç†å¼•æ“
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            checkpoint_path: checkpointæ–‡ä»¶è·¯å¾„
            device: æ¨ç†è®¾å¤‡
            use_incremental: æ˜¯å¦ä½¿ç”¨å¢é‡æ¨ç†
            temperature: é‡‡æ ·æ¸©åº¦
            eos_threshold: EOSé˜ˆå€¼
        """
        self.config_path = config_path
        self.checkpoint_path = checkpoint_path
        self.device = device
        self.use_incremental = use_incremental
        self.eos_threshold = eos_threshold
        
        # åŠ è½½é…ç½®
        self.config_loader = ConfigLoader()
        self.config_loader.load_unified_config(config_path)
        
        # ä»é…ç½®æ–‡ä»¶è¯»å–æ¸©åº¦å‚æ•°
        if temperature is None:
            opt_config = self.config_loader.get_training_config().get('optimizations', {})
            inference_config = opt_config.get('incremental_inference', {})
            self.temperature = inference_config.get('temperature', 0.3)
        else:
            self.temperature = temperature
        
        # åˆ›å»ºæ¨¡å‹
        self.model = self._create_model()
        
        # åŠ è½½checkpoint
        self._load_checkpoint()
        
        # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
        self.model.eval()
        
        print(f"âœ… æ¨ç†å¼•æ“åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“ é…ç½®æ–‡ä»¶: {config_path}")
        print(f"ğŸ’¾ Checkpoint: {checkpoint_path}")
        print(f"ğŸ–¥ï¸  è®¾å¤‡: {device}")
        print(f"ğŸ”„ å¢é‡æ¨ç†: {'å¯ç”¨' if use_incremental else 'ç¦ç”¨'}")
        print(f"ğŸŒ¡ï¸  æ¸©åº¦: {temperature}")
        print(f"ğŸ›‘ EOSé˜ˆå€¼: {eos_threshold}")
    
    def _create_model(self) -> PrimitiveTransformer3D:
        """æ ¹æ®é…ç½®åˆ›å»ºæ¨¡å‹"""
        model_config = self.config_loader.get_model_config()
        global_config = self.config_loader.get_global_config()
        data_config = self.config_loader.get_data_config()
        
        # æå–æ¨¡å‹å‚æ•°
        discretization = model_config.get('discretization', {})
        embeddings = model_config.get('embeddings', {})
        transformer = model_config.get('transformer', {})
        image_encoder = model_config.get('image_encoder', {})
        conditioning = model_config.get('conditioning', {})
        advanced = model_config.get('advanced', {})
        
        # è¿ç»­å€¼èŒƒå›´
        continuous_ranges = data_config.get('continuous_ranges', {})
        
        # åˆ›å»ºæ¨¡å‹
        model = PrimitiveTransformer3D(
            # ç¦»æ•£åŒ–å‚æ•°
            num_discrete_x=discretization.get('num_discrete_x', 128),
            num_discrete_y=discretization.get('num_discrete_y', 128),
            num_discrete_z=discretization.get('num_discrete_z', 128),
            num_discrete_w=discretization.get('num_discrete_w', 64),
            num_discrete_h=discretization.get('num_discrete_h', 64),
            num_discrete_l=discretization.get('num_discrete_l', 64),
            
            # è¿ç»­å€¼èŒƒå›´
            continuous_range_x=continuous_ranges.get('x', [0.5, 3.0]),
            continuous_range_y=continuous_ranges.get('y', [-2.5, 2.5]),
            continuous_range_z=continuous_ranges.get('z', [-1.5, 0.5]),
            continuous_range_w=continuous_ranges.get('w', [0.3, 0.7]),
            continuous_range_h=continuous_ranges.get('h', [0.3, 0.7]),
            continuous_range_l=continuous_ranges.get('l', [0.3, 0.7]),
            
            # åµŒå…¥ç»´åº¦
            dim_x_embed=embeddings.get('dim_x_embed', 64),
            dim_y_embed=embeddings.get('dim_y_embed', 64),
            dim_z_embed=embeddings.get('dim_z_embed', 64),
            dim_w_embed=embeddings.get('dim_w_embed', 64),
            dim_h_embed=embeddings.get('dim_h_embed', 64),
            dim_l_embed=embeddings.get('dim_l_embed', 64),
            
            # Transformerå‚æ•°
            dim=transformer.get('dim', 512),
            max_primitive_len=global_config.get('max_seq_len', 12),
            attn_depth=transformer.get('depth', 6),
            attn_dim_head=transformer.get('dim_head', 64),
            attn_heads=transformer.get('heads', 8),
            attn_dropout=transformer.get('attn_dropout', 0.1),
            ff_dropout=transformer.get('ff_dropout', 0.1),
            
            # å›¾åƒç¼–ç å™¨
            image_encoder_dim=image_encoder.get('output_dim', 256),
            use_fpn=image_encoder.get('use_fpn', True),
            backbone=image_encoder.get('backbone', 'resnet50'),
            pretrained=image_encoder.get('pretrained', True),
            
            # æ¡ä»¶åŒ–
            condition_on_image=conditioning.get('condition_on_image', True),
            
            # é«˜çº§é…ç½®
            gateloop_depth=advanced.get('gateloop', {}).get('depth', 2),
            gateloop_use_heinsen=advanced.get('gateloop_use_heinsen', False),
            
            # å…¶ä»–å‚æ•°
            pad_id=global_config.get('pad_id', -1.0)
        )
        
        print(f"âœ… æ¨¡å‹åˆ›å»ºå®Œæˆ")
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")
        
        return model
    
    def _load_checkpoint(self):
        """åŠ è½½checkpoint"""
        if not os.path.exists(self.checkpoint_path):
            raise FileNotFoundError(f"Checkpointæ–‡ä»¶ä¸å­˜åœ¨: {self.checkpoint_path}")
        
        print(f"ğŸ“¥ åŠ è½½checkpoint: {self.checkpoint_path}")
        
        # åŠ è½½checkpoint
        checkpoint = torch.load(self.checkpoint_path, map_location=self.device, weights_only=False)
        
        # æ£€æŸ¥checkpointæ ¼å¼
        if 'model' in checkpoint:
            # æ ‡å‡†checkpointæ ¼å¼
            model_state_dict = checkpoint['model']
            print(f"ğŸ“Š Checkpointä¿¡æ¯:")
            print(f"  - Epoch: {checkpoint.get('epoch', 'unknown')}")
            print(f"  - Loss: {checkpoint.get('loss', 'unknown')}")
            print(f"  - æ¨¡å‹çŠ¶æ€: {'âœ… å·²ä¿å­˜' if 'model' in checkpoint else 'âŒ ç¼ºå¤±'}")
        elif isinstance(checkpoint, dict) and any(key.startswith('image_encoder') or key.startswith('decoder') for key in checkpoint.keys()):
            # ç›´æ¥æ˜¯æ¨¡å‹çŠ¶æ€å­—å…¸
            model_state_dict = checkpoint
            print(f"ğŸ“Š Checkpointæ ¼å¼: ç›´æ¥æ¨¡å‹çŠ¶æ€å­—å…¸")
        else:
            raise ValueError(f"æ— æ³•è¯†åˆ«çš„checkpointæ ¼å¼: {type(checkpoint)}")
        
        # åŠ è½½æ¨¡å‹æƒé‡
        try:
            self.model.load_state_dict(model_state_dict, strict=True)
            print(f"âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸  ä¸¥æ ¼åŠ è½½å¤±è´¥ï¼Œå°è¯•éä¸¥æ ¼åŠ è½½: {e}")
            try:
                self.model.load_state_dict(model_state_dict, strict=False)
                print(f"âœ… æ¨¡å‹æƒé‡éä¸¥æ ¼åŠ è½½æˆåŠŸ")
            except Exception as e2:
                raise RuntimeError(f"æ¨¡å‹æƒé‡åŠ è½½å¤±è´¥: {e2}")
        
        # ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
        self.model = self.model.to(self.device)
        print(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ°è®¾å¤‡: {self.device}")
    
    def create_dataloader(
        self,
        data_root: str,
        stage: str = "test",
        batch_size: int = 1,
        num_workers: int = 0,
        shuffle: bool = False
    ) -> DataLoader:
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        global_config = self.config_loader.get_global_config()
        data_config = self.config_loader.get_data_config()
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = Box3DDataset(
            data_root=data_root,
            stage=stage,
            max_boxes=global_config.get('max_seq_len', 12),
            image_size=global_config.get('image_size', [640, 640]),
            continuous_ranges=data_config.get('continuous_ranges', {}),
            augmentation_config={},  # æ¨ç†æ—¶ä¸ä½¿ç”¨æ•°æ®å¢å¼º
            augment=False,
            pad_id=global_config.get('pad_id', -1.0)
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=num_workers,
            pin_memory=True,
            drop_last=False
        )
        
        print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ")
        print(f"ğŸ“Š æ•°æ®é›†å¤§å°: {len(dataset)}")
        print(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"ğŸ”„ æ•°æ®é˜¶æ®µ: {stage}")
        
        return dataloader
    
    def inference_single_batch(
        self,
        batch: Dict[str, torch.Tensor],
        batch_idx: int
    ) -> Dict[str, Any]:
        """
        å¯¹å•ä¸ªæ‰¹æ¬¡è¿›è¡Œæ¨ç†
        
        Args:
            batch: æ‰¹æ¬¡æ•°æ®
            batch_idx: æ‰¹æ¬¡ç´¢å¼•
            
        Returns:
            æ¨ç†ç»“æœå­—å…¸
        """
        # å‡†å¤‡è¾“å…¥æ•°æ®
        rgbxyz = batch['image'].to(self.device)
        targets = {
            'x': batch['x'].to(self.device),
            'y': batch['y'].to(self.device),
            'z': batch['z'].to(self.device),
            'w': batch['w'].to(self.device),
            'h': batch['h'].to(self.device),
            'l': batch['l'].to(self.device),
            'rotations': batch['rotations'].to(self.device),
        }
        
        # æ£€æŸ¥è¾“å…¥æ•°æ®æœ‰æ•ˆæ€§
        if torch.isnan(rgbxyz).any() or torch.isinf(rgbxyz).any():
            print(f"âš ï¸  æ‰¹æ¬¡ {batch_idx}: è¾“å…¥æ•°æ®åŒ…å«NaNæˆ–Infå€¼ï¼Œè·³è¿‡")
            return None
        
        with torch.no_grad():
            try:
                # æ ¹æ®é…ç½®é€‰æ‹©æ¨ç†æ–¹æ³•
                if self.use_incremental:
                    print(f"ğŸ”„ ä½¿ç”¨å¢é‡æ¨ç†")
                    # ä½¿ç”¨å¢é‡æ¨ç†
                    max_len = self.config_loader.get_global_config()['max_seq_len']
                    gen_results = self.model.generate_incremental(
                        image=rgbxyz,
                        max_seq_len=max_len,
                        temperature=self.temperature,
                        eos_threshold=self.eos_threshold
                    )
                else:
                    print(f"ğŸ”„ ä½¿ç”¨ä¼ ç»Ÿæ¨ç†")
                    # ä½¿ç”¨ä¼ ç»Ÿæ¨ç†æ–¹æ³•
                    max_len = self.config_loader.get_global_config()['max_seq_len']
                    gen_results = self.model.generate(
                        image=rgbxyz,
                        max_seq_len=max_len,
                        temperature=1.0,  # ä¼ ç»Ÿæ¨ç†ä½¿ç”¨æ¸©åº¦1.0ï¼Œä¸è®­ç»ƒæ—¶ä¸€è‡´
                        eos_threshold=self.eos_threshold
                    )
                
                # æ£€æŸ¥ç”Ÿæˆç»“æœçš„æœ‰æ•ˆæ€§
                if not gen_results or not isinstance(gen_results, dict):
                    print(f"âš ï¸  æ‰¹æ¬¡ {batch_idx}: ç”Ÿæˆç»“æœæ— æ•ˆï¼Œè·³è¿‡")
                    return None
                
                # æ„å»ºç»“æœå­—å…¸
                result = {
                    'batch_idx': batch_idx,
                    'folder_name': batch['folder_name'][0] if 'folder_name' in batch else f'test_{batch_idx}',
                    'generation_results': {
                        'x': gen_results['x'].cpu().tolist() if 'x' in gen_results else [],
                        'y': gen_results['y'].cpu().tolist() if 'y' in gen_results else [],
                        'z': gen_results['z'].cpu().tolist() if 'z' in gen_results else [],
                        'w': gen_results['w'].cpu().tolist() if 'w' in gen_results else [],
                        'h': gen_results['h'].cpu().tolist() if 'h' in gen_results else [],
                        'l': gen_results['l'].cpu().tolist() if 'l' in gen_results else [],
                    },
                    'ground_truth': {
                        'x': targets['x'].cpu().tolist(),
                        'y': targets['y'].cpu().tolist(),
                        'z': targets['z'].cpu().tolist(),
                        'w': targets['w'].cpu().tolist(),
                        'h': targets['h'].cpu().tolist(),
                        'l': targets['l'].cpu().tolist(),
                        'rotations': targets['rotations'].cpu().tolist(),
                    }
                }
                
                return result
                
            except Exception as e:
                print(f"âŒ æ‰¹æ¬¡ {batch_idx}: æ¨ç†å¤±è´¥ - {e}")
                return None
    
    def inference_dataset(
        self,
        data_root: str,
        stage: str = "test",
        batch_size: int = 1,
        max_samples: Optional[int] = None,
        save_path: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œæ¨ç†
        
        Args:
            data_root: æ•°æ®æ ¹ç›®å½•
            stage: æ•°æ®é˜¶æ®µ (test/val)
            batch_size: æ‰¹æ¬¡å¤§å°
            max_samples: æœ€å¤§æ ·æœ¬æ•°ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
            save_path: ä¿å­˜è·¯å¾„ï¼ˆNoneè¡¨ç¤ºä¸ä¿å­˜ï¼‰
            
        Returns:
            å®Œæ•´çš„æ¨ç†ç»“æœ
        """
        print(f"ğŸš€ å¼€å§‹æ•°æ®é›†æ¨ç†")
        print(f"ğŸ“ æ•°æ®æ ¹ç›®å½•: {data_root}")
        print(f"ğŸ”„ æ•°æ®é˜¶æ®µ: {stage}")
        print(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {batch_size}")
        if max_samples:
            print(f"ğŸ”¢ æœ€å¤§æ ·æœ¬æ•°: {max_samples}")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        dataloader = self.create_dataloader(
            data_root=data_root,
            stage=stage,
            batch_size=batch_size,
            shuffle=False
        )
        
        # å­˜å‚¨æ‰€æœ‰ç»“æœ
        all_results = []
        processed_samples = 0
        total_batches = len(dataloader)
        
        # æ¨ç†å¾ªç¯
        with tqdm(total=total_batches, desc="æ¨ç†è¿›åº¦") as pbar:
            for batch_idx, batch_data in enumerate(dataloader):
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§æ ·æœ¬æ•°
                if max_samples and processed_samples >= max_samples:
                    print(f"âœ… å·²è¾¾åˆ°æœ€å¤§æ ·æœ¬æ•° {max_samples}ï¼Œåœæ­¢æ¨ç†")
                    break
                
                # è¿›è¡Œæ¨ç†
                batch_result = self.inference_single_batch(
                    batch=batch_data,
                    batch_idx=batch_idx
                )
                
                if batch_result is not None:
                    all_results.append(batch_result)
                    processed_samples += 1
                
                pbar.update(1)
                pbar.set_postfix({
                    'processed': processed_samples,
                    'batches': len(all_results)
                })
        
        # æ„å»ºæœ€ç»ˆç»“æœ
        final_result = {
            'test_summary': {
                'num_samples': processed_samples,
                'config_path': self.config_path,
                'checkpoint_path': self.checkpoint_path,
                'device': self.device,
                'use_incremental': self.use_incremental,
                'temperature': self.temperature,
                'eos_threshold': self.eos_threshold,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            },
            'detailed_results': all_results
        }
        
        # ä¿å­˜ç»“æœ
        if save_path:
            self._save_results(final_result, save_path)
        
        print(f"âœ… æ¨ç†å®Œæˆ")
        print(f"ğŸ“Š å¤„ç†æ ·æœ¬æ•°: {processed_samples}")
        print(f"ğŸ“¦ æˆåŠŸæ‰¹æ¬¡: {len(all_results)}")
        
        return final_result
    
    def _save_results(self, results: Dict[str, Any], save_path: str):
        """ä¿å­˜æ¨ç†ç»“æœåˆ°JSONæ–‡ä»¶"""
        save_path = Path(save_path)
        save_path.parent.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ’¾ ä¿å­˜ç»“æœåˆ°: {save_path}")
        
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ç»“æœå·²ä¿å­˜")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='3Dç‰©ä½“æ£€æµ‹æ¨ç†è„šæœ¬')
    
    # å¿…éœ€å‚æ•°
    parser.add_argument('--config', type=str, required=True,
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--checkpoint', type=str, required=True,
                       help='Checkpointæ–‡ä»¶è·¯å¾„ (.pt æˆ– .pth)')
    parser.add_argument('--data-root', type=str, required=True,
                       help='æ•°æ®æ ¹ç›®å½•è·¯å¾„')
    
    # å¯é€‰å‚æ•°
    parser.add_argument('--output', type=str, default=None,
                       help='è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼šcheckpointç›®å½•/inference_results.jsonï¼‰')
    parser.add_argument('--stage', type=str, default='test', choices=['test', 'val'],
                       help='æ•°æ®é˜¶æ®µ (é»˜è®¤: test)')
    parser.add_argument('--batch-size', type=int, default=1,
                       help='æ‰¹æ¬¡å¤§å° (é»˜è®¤: 1)')
    parser.add_argument('--max-samples', type=int, default=None,
                       help='æœ€å¤§æ ·æœ¬æ•° (é»˜è®¤: å…¨éƒ¨)')
    parser.add_argument('--device', type=str, default='cuda',
                       help='æ¨ç†è®¾å¤‡ (é»˜è®¤: cuda)')
    parser.add_argument('--no-incremental', action='store_true',
                       help='ç¦ç”¨å¢é‡æ¨ç†')
    parser.add_argument('--temperature', type=float, default=0.3,
                       help='é‡‡æ ·æ¸©åº¦ (é»˜è®¤: 0.3)')
    parser.add_argument('--eos-threshold', type=float, default=0.5,
                       help='EOSé˜ˆå€¼ (é»˜è®¤: 0.5)')
    parser.add_argument('--num-workers', type=int, default=0,
                       help='æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•° (é»˜è®¤: 0)')
    
    args = parser.parse_args()
    
    # éªŒè¯å‚æ•°
    if not os.path.exists(args.config):
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        sys.exit(1)
    
    if not os.path.exists(args.checkpoint):
        print(f"âŒ Checkpointæ–‡ä»¶ä¸å­˜åœ¨: {args.checkpoint}")
        sys.exit(1)
    
    if not os.path.exists(args.data_root):
        print(f"âŒ æ•°æ®æ ¹ç›®å½•ä¸å­˜åœ¨: {args.data_root}")
        sys.exit(1)
    
    # è®¾ç½®è¾“å‡ºè·¯å¾„
    if args.output is None:
        checkpoint_dir = Path(args.checkpoint).parent
        args.output = checkpoint_dir / 'inference_results.json'
    
    # æ£€æŸ¥è®¾å¤‡
    if args.device == 'cuda' and not torch.cuda.is_available():
        print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ°CPU")
        args.device = 'cpu'
    
    try:
        # åˆ›å»ºæ¨ç†å¼•æ“
        print("ğŸ”§ åˆå§‹åŒ–æ¨ç†å¼•æ“...")
        engine = InferenceEngine(
            config_path=args.config,
            checkpoint_path=args.checkpoint,
            device=args.device,
            use_incremental=not args.no_incremental,
            temperature=args.temperature,
            eos_threshold=args.eos_threshold
        )
        
        # è¿›è¡Œæ¨ç†
        print("ğŸš€ å¼€å§‹æ¨ç†...")
        results = engine.inference_dataset(
            data_root=args.data_root,
            stage=args.stage,
            batch_size=args.batch_size,
            max_samples=args.max_samples,
            save_path=str(args.output)
        )
        
        print("ğŸ‰ æ¨ç†å®Œæˆï¼")
        print(f"ğŸ“Š ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  æ¨ç†è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ æ¨ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
