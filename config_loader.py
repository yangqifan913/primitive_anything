#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
é…ç½®åŠ è½½å™¨ - ç»Ÿä¸€ç®¡ç†æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œæ•°æ®åŠ è½½å™¨çš„é…ç½®
"""

import yaml
import torch
from pathlib import Path
from typing import Dict, Any, Optional

from primitive_anything_3d import PrimitiveTransformer3D
from loss_3d import AdaptivePrimitiveTransformer3DLoss
from dataloader_3d import create_dataloader


class ConfigLoader:
    def __init__(self):
        self.config = {}
        self._unified_mode = True  # ğŸ”§ ä¿®å¤ï¼šå¼ºåˆ¶ä½¿ç”¨ç»Ÿä¸€æ¨¡å¼
    
    def load_unified_config(self, config_path: str = 'training_config.yaml'):
        """åŠ è½½ç»Ÿä¸€é…ç½®æ–‡ä»¶ - ç°åœ¨æ˜¯å”¯ä¸€çš„é…ç½®åŠ è½½æ–¹å¼"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                self.config = yaml.safe_load(f)
            self._unified_mode = True
            
            # éªŒè¯é…ç½®å®Œæ•´æ€§
            self._validate_unified_config()
            
            print(f"âœ… æˆåŠŸåŠ è½½ç»Ÿä¸€é…ç½®æ–‡ä»¶: {config_path}")
            print(f"ğŸ“Š é…ç½®ç‰ˆæœ¬: {self.config.get('version', 'unknown')}")
            
            # æ˜¾ç¤ºè®­ç»ƒé˜¶æ®µä¿¡æ¯
            phases = self.config.get('training', {}).get('phases', {})
            total_epochs = sum(phase.get('epochs', 0) for phase in phases.values())
            print(f"ğŸ¯ è®­ç»ƒé˜¶æ®µ: {len(phases)}ä¸ªé˜¶æ®µï¼Œæ€»è®¡{total_epochs}ä¸ªepoch")
            for name, phase in phases.items():
                print(f"  - {name}: {phase.get('epochs', 0)} epochs")
            
            return self.config
            
        except FileNotFoundError:
            print(f"âŒ é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {config_path}")
            raise
        except yaml.YAMLError as e:
            print(f"âŒ é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
            raise
    
    def _validate_unified_config(self):
        """éªŒè¯ç»Ÿä¸€é…ç½®çš„å®Œæ•´æ€§"""
        required_sections = ['version', 'global', 'model', 'training', 'data', 'loss']
        
        for section in required_sections:
            if section not in self.config:
                raise ValueError(f"é…ç½®æ–‡ä»¶ç¼ºå°‘å¿…éœ€çš„section: {section}")
        
        # éªŒè¯å…³é”®å‚æ•°
        global_config = self.config['global']
        required_global = ['max_seq_len', 'image_size']
        
        for param in required_global:
            if param not in global_config:
                raise ValueError(f"å…¨å±€é…ç½®ç¼ºå°‘å¿…éœ€å‚æ•°: {param}")
    
    # ======================================================================
    # é…ç½®è®¿é—®æ–¹æ³•
    # ======================================================================
    
    def get_model_config(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹é…ç½®"""
        return self.config.get('model', {})
    
    def get_loss_config(self) -> Dict[str, Any]:
        """è·å–æŸå¤±å‡½æ•°é…ç½®"""
        return self.config.get('loss', {})
    
    def get_data_config(self) -> Dict[str, Any]:
        """è·å–æ•°æ®é…ç½®"""
        return self.config.get('data', {})
    
    def get_training_config(self) -> Dict[str, Any]:
        """è·å–è®­ç»ƒé…ç½®"""
        return self.config.get('training', {})
    
    def get_global_config(self) -> Dict[str, Any]:
        """è·å–å…¨å±€é…ç½®"""
        return self.config.get('global', {})
    
    def get_experiment_config(self) -> Dict[str, Any]:
        """è·å–å®éªŒé…ç½®"""
        return self.config.get('experiment', {})
    
    def get_training_phases(self) -> Dict[str, Any]:
        """è·å–è®­ç»ƒé˜¶æ®µé…ç½®"""
        return self.config.get('training', {}).get('phases', {})
    
    def get_optimizer_config(self) -> Dict[str, Any]:
        """è·å–ä¼˜åŒ–å™¨é…ç½®"""
        return self.config.get('training', {}).get('optimizer', {})
    
    def get_scheduler_config(self) -> Dict[str, Any]:
        """è·å–å­¦ä¹ ç‡è°ƒåº¦é…ç½®"""
        return self.config.get('training', {}).get('scheduler', {})
    
    def get_optimization_config(self) -> Dict[str, Any]:
        """è·å–è®­ç»ƒä¼˜åŒ–é…ç½®"""
        return self.config.get('training', {}).get('optimizations', {})
    
    # ======================================================================
    # ä¾¿æ·è®¿é—®æ–¹æ³•
    # ======================================================================
    
    def get(self, key: str, default: Any = None) -> Any:
        """è·å–é…ç½®å€¼ï¼Œæ”¯æŒç‚¹è®°æ³• (e.g., 'global.max_seq_len')"""
        keys = key.split('.')
        value = self.config
        
        for k in keys:
            if isinstance(value, dict) and k in value:
                value = value[k]
            else:
                return default
        
        return value
    
    def set(self, key: str, value: Any):
        """è®¾ç½®é…ç½®å€¼ï¼Œæ”¯æŒç‚¹è®°æ³•"""
        keys = key.split('.')
        current = self.config
        
        for k in keys[:-1]:
            if k not in current:
                current[k] = {}
            current = current[k]
        
        current[keys[-1]] = value
    
    def save_config(self, config_path: str):
        """ä¿å­˜å½“å‰é…ç½®åˆ°æ–‡ä»¶"""
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(self.config, f, default_flow_style=False, allow_unicode=True)
        
        print(f"âœ… é…ç½®å·²ä¿å­˜åˆ°: {config_path}")
    
    def print_summary(self):
        """æ‰“å°é…ç½®æ‘˜è¦"""
        print("\n" + "="*50)
        print("ğŸ“‹ é…ç½®æ‘˜è¦")
        print("="*50)
        
        # å…¨å±€é…ç½®
        global_config = self.get_global_config()
        print(f"ğŸŒ å…¨å±€é…ç½®:")
        print(f"  â€¢ æœ€å¤§åºåˆ—é•¿åº¦: {global_config.get('max_seq_len', 'N/A')}")
        print(f"  â€¢ å›¾åƒå°ºå¯¸: {global_config.get('image_size', 'N/A')}")
        
        # æ¨¡å‹é…ç½®
        model_config = self.get_model_config()
        print(f"ğŸ§  æ¨¡å‹é…ç½®:")
        print(f"  â€¢ æ¨¡å‹ç»´åº¦: {model_config.get('dim', 'N/A')}")
        print(f"  â€¢ Transformerå±‚æ•°: {model_config.get('depth', 'N/A')}")
        print(f"  â€¢ æ³¨æ„åŠ›å¤´æ•°: {model_config.get('heads', 'N/A')}")
        
        # è®­ç»ƒé…ç½®
        training_config = self.get_training_config()
        print(f"ğŸ¯ è®­ç»ƒé…ç½®:")
        batch_size = training_config.get('dataloader', {}).get('batch_size', 'N/A')
        print(f"  â€¢ æ‰¹æ¬¡å¤§å°: {batch_size}")
        
        optimizer_config = training_config.get('optimizer', {})
        print(f"  â€¢ ä¼˜åŒ–å™¨: {optimizer_config.get('name', 'N/A')}")
        print(f"  â€¢ å­¦ä¹ ç‡: {optimizer_config.get('lr', 'N/A')}")
        
        # æ•°æ®é…ç½®
        data_config = self.get_data_config()
        augmentation = data_config.get('augmentation', {}).get('enabled', False)
        print(f"ğŸ“Š æ•°æ®é…ç½®:")
        print(f"  â€¢ æ•°æ®å¢å¼º: {'å¯ç”¨' if augmentation else 'ç¦ç”¨'}")
        
        print("="*50)


# å‘åå…¼å®¹çš„ä¾¿æ·å‡½æ•°
def load_config(config_path: str = 'training_config.yaml') -> ConfigLoader:
    """ä¾¿æ·å‡½æ•°ï¼šåŠ è½½é…ç½®"""
    loader = ConfigLoader()
    loader.load_unified_config(config_path)
    return loader 